{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.6878 - acc: 0.8239\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.3508 - acc: 0.9017\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.3016 - acc: 0.9143\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2711 - acc: 0.9232\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2479 - acc: 0.9301\n",
      "10000/10000 [==============================] - ETA: 0s\n",
      "loss_and_metrics : [0.23114196219146252, 0.93359999999999999]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=32)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"221pt\" viewBox=\"0.00 0.00 313.00 221.00\" width=\"313pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-217 309,-217 309,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2177216202736 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2177216202736</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 305,-212.5 305,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-185.8\">dense_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"166,-166.5 166,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"166,-189.5 222,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"222,-166.5 222,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-197.3\">(None, 784)</text>\n",
       "<polyline fill=\"none\" points=\"222,-189.5 305,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-174.3\">(None, 784)</text>\n",
       "</g>\n",
       "<!-- 2177113530832 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2177113530832</title>\n",
       "<polygon fill=\"none\" points=\"31,-83.5 31,-129.5 274,-129.5 274,-83.5 31,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-102.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"135,-83.5 135,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"135,-106.5 191,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191,-83.5 191,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-114.3\">(None, 784)</text>\n",
       "<polyline fill=\"none\" points=\"191,-106.5 274,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-91.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 2177216202736&#45;&gt;2177113530832 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2177216202736-&gt;2177113530832</title>\n",
       "<path d=\"M152.5,-166.366C152.5,-158.152 152.5,-148.658 152.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"156,-139.607 152.5,-129.607 149,-139.607 156,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2177113529984 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2177113529984</title>\n",
       "<polygon fill=\"none\" points=\"34,-0.5 34,-46.5 271,-46.5 271,-0.5 34,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"138,-0.5 138,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"138,-23.5 194,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"194,-0.5 194,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-31.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"194,-23.5 271,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.5\" y=\"-8.3\">(None, 10)</text>\n",
       "</g>\n",
       "<!-- 2177113530832&#45;&gt;2177113529984 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2177113530832-&gt;2177113529984</title>\n",
       "<path d=\"M152.5,-83.3664C152.5,-75.1516 152.5,-65.6579 152.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"156,-56.6068 152.5,-46.6068 149,-56.6069 156,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('mnist_mlp_model.h5')\n",
    "model = load_model('mnist_mlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.2208 - acc: 0.9375 - val_loss: 0.1885 - val_acc: 0.9454\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1822 - acc: 0.9483 - val_loss: 0.1676 - val_acc: 0.9505\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1564 - acc: 0.9555 - val_loss: 0.1488 - val_acc: 0.9559\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1372 - acc: 0.9614 - val_loss: 0.1361 - val_acc: 0.9601\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1234 - acc: 0.9650 - val_loss: 0.1260 - val_acc: 0.9627\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1125 - acc: 0.9685 - val_loss: 0.1157 - val_acc: 0.9652\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.1029 - acc: 0.9708 - val_loss: 0.1169 - val_acc: 0.9650\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0951 - acc: 0.9734 - val_loss: 0.1052 - val_acc: 0.9689\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0885 - acc: 0.9755 - val_loss: 0.1004 - val_acc: 0.9701\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 0.0824 - acc: 0.9771 - val_loss: 0.0982 - val_acc: 0.9700\n",
      "[0.2208128362649586, 0.18221980188181625, 0.15638889406609816, 0.13722120978628907, 0.12336300864128862, 0.11253509646450402, 0.10287403994612396, 0.095125790434098842, 0.088497841616229075, 0.082413017516266937]\n",
      "[0.93753332608938222, 0.9482833264668783, 0.95553332684437431, 0.96143332755565647, 0.96503332773844397, 0.96853332792719204, 0.97084999498724933, 0.97336666185657184, 0.9754999956786633, 0.97706666246056562]\n",
      "[0.1885249664124567, 0.16761114125431051, 0.14875261420564492, 0.13608836700365645, 0.12603482340133632, 0.11570457441097824, 0.11694487808339181, 0.1051946041273186, 0.10043946655088803, 0.098192123564775108]\n",
      "[0.94539999377727513, 0.95049999392032625, 0.95589999449253082, 0.96009999459981921, 0.96269999486207958, 0.96519999468326567, 0.96499999475479126, 0.96889999496936796, 0.97009999507665634, 0.96999999529123304]\n"
     ]
    }
   ],
   "source": [
    "(X_val, Y_val) = (X_test, Y_test)\n",
    "hist = model.fit(X_train, Y_train, epochs=10, batch_size=10, validation_data=(X_val, Y_val))\n",
    "\n",
    "print(hist.history['loss'])\n",
    "print(hist.history['acc'])\n",
    "print(hist.history['val_loss'])\n",
    "print(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAEKCAYAAABkPZDwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdbw73kgVno8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272d09e98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2072 - acc: 0.1657 - val_loss: 2.1908 - val_acc: 0.1800\n",
      "Epoch 3/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1730 - acc: 0.1729 - val_loss: 2.1631 - val_acc: 0.1867\n",
      "Epoch 4/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1441 - acc: 0.1786 - val_loss: 2.1372 - val_acc: 0.1867\n",
      "Epoch 5/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1177 - acc: 0.1900 - val_loss: 2.1141 - val_acc: 0.1867\n",
      "Epoch 6/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0940 - acc: 0.2029 - val_loss: 2.0931 - val_acc: 0.2033\n",
      "Epoch 7/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.2086 - val_loss: 2.0727 - val_acc: 0.2067\n",
      "Epoch 8/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0521 - acc: 0.2129 - val_loss: 2.0563 - val_acc: 0.2067\n",
      "Epoch 9/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0341 - acc: 0.2157 - val_loss: 2.0409 - val_acc: 0.2033\n",
      "Epoch 10/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0188 - acc: 0.2129 - val_loss: 2.0271 - val_acc: 0.2067\n",
      "Epoch 11/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0042 - acc: 0.2200 - val_loss: 2.0125 - val_acc: 0.2100\n",
      "Epoch 12/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9912 - acc: 0.2186 - val_loss: 2.0036 - val_acc: 0.2100\n",
      "Epoch 13/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9788 - acc: 0.2271 - val_loss: 1.9953 - val_acc: 0.2100\n",
      "Epoch 14/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9684 - acc: 0.2314 - val_loss: 1.9833 - val_acc: 0.2033\n",
      "Epoch 15/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9582 - acc: 0.2214 - val_loss: 1.9753 - val_acc: 0.2067\n",
      "Epoch 16/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9484 - acc: 0.2357 - val_loss: 1.9686 - val_acc: 0.2000\n",
      "Epoch 17/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9394 - acc: 0.2343 - val_loss: 1.9612 - val_acc: 0.2033\n",
      "Epoch 18/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9309 - acc: 0.2314 - val_loss: 1.9537 - val_acc: 0.2100\n",
      "Epoch 19/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.2271 - val_loss: 1.9452 - val_acc: 0.2100\n",
      "Epoch 20/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9156 - acc: 0.2386 - val_loss: 1.9392 - val_acc: 0.2100\n",
      "Epoch 21/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9085 - acc: 0.2314 - val_loss: 1.9360 - val_acc: 0.2067\n",
      "Epoch 22/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9011 - acc: 0.2386 - val_loss: 1.9288 - val_acc: 0.2033\n",
      "Epoch 23/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8954 - acc: 0.2357 - val_loss: 1.9233 - val_acc: 0.2100\n",
      "Epoch 24/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8895 - acc: 0.2314 - val_loss: 1.9200 - val_acc: 0.2067\n",
      "Epoch 25/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8831 - acc: 0.2371 - val_loss: 1.9167 - val_acc: 0.2167\n",
      "Epoch 26/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8771 - acc: 0.2329 - val_loss: 1.9112 - val_acc: 0.2167\n",
      "Epoch 27/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8715 - acc: 0.2400 - val_loss: 1.9101 - val_acc: 0.2167\n",
      "Epoch 28/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8663 - acc: 0.2386 - val_loss: 1.9095 - val_acc: 0.2000\n",
      "Epoch 29/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2414 - val_loss: 1.9055 - val_acc: 0.1900\n",
      "Epoch 30/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8567 - acc: 0.2271 - val_loss: 1.8978 - val_acc: 0.2167\n",
      "Epoch 31/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8514 - acc: 0.2471 - val_loss: 1.8972 - val_acc: 0.1900\n",
      "Epoch 32/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8464 - acc: 0.2371 - val_loss: 1.8928 - val_acc: 0.1867\n",
      "Epoch 33/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8425 - acc: 0.2243 - val_loss: 1.8872 - val_acc: 0.2100\n",
      "Epoch 34/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8384 - acc: 0.2300 - val_loss: 1.8811 - val_acc: 0.1967\n",
      "Epoch 35/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8336 - acc: 0.2471 - val_loss: 1.8837 - val_acc: 0.1933\n",
      "Epoch 36/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8300 - acc: 0.2386 - val_loss: 1.8756 - val_acc: 0.1900\n",
      "Epoch 37/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8258 - acc: 0.2514 - val_loss: 1.8744 - val_acc: 0.1800\n",
      "Epoch 38/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8221 - acc: 0.2357 - val_loss: 1.8700 - val_acc: 0.1933\n",
      "Epoch 39/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8185 - acc: 0.2443 - val_loss: 1.8705 - val_acc: 0.1767\n",
      "Epoch 40/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8145 - acc: 0.2357 - val_loss: 1.8681 - val_acc: 0.1967\n",
      "Epoch 41/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8105 - acc: 0.2414 - val_loss: 1.8669 - val_acc: 0.1833\n",
      "Epoch 42/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8076 - acc: 0.2471 - val_loss: 1.8636 - val_acc: 0.1800\n",
      "Epoch 43/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8044 - acc: 0.2443 - val_loss: 1.8609 - val_acc: 0.1733\n",
      "Epoch 44/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8002 - acc: 0.2371 - val_loss: 1.8573 - val_acc: 0.1967\n",
      "Epoch 45/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7970 - acc: 0.2471 - val_loss: 1.8566 - val_acc: 0.1733\n",
      "Epoch 46/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7939 - acc: 0.2243 - val_loss: 1.8528 - val_acc: 0.1933\n",
      "Epoch 47/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7914 - acc: 0.2586 - val_loss: 1.8552 - val_acc: 0.1800\n",
      "Epoch 48/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7886 - acc: 0.2486 - val_loss: 1.8546 - val_acc: 0.1867\n",
      "Epoch 49/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7859 - acc: 0.2471 - val_loss: 1.8503 - val_acc: 0.1833\n",
      "Epoch 50/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7819 - acc: 0.2457 - val_loss: 1.8445 - val_acc: 0.2333\n",
      "Epoch 51/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7794 - acc: 0.2614 - val_loss: 1.8469 - val_acc: 0.1900\n",
      "Epoch 52/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7761 - acc: 0.2500 - val_loss: 1.8413 - val_acc: 0.1967\n",
      "Epoch 53/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7743 - acc: 0.2543 - val_loss: 1.8489 - val_acc: 0.2067\n",
      "Epoch 54/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7714 - acc: 0.2671 - val_loss: 1.8482 - val_acc: 0.1867\n",
      "Epoch 55/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7685 - acc: 0.2514 - val_loss: 1.8358 - val_acc: 0.2033\n",
      "Epoch 56/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7671 - acc: 0.2529 - val_loss: 1.8422 - val_acc: 0.2200\n",
      "Epoch 57/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7639 - acc: 0.2743 - val_loss: 1.8387 - val_acc: 0.2167\n",
      "Epoch 58/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7613 - acc: 0.2586 - val_loss: 1.8353 - val_acc: 0.2233\n",
      "Epoch 59/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7588 - acc: 0.2657 - val_loss: 1.8336 - val_acc: 0.2233\n",
      "Epoch 60/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7549 - acc: 0.2629 - val_loss: 1.8263 - val_acc: 0.2533\n",
      "Epoch 61/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7563 - acc: 0.2843 - val_loss: 1.8339 - val_acc: 0.2367\n",
      "Epoch 62/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7528 - acc: 0.2714 - val_loss: 1.8319 - val_acc: 0.2233\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7501 - acc: 0.2800 - val_loss: 1.8305 - val_acc: 0.2000\n",
      "Epoch 64/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7480 - acc: 0.2800 - val_loss: 1.8279 - val_acc: 0.2167\n",
      "Epoch 65/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7455 - acc: 0.2843 - val_loss: 1.8307 - val_acc: 0.2000\n",
      "Epoch 66/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7436 - acc: 0.2800 - val_loss: 1.8306 - val_acc: 0.2067\n",
      "Epoch 67/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7417 - acc: 0.2671 - val_loss: 1.8301 - val_acc: 0.2033\n",
      "Epoch 68/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7402 - acc: 0.2757 - val_loss: 1.8249 - val_acc: 0.2000\n",
      "Epoch 69/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7373 - acc: 0.2829 - val_loss: 1.8310 - val_acc: 0.2167\n",
      "Epoch 70/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7354 - acc: 0.2857 - val_loss: 1.8291 - val_acc: 0.2433\n",
      "Epoch 71/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7342 - acc: 0.2814 - val_loss: 1.8223 - val_acc: 0.2167\n",
      "Epoch 72/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7325 - acc: 0.2857 - val_loss: 1.8227 - val_acc: 0.2133\n",
      "Epoch 73/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7295 - acc: 0.2843 - val_loss: 1.8262 - val_acc: 0.2467\n",
      "Epoch 74/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7280 - acc: 0.2829 - val_loss: 1.8266 - val_acc: 0.1967\n",
      "Epoch 75/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7265 - acc: 0.2771 - val_loss: 1.8193 - val_acc: 0.2233\n",
      "Epoch 76/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7252 - acc: 0.2871 - val_loss: 1.8207 - val_acc: 0.2200\n",
      "Epoch 77/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7225 - acc: 0.3071 - val_loss: 1.8241 - val_acc: 0.2033\n",
      "Epoch 78/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7208 - acc: 0.2857 - val_loss: 1.8190 - val_acc: 0.1967\n",
      "Epoch 79/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7195 - acc: 0.2814 - val_loss: 1.8205 - val_acc: 0.2433\n",
      "Epoch 80/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7187 - acc: 0.2929 - val_loss: 1.8202 - val_acc: 0.2067\n",
      "Epoch 81/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7163 - acc: 0.2800 - val_loss: 1.8262 - val_acc: 0.2067\n",
      "Epoch 82/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7137 - acc: 0.2829 - val_loss: 1.8161 - val_acc: 0.2667\n",
      "Epoch 83/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7129 - acc: 0.2857 - val_loss: 1.8209 - val_acc: 0.2400\n",
      "Epoch 84/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7121 - acc: 0.2943 - val_loss: 1.8230 - val_acc: 0.2167\n",
      "Epoch 85/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7096 - acc: 0.2986 - val_loss: 1.8169 - val_acc: 0.2267\n",
      "Epoch 86/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7079 - acc: 0.2800 - val_loss: 1.8158 - val_acc: 0.2633\n",
      "Epoch 87/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7048 - acc: 0.3086 - val_loss: 1.8199 - val_acc: 0.2733\n",
      "Epoch 88/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7058 - acc: 0.3043 - val_loss: 1.8128 - val_acc: 0.2433\n",
      "Epoch 89/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7040 - acc: 0.3029 - val_loss: 1.8177 - val_acc: 0.2167\n",
      "Epoch 90/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7024 - acc: 0.2829 - val_loss: 1.8166 - val_acc: 0.2233\n",
      "Epoch 91/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7014 - acc: 0.3086 - val_loss: 1.8201 - val_acc: 0.2167\n",
      "Epoch 92/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6984 - acc: 0.3129 - val_loss: 1.8232 - val_acc: 0.2067\n",
      "Epoch 93/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6978 - acc: 0.3029 - val_loss: 1.8179 - val_acc: 0.2733\n",
      "Epoch 94/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6963 - acc: 0.3171 - val_loss: 1.8198 - val_acc: 0.2100\n",
      "Epoch 95/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6935 - acc: 0.3029 - val_loss: 1.8246 - val_acc: 0.2833\n",
      "Epoch 96/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6941 - acc: 0.3071 - val_loss: 1.8115 - val_acc: 0.2200\n",
      "Epoch 97/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6922 - acc: 0.3057 - val_loss: 1.8273 - val_acc: 0.2200\n",
      "Epoch 98/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6915 - acc: 0.3043 - val_loss: 1.8128 - val_acc: 0.2167\n",
      "Epoch 99/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6894 - acc: 0.3129 - val_loss: 1.8256 - val_acc: 0.2200\n",
      "Epoch 100/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6882 - acc: 0.3029 - val_loss: 1.8232 - val_acc: 0.2300\n",
      "Epoch 101/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6868 - acc: 0.3143 - val_loss: 1.8262 - val_acc: 0.2167\n",
      "Epoch 102/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6870 - acc: 0.3100 - val_loss: 1.8221 - val_acc: 0.2300\n",
      "Epoch 103/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6833 - acc: 0.3071 - val_loss: 1.8113 - val_acc: 0.2467\n",
      "Epoch 104/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6833 - acc: 0.3157 - val_loss: 1.8140 - val_acc: 0.2200\n",
      "Epoch 105/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6811 - acc: 0.3057 - val_loss: 1.8079 - val_acc: 0.2100\n",
      "Epoch 106/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6804 - acc: 0.3186 - val_loss: 1.8204 - val_acc: 0.2167\n",
      "Epoch 107/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6794 - acc: 0.3114 - val_loss: 1.8242 - val_acc: 0.2367\n",
      "Epoch 108/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6775 - acc: 0.3157 - val_loss: 1.8196 - val_acc: 0.2767\n",
      "Epoch 109/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6770 - acc: 0.3143 - val_loss: 1.8199 - val_acc: 0.2367\n",
      "Epoch 110/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6764 - acc: 0.3214 - val_loss: 1.8167 - val_acc: 0.2200\n",
      "Epoch 111/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6764 - acc: 0.3086 - val_loss: 1.8221 - val_acc: 0.2200\n",
      "Epoch 112/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6734 - acc: 0.3171 - val_loss: 1.8224 - val_acc: 0.2300\n",
      "Epoch 113/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6724 - acc: 0.3143 - val_loss: 1.8206 - val_acc: 0.2367\n",
      "Epoch 114/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6711 - acc: 0.3100 - val_loss: 1.8256 - val_acc: 0.2200\n",
      "Epoch 115/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6698 - acc: 0.3186 - val_loss: 1.8155 - val_acc: 0.2367\n",
      "Epoch 116/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6688 - acc: 0.3214 - val_loss: 1.8305 - val_acc: 0.2300\n",
      "Epoch 117/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6683 - acc: 0.3129 - val_loss: 1.8229 - val_acc: 0.2333\n",
      "Epoch 118/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6668 - acc: 0.3157 - val_loss: 1.8286 - val_acc: 0.2300\n",
      "Epoch 119/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6655 - acc: 0.3286 - val_loss: 1.8267 - val_acc: 0.2233\n",
      "Epoch 120/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6533 - acc: 0.333 - ETA: 0s - loss: 1.6646 - acc: 0.3200 - val_loss: 1.8247 - val_acc: 0.2167\n",
      "Epoch 121/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6633 - acc: 0.3186 - val_loss: 1.8154 - val_acc: 0.2267\n",
      "Epoch 122/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6622 - acc: 0.3214 - val_loss: 1.8196 - val_acc: 0.2267\n",
      "Epoch 123/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6613 - acc: 0.3214 - val_loss: 1.8167 - val_acc: 0.2233\n",
      "Epoch 124/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6600 - acc: 0.3186 - val_loss: 1.8243 - val_acc: 0.2367\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6590 - acc: 0.3157 - val_loss: 1.8284 - val_acc: 0.2367\n",
      "Epoch 126/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6572 - acc: 0.3186 - val_loss: 1.8288 - val_acc: 0.2267\n",
      "Epoch 127/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6564 - acc: 0.3171 - val_loss: 1.8174 - val_acc: 0.2433\n",
      "Epoch 128/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6545 - acc: 0.3357 - val_loss: 1.8320 - val_acc: 0.2200\n",
      "Epoch 129/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6548 - acc: 0.3129 - val_loss: 1.8262 - val_acc: 0.2233\n",
      "Epoch 130/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6527 - acc: 0.3257 - val_loss: 1.8173 - val_acc: 0.2567\n",
      "Epoch 131/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6523 - acc: 0.3229 - val_loss: 1.8279 - val_acc: 0.2533\n",
      "Epoch 132/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6521 - acc: 0.3300 - val_loss: 1.8162 - val_acc: 0.2233\n",
      "Epoch 133/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6504 - acc: 0.3257 - val_loss: 1.8350 - val_acc: 0.2500\n",
      "Epoch 134/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6490 - acc: 0.3200 - val_loss: 1.8276 - val_acc: 0.2733\n",
      "Epoch 135/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6488 - acc: 0.3314 - val_loss: 1.8234 - val_acc: 0.2267\n",
      "Epoch 136/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6471 - acc: 0.3243 - val_loss: 1.8213 - val_acc: 0.2333\n",
      "Epoch 137/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6461 - acc: 0.3214 - val_loss: 1.8224 - val_acc: 0.2300\n",
      "Epoch 138/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6465 - acc: 0.3214 - val_loss: 1.8261 - val_acc: 0.2367\n",
      "Epoch 139/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6437 - acc: 0.3314 - val_loss: 1.8509 - val_acc: 0.2467\n",
      "Epoch 140/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6439 - acc: 0.3286 - val_loss: 1.8180 - val_acc: 0.2300\n",
      "Epoch 141/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6422 - acc: 0.3286 - val_loss: 1.8326 - val_acc: 0.2367\n",
      "Epoch 142/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6410 - acc: 0.3300 - val_loss: 1.8209 - val_acc: 0.2633\n",
      "Epoch 143/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6410 - acc: 0.3286 - val_loss: 1.8373 - val_acc: 0.2533\n",
      "Epoch 144/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6404 - acc: 0.3443 - val_loss: 1.8323 - val_acc: 0.2333\n",
      "Epoch 145/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6393 - acc: 0.3300 - val_loss: 1.8229 - val_acc: 0.2200\n",
      "Epoch 146/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6367 - acc: 0.3386 - val_loss: 1.8352 - val_acc: 0.2267\n",
      "Epoch 147/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6388 - acc: 0.3171 - val_loss: 1.8356 - val_acc: 0.2333\n",
      "Epoch 148/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6369 - acc: 0.3357 - val_loss: 1.8314 - val_acc: 0.2533\n",
      "Epoch 149/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6356 - acc: 0.3357 - val_loss: 1.8216 - val_acc: 0.2133\n",
      "Epoch 150/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6348 - acc: 0.3286 - val_loss: 1.8304 - val_acc: 0.2267\n",
      "Epoch 151/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6346 - acc: 0.3286 - val_loss: 1.8257 - val_acc: 0.2267\n",
      "Epoch 152/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6337 - acc: 0.3314 - val_loss: 1.8306 - val_acc: 0.2333\n",
      "Epoch 153/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6319 - acc: 0.3429 - val_loss: 1.8300 - val_acc: 0.2267\n",
      "Epoch 154/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6302 - acc: 0.3514 - val_loss: 1.8372 - val_acc: 0.2233\n",
      "Epoch 155/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6303 - acc: 0.3257 - val_loss: 1.8237 - val_acc: 0.2333\n",
      "Epoch 156/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6308 - acc: 0.3329 - val_loss: 1.8350 - val_acc: 0.2267\n",
      "Epoch 157/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6289 - acc: 0.3371 - val_loss: 1.8279 - val_acc: 0.2200\n",
      "Epoch 158/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6292 - acc: 0.3371 - val_loss: 1.8353 - val_acc: 0.2267\n",
      "Epoch 159/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6272 - acc: 0.3300 - val_loss: 1.8358 - val_acc: 0.2467\n",
      "Epoch 160/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6263 - acc: 0.3329 - val_loss: 1.8322 - val_acc: 0.2300\n",
      "Epoch 161/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6258 - acc: 0.3343 - val_loss: 1.8412 - val_acc: 0.2300\n",
      "Epoch 162/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6252 - acc: 0.3314 - val_loss: 1.8381 - val_acc: 0.2267\n",
      "Epoch 163/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6240 - acc: 0.3357 - val_loss: 1.8373 - val_acc: 0.2267\n",
      "Epoch 164/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6227 - acc: 0.3400 - val_loss: 1.8274 - val_acc: 0.2167\n",
      "Epoch 165/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6223 - acc: 0.3243 - val_loss: 1.8407 - val_acc: 0.2400\n",
      "Epoch 166/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6218 - acc: 0.3329 - val_loss: 1.8359 - val_acc: 0.2233\n",
      "Epoch 167/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6208 - acc: 0.3314 - val_loss: 1.8402 - val_acc: 0.2567\n",
      "Epoch 168/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6196 - acc: 0.3443 - val_loss: 1.8459 - val_acc: 0.2200\n",
      "Epoch 169/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6187 - acc: 0.3443 - val_loss: 1.8538 - val_acc: 0.2233\n",
      "Epoch 170/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6181 - acc: 0.3386 - val_loss: 1.8355 - val_acc: 0.2233\n",
      "Epoch 171/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6160 - acc: 0.3271 - val_loss: 1.8514 - val_acc: 0.2633\n",
      "Epoch 172/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6151 - acc: 0.3429 - val_loss: 1.8399 - val_acc: 0.2200\n",
      "Epoch 173/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6170 - acc: 0.3400 - val_loss: 1.8403 - val_acc: 0.2467\n",
      "Epoch 174/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6158 - acc: 0.3357 - val_loss: 1.8422 - val_acc: 0.2200\n",
      "Epoch 175/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6155 - acc: 0.3386 - val_loss: 1.8427 - val_acc: 0.2200\n",
      "Epoch 176/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6133 - acc: 0.3286 - val_loss: 1.8411 - val_acc: 0.2633\n",
      "Epoch 177/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6124 - acc: 0.3414 - val_loss: 1.8393 - val_acc: 0.2200\n",
      "Epoch 178/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6130 - acc: 0.3386 - val_loss: 1.8432 - val_acc: 0.2300\n",
      "Epoch 179/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6125 - acc: 0.3471 - val_loss: 1.8382 - val_acc: 0.2233\n",
      "Epoch 180/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6103 - acc: 0.3400 - val_loss: 1.8454 - val_acc: 0.2667\n",
      "Epoch 181/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6105 - acc: 0.3471 - val_loss: 1.8367 - val_acc: 0.2567\n",
      "Epoch 182/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6097 - acc: 0.3457 - val_loss: 1.8395 - val_acc: 0.2167\n",
      "Epoch 183/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.3343 - val_loss: 1.8450 - val_acc: 0.2333\n",
      "Epoch 184/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6083 - acc: 0.3500 - val_loss: 1.8475 - val_acc: 0.2267\n",
      "Epoch 185/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6066 - acc: 0.3414 - val_loss: 1.8667 - val_acc: 0.2233\n",
      "Epoch 186/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6077 - acc: 0.3443 - val_loss: 1.8466 - val_acc: 0.2267\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6056 - acc: 0.3543 - val_loss: 1.8455 - val_acc: 0.2200\n",
      "Epoch 188/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6065 - acc: 0.3414 - val_loss: 1.8428 - val_acc: 0.2200\n",
      "Epoch 189/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6037 - acc: 0.3529 - val_loss: 1.8553 - val_acc: 0.2200\n",
      "Epoch 190/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6057 - acc: 0.3314 - val_loss: 1.8422 - val_acc: 0.2333\n",
      "Epoch 191/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6032 - acc: 0.3571 - val_loss: 1.8622 - val_acc: 0.2233\n",
      "Epoch 192/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6035 - acc: 0.3471 - val_loss: 1.8539 - val_acc: 0.2200\n",
      "Epoch 193/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6021 - acc: 0.3443 - val_loss: 1.8578 - val_acc: 0.2300\n",
      "Epoch 194/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6016 - acc: 0.3400 - val_loss: 1.8508 - val_acc: 0.2300\n",
      "Epoch 195/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6012 - acc: 0.3471 - val_loss: 1.8488 - val_acc: 0.2467\n",
      "Epoch 196/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6008 - acc: 0.3514 - val_loss: 1.8513 - val_acc: 0.2200\n",
      "Epoch 197/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5995 - acc: 0.3514 - val_loss: 1.8496 - val_acc: 0.2300\n",
      "Epoch 198/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5998 - acc: 0.3400 - val_loss: 1.8602 - val_acc: 0.2233\n",
      "Epoch 199/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5995 - acc: 0.3414 - val_loss: 1.8571 - val_acc: 0.2233\n",
      "Epoch 200/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5967 - acc: 0.3457 - val_loss: 1.8616 - val_acc: 0.2633\n",
      "Epoch 201/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5978 - acc: 0.3443 - val_loss: 1.8491 - val_acc: 0.2133\n",
      "Epoch 202/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5972 - acc: 0.3486 - val_loss: 1.8495 - val_acc: 0.2100\n",
      "Epoch 203/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5959 - acc: 0.3343 - val_loss: 1.8595 - val_acc: 0.2300\n",
      "Epoch 204/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5946 - acc: 0.3614 - val_loss: 1.8485 - val_acc: 0.2167\n",
      "Epoch 205/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5946 - acc: 0.3457 - val_loss: 1.8550 - val_acc: 0.2233\n",
      "Epoch 206/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5939 - acc: 0.3543 - val_loss: 1.8505 - val_acc: 0.2133\n",
      "Epoch 207/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5940 - acc: 0.3543 - val_loss: 1.8547 - val_acc: 0.2267\n",
      "Epoch 208/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5913 - acc: 0.3457 - val_loss: 1.8624 - val_acc: 0.2233\n",
      "Epoch 209/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5913 - acc: 0.3514 - val_loss: 1.8547 - val_acc: 0.2367\n",
      "Epoch 210/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5907 - acc: 0.3471 - val_loss: 1.8547 - val_acc: 0.2300\n",
      "Epoch 211/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5897 - acc: 0.3429 - val_loss: 1.8480 - val_acc: 0.2600\n",
      "Epoch 212/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5907 - acc: 0.3514 - val_loss: 1.8623 - val_acc: 0.2333\n",
      "Epoch 213/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5888 - acc: 0.3529 - val_loss: 1.8556 - val_acc: 0.2300\n",
      "Epoch 214/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5899 - acc: 0.3457 - val_loss: 1.8576 - val_acc: 0.2300\n",
      "Epoch 215/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5880 - acc: 0.3500 - val_loss: 1.8604 - val_acc: 0.2667\n",
      "Epoch 216/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5889 - acc: 0.3514 - val_loss: 1.8652 - val_acc: 0.2267\n",
      "Epoch 217/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5875 - acc: 0.3457 - val_loss: 1.8673 - val_acc: 0.2267\n",
      "Epoch 218/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5870 - acc: 0.3457 - val_loss: 1.8629 - val_acc: 0.2300\n",
      "Epoch 219/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5859 - acc: 0.3471 - val_loss: 1.8686 - val_acc: 0.2267\n",
      "Epoch 220/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5850 - acc: 0.3514 - val_loss: 1.8620 - val_acc: 0.2267\n",
      "Epoch 221/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5851 - acc: 0.3500 - val_loss: 1.8659 - val_acc: 0.2267\n",
      "Epoch 222/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3514 - val_loss: 1.8789 - val_acc: 0.2467\n",
      "Epoch 223/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5838 - acc: 0.3414 - val_loss: 1.8650 - val_acc: 0.2600\n",
      "Epoch 224/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3543 - val_loss: 1.8798 - val_acc: 0.2433\n",
      "Epoch 225/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5828 - acc: 0.3429 - val_loss: 1.8601 - val_acc: 0.2200\n",
      "Epoch 226/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5830 - acc: 0.3500 - val_loss: 1.8716 - val_acc: 0.2267\n",
      "Epoch 227/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5819 - acc: 0.3500 - val_loss: 1.8667 - val_acc: 0.2167\n",
      "Epoch 228/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5819 - acc: 0.3443 - val_loss: 1.8712 - val_acc: 0.2267\n",
      "Epoch 229/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5796 - acc: 0.3557 - val_loss: 1.8726 - val_acc: 0.2233\n",
      "Epoch 230/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5812 - acc: 0.3429 - val_loss: 1.8655 - val_acc: 0.2167\n",
      "Epoch 231/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5793 - acc: 0.3529 - val_loss: 1.8673 - val_acc: 0.2267\n",
      "Epoch 232/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5804 - acc: 0.3529 - val_loss: 1.8788 - val_acc: 0.2200\n",
      "Epoch 233/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5792 - acc: 0.3471 - val_loss: 1.8843 - val_acc: 0.2300\n",
      "Epoch 234/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5786 - acc: 0.3457 - val_loss: 1.8724 - val_acc: 0.2200\n",
      "Epoch 235/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5771 - acc: 0.3586 - val_loss: 1.8742 - val_acc: 0.2100\n",
      "Epoch 236/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5774 - acc: 0.3471 - val_loss: 1.8713 - val_acc: 0.2167\n",
      "Epoch 237/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3471 - val_loss: 1.8772 - val_acc: 0.2300\n",
      "Epoch 238/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5769 - acc: 0.3457 - val_loss: 1.8655 - val_acc: 0.2133\n",
      "Epoch 239/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3543 - val_loss: 1.8728 - val_acc: 0.2300\n",
      "Epoch 240/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5745 - acc: 0.3543 - val_loss: 1.8772 - val_acc: 0.2467\n",
      "Epoch 241/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5743 - acc: 0.3571 - val_loss: 1.8783 - val_acc: 0.2400\n",
      "Epoch 242/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5739 - acc: 0.3543 - val_loss: 1.8782 - val_acc: 0.2067\n",
      "Epoch 243/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5738 - acc: 0.3600 - val_loss: 1.8843 - val_acc: 0.2167\n",
      "Epoch 244/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5706 - acc: 0.3600 - val_loss: 1.8758 - val_acc: 0.2200\n",
      "Epoch 245/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5710 - acc: 0.3586 - val_loss: 1.8888 - val_acc: 0.2267\n",
      "Epoch 246/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5707 - acc: 0.3543 - val_loss: 1.8758 - val_acc: 0.2633\n",
      "Epoch 247/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5717 - acc: 0.3571 - val_loss: 1.8652 - val_acc: 0.2200\n",
      "Epoch 248/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5710 - acc: 0.3557 - val_loss: 1.8728 - val_acc: 0.2300\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5708 - acc: 0.3471 - val_loss: 1.8868 - val_acc: 0.2267\n",
      "Epoch 250/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5675 - acc: 0.3643 - val_loss: 1.9023 - val_acc: 0.2167\n",
      "Epoch 251/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5699 - acc: 0.3471 - val_loss: 1.8839 - val_acc: 0.2300\n",
      "Epoch 252/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3657 - val_loss: 1.8803 - val_acc: 0.2267\n",
      "Epoch 253/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5684 - acc: 0.3543 - val_loss: 1.8864 - val_acc: 0.2167\n",
      "Epoch 254/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5674 - acc: 0.3557 - val_loss: 1.8861 - val_acc: 0.2267\n",
      "Epoch 255/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3457 - val_loss: 1.8744 - val_acc: 0.2300\n",
      "Epoch 256/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5653 - acc: 0.3529 - val_loss: 1.8796 - val_acc: 0.2533\n",
      "Epoch 257/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5663 - acc: 0.3486 - val_loss: 1.8947 - val_acc: 0.2367\n",
      "Epoch 258/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5644 - acc: 0.3600 - val_loss: 1.9015 - val_acc: 0.2567\n",
      "Epoch 259/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5643 - acc: 0.3629 - val_loss: 1.8974 - val_acc: 0.2233\n",
      "Epoch 260/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5653 - acc: 0.3600 - val_loss: 1.9021 - val_acc: 0.2233\n",
      "Epoch 261/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5632 - acc: 0.3586 - val_loss: 1.8781 - val_acc: 0.2167\n",
      "Epoch 262/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3600 - val_loss: 1.8893 - val_acc: 0.2100\n",
      "Epoch 263/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5627 - acc: 0.3643 - val_loss: 1.8954 - val_acc: 0.2200\n",
      "Epoch 264/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3557 - val_loss: 1.8871 - val_acc: 0.2133\n",
      "Epoch 265/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3657 - val_loss: 1.8966 - val_acc: 0.2267\n",
      "Epoch 266/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5609 - acc: 0.3629 - val_loss: 1.9027 - val_acc: 0.2500\n",
      "Epoch 267/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5610 - acc: 0.3657 - val_loss: 1.8957 - val_acc: 0.2267\n",
      "Epoch 268/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5608 - acc: 0.3614 - val_loss: 1.8805 - val_acc: 0.2167\n",
      "Epoch 269/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5596 - acc: 0.3643 - val_loss: 1.8882 - val_acc: 0.2167\n",
      "Epoch 270/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5592 - acc: 0.3686 - val_loss: 1.8747 - val_acc: 0.2333\n",
      "Epoch 271/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5605 - acc: 0.3643 - val_loss: 1.8974 - val_acc: 0.2333\n",
      "Epoch 272/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5591 - acc: 0.3586 - val_loss: 1.9102 - val_acc: 0.2267\n",
      "Epoch 273/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5587 - acc: 0.3571 - val_loss: 1.9020 - val_acc: 0.2133\n",
      "Epoch 274/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5584 - acc: 0.3471 - val_loss: 1.8899 - val_acc: 0.2233\n",
      "Epoch 275/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5585 - acc: 0.3586 - val_loss: 1.9001 - val_acc: 0.2233\n",
      "Epoch 276/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5575 - acc: 0.3543 - val_loss: 1.9018 - val_acc: 0.2567\n",
      "Epoch 277/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5576 - acc: 0.3743 - val_loss: 1.8967 - val_acc: 0.2167\n",
      "Epoch 278/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5574 - acc: 0.3600 - val_loss: 1.9086 - val_acc: 0.2300\n",
      "Epoch 279/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5554 - acc: 0.3586 - val_loss: 1.8916 - val_acc: 0.2200\n",
      "Epoch 280/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5555 - acc: 0.3657 - val_loss: 1.8932 - val_acc: 0.2167\n",
      "Epoch 281/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5537 - acc: 0.3557 - val_loss: 1.8958 - val_acc: 0.2333\n",
      "Epoch 282/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5549 - acc: 0.3700 - val_loss: 1.8884 - val_acc: 0.2167\n",
      "Epoch 283/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5537 - acc: 0.3571 - val_loss: 1.8992 - val_acc: 0.2233\n",
      "Epoch 284/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5538 - acc: 0.3586 - val_loss: 1.8983 - val_acc: 0.2600\n",
      "Epoch 285/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5531 - acc: 0.3586 - val_loss: 1.8938 - val_acc: 0.2200\n",
      "Epoch 286/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5532 - acc: 0.3557 - val_loss: 1.9053 - val_acc: 0.2233\n",
      "Epoch 287/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5523 - acc: 0.3700 - val_loss: 1.9063 - val_acc: 0.2133\n",
      "Epoch 288/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5520 - acc: 0.3600 - val_loss: 1.8988 - val_acc: 0.2100\n",
      "Epoch 289/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5515 - acc: 0.3600 - val_loss: 1.9080 - val_acc: 0.2100\n",
      "Epoch 290/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5510 - acc: 0.3643 - val_loss: 1.9102 - val_acc: 0.2200\n",
      "Epoch 291/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5509 - acc: 0.3586 - val_loss: 1.9092 - val_acc: 0.2167\n",
      "Epoch 292/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5503 - acc: 0.3514 - val_loss: 1.9041 - val_acc: 0.2400\n",
      "Epoch 293/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5502 - acc: 0.3543 - val_loss: 1.9064 - val_acc: 0.2333\n",
      "Epoch 294/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5494 - acc: 0.3571 - val_loss: 1.9065 - val_acc: 0.2533\n",
      "Epoch 295/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5495 - acc: 0.3729 - val_loss: 1.9167 - val_acc: 0.2133\n",
      "Epoch 296/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5490 - acc: 0.3571 - val_loss: 1.9152 - val_acc: 0.2067\n",
      "Epoch 297/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5475 - acc: 0.3586 - val_loss: 1.9184 - val_acc: 0.2167\n",
      "Epoch 298/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5473 - acc: 0.3486 - val_loss: 1.9016 - val_acc: 0.2300\n",
      "Epoch 299/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5480 - acc: 0.3671 - val_loss: 1.9222 - val_acc: 0.2300\n",
      "Epoch 300/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5469 - acc: 0.3657 - val_loss: 1.9094 - val_acc: 0.2300\n",
      "Epoch 301/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5465 - acc: 0.3629 - val_loss: 1.8994 - val_acc: 0.2200\n",
      "Epoch 302/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5476 - acc: 0.3686 - val_loss: 1.9139 - val_acc: 0.2200\n",
      "Epoch 303/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5461 - acc: 0.3543 - val_loss: 1.9058 - val_acc: 0.2367\n",
      "Epoch 304/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5448 - acc: 0.3714 - val_loss: 1.9130 - val_acc: 0.2167\n",
      "Epoch 305/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5428 - acc: 0.3671 - val_loss: 1.9212 - val_acc: 0.2233\n",
      "Epoch 306/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5455 - acc: 0.3657 - val_loss: 1.9147 - val_acc: 0.2333\n",
      "Epoch 307/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3714 - val_loss: 1.9138 - val_acc: 0.2300\n",
      "Epoch 308/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5448 - acc: 0.3600 - val_loss: 1.9184 - val_acc: 0.2333\n",
      "Epoch 309/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5432 - acc: 0.3629 - val_loss: 1.9206 - val_acc: 0.2333\n",
      "Epoch 310/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5444 - acc: 0.3657 - val_loss: 1.9130 - val_acc: 0.2233\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5423 - acc: 0.3600 - val_loss: 1.9091 - val_acc: 0.2300\n",
      "Epoch 312/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5419 - acc: 0.3729 - val_loss: 1.9359 - val_acc: 0.2333\n",
      "Epoch 313/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3714 - val_loss: 1.9112 - val_acc: 0.2567\n",
      "Epoch 314/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5428 - acc: 0.3686 - val_loss: 1.9198 - val_acc: 0.2300\n",
      "Epoch 315/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3657 - val_loss: 1.9171 - val_acc: 0.2233\n",
      "Epoch 316/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3643 - val_loss: 1.9198 - val_acc: 0.2267\n",
      "Epoch 317/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5408 - acc: 0.3600 - val_loss: 1.9277 - val_acc: 0.2333\n",
      "Epoch 318/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5404 - acc: 0.3643 - val_loss: 1.9164 - val_acc: 0.2300\n",
      "Epoch 319/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5386 - acc: 0.3586 - val_loss: 1.9136 - val_acc: 0.2233\n",
      "Epoch 320/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5389 - acc: 0.3600 - val_loss: 1.9330 - val_acc: 0.2233\n",
      "Epoch 321/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5388 - acc: 0.3686 - val_loss: 1.9325 - val_acc: 0.2300\n",
      "Epoch 322/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5370 - acc: 0.3657 - val_loss: 1.9242 - val_acc: 0.2567\n",
      "Epoch 323/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5379 - acc: 0.3700 - val_loss: 1.9245 - val_acc: 0.2300\n",
      "Epoch 324/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5368 - acc: 0.3714 - val_loss: 1.9307 - val_acc: 0.2233\n",
      "Epoch 325/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3600 - val_loss: 1.9195 - val_acc: 0.2133\n",
      "Epoch 326/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5361 - acc: 0.3600 - val_loss: 1.9335 - val_acc: 0.2300\n",
      "Epoch 327/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5355 - acc: 0.3743 - val_loss: 1.9443 - val_acc: 0.2300\n",
      "Epoch 328/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5363 - acc: 0.3614 - val_loss: 1.9365 - val_acc: 0.2233\n",
      "Epoch 329/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5350 - acc: 0.3743 - val_loss: 1.9217 - val_acc: 0.2267\n",
      "Epoch 330/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5347 - acc: 0.3657 - val_loss: 1.9312 - val_acc: 0.2333\n",
      "Epoch 331/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.3657 - val_loss: 1.9221 - val_acc: 0.2167\n",
      "Epoch 332/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5342 - acc: 0.3600 - val_loss: 1.9381 - val_acc: 0.2533\n",
      "Epoch 333/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5354 - acc: 0.3729 - val_loss: 1.9298 - val_acc: 0.2300\n",
      "Epoch 334/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5346 - acc: 0.3614 - val_loss: 1.9326 - val_acc: 0.2300\n",
      "Epoch 335/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5323 - acc: 0.3786 - val_loss: 1.9313 - val_acc: 0.2567\n",
      "Epoch 336/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5331 - acc: 0.3814 - val_loss: 1.9261 - val_acc: 0.2300\n",
      "Epoch 337/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5327 - acc: 0.3743 - val_loss: 1.9394 - val_acc: 0.2233\n",
      "Epoch 338/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5316 - acc: 0.3786 - val_loss: 1.9305 - val_acc: 0.2133\n",
      "Epoch 339/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5308 - acc: 0.3700 - val_loss: 1.9240 - val_acc: 0.2167\n",
      "Epoch 340/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5323 - acc: 0.3729 - val_loss: 1.9486 - val_acc: 0.2233\n",
      "Epoch 341/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5312 - acc: 0.3786 - val_loss: 1.9334 - val_acc: 0.2333\n",
      "Epoch 342/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5308 - acc: 0.3571 - val_loss: 1.9496 - val_acc: 0.2167\n",
      "Epoch 343/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5307 - acc: 0.3686 - val_loss: 1.9379 - val_acc: 0.2267\n",
      "Epoch 344/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5304 - acc: 0.3714 - val_loss: 1.9505 - val_acc: 0.2233\n",
      "Epoch 345/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5287 - acc: 0.3700 - val_loss: 1.9509 - val_acc: 0.2533\n",
      "Epoch 346/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5296 - acc: 0.3714 - val_loss: 1.9366 - val_acc: 0.2267\n",
      "Epoch 347/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5272 - acc: 0.3786 - val_loss: 1.9216 - val_acc: 0.2233\n",
      "Epoch 348/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5280 - acc: 0.3629 - val_loss: 1.9342 - val_acc: 0.2167\n",
      "Epoch 349/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5278 - acc: 0.3714 - val_loss: 1.9341 - val_acc: 0.2200\n",
      "Epoch 350/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5276 - acc: 0.3671 - val_loss: 1.9376 - val_acc: 0.2333\n",
      "Epoch 351/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5259 - acc: 0.3729 - val_loss: 1.9388 - val_acc: 0.2367\n",
      "Epoch 352/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5278 - acc: 0.3743 - val_loss: 1.9414 - val_acc: 0.2300\n",
      "Epoch 353/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5252 - acc: 0.3714 - val_loss: 1.9486 - val_acc: 0.2333\n",
      "Epoch 354/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5275 - acc: 0.3743 - val_loss: 1.9401 - val_acc: 0.2200\n",
      "Epoch 355/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5258 - acc: 0.3629 - val_loss: 1.9606 - val_acc: 0.2333\n",
      "Epoch 356/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5254 - acc: 0.3700 - val_loss: 1.9508 - val_acc: 0.2433\n",
      "Epoch 357/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5253 - acc: 0.3700 - val_loss: 1.9444 - val_acc: 0.2300\n",
      "Epoch 358/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5245 - acc: 0.3729 - val_loss: 1.9578 - val_acc: 0.2167\n",
      "Epoch 359/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5247 - acc: 0.3771 - val_loss: 1.9528 - val_acc: 0.2200\n",
      "Epoch 360/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5227 - acc: 0.3714 - val_loss: 1.9545 - val_acc: 0.2233\n",
      "Epoch 361/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5226 - acc: 0.3729 - val_loss: 1.9539 - val_acc: 0.2333\n",
      "Epoch 362/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5235 - acc: 0.3686 - val_loss: 1.9560 - val_acc: 0.2200\n",
      "Epoch 363/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5206 - acc: 0.3771 - val_loss: 1.9615 - val_acc: 0.2333\n",
      "Epoch 364/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5218 - acc: 0.3600 - val_loss: 1.9413 - val_acc: 0.2267\n",
      "Epoch 365/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5214 - acc: 0.3814 - val_loss: 1.9619 - val_acc: 0.2200\n",
      "Epoch 366/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5211 - acc: 0.3771 - val_loss: 1.9450 - val_acc: 0.2200\n",
      "Epoch 367/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5210 - acc: 0.3786 - val_loss: 1.9541 - val_acc: 0.2267\n",
      "Epoch 368/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5196 - acc: 0.3829 - val_loss: 1.9569 - val_acc: 0.2300\n",
      "Epoch 369/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5193 - acc: 0.3729 - val_loss: 1.9387 - val_acc: 0.2367\n",
      "Epoch 370/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5185 - acc: 0.3700 - val_loss: 1.9580 - val_acc: 0.2333\n",
      "Epoch 371/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5192 - acc: 0.3657 - val_loss: 1.9584 - val_acc: 0.2200\n",
      "Epoch 372/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5179 - acc: 0.3771 - val_loss: 1.9639 - val_acc: 0.2267\n",
      "Epoch 373/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5188 - acc: 0.3743 - val_loss: 1.9600 - val_acc: 0.2167\n",
      "Epoch 374/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5179 - acc: 0.3657 - val_loss: 1.9539 - val_acc: 0.2300\n",
      "Epoch 375/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5165 - acc: 0.3843 - val_loss: 1.9440 - val_acc: 0.2267\n",
      "Epoch 376/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5165 - acc: 0.3771 - val_loss: 1.9570 - val_acc: 0.2200\n",
      "Epoch 377/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5157 - acc: 0.3729 - val_loss: 1.9742 - val_acc: 0.2233\n",
      "Epoch 378/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5162 - acc: 0.3743 - val_loss: 1.9813 - val_acc: 0.2333\n",
      "Epoch 379/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5170 - acc: 0.3700 - val_loss: 1.9542 - val_acc: 0.2400\n",
      "Epoch 380/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5145 - acc: 0.3671 - val_loss: 1.9639 - val_acc: 0.2567\n",
      "Epoch 381/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5160 - acc: 0.3743 - val_loss: 1.9754 - val_acc: 0.2133\n",
      "Epoch 382/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5143 - acc: 0.3771 - val_loss: 1.9606 - val_acc: 0.2200\n",
      "Epoch 383/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5135 - acc: 0.3814 - val_loss: 1.9690 - val_acc: 0.2400\n",
      "Epoch 384/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5142 - acc: 0.3829 - val_loss: 1.9528 - val_acc: 0.2267\n",
      "Epoch 385/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5144 - acc: 0.3814 - val_loss: 1.9602 - val_acc: 0.2233\n",
      "Epoch 386/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5135 - acc: 0.3757 - val_loss: 1.9519 - val_acc: 0.2400\n",
      "Epoch 387/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5133 - acc: 0.3814 - val_loss: 1.9623 - val_acc: 0.2300\n",
      "Epoch 388/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5120 - acc: 0.3729 - val_loss: 1.9712 - val_acc: 0.2467\n",
      "Epoch 389/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5138 - acc: 0.3786 - val_loss: 1.9549 - val_acc: 0.2267\n",
      "Epoch 390/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5101 - acc: 0.3671 - val_loss: 1.9582 - val_acc: 0.2367\n",
      "Epoch 391/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5109 - acc: 0.3857 - val_loss: 1.9591 - val_acc: 0.2400\n",
      "Epoch 392/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5115 - acc: 0.3757 - val_loss: 1.9601 - val_acc: 0.2267\n",
      "Epoch 393/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5117 - acc: 0.3771 - val_loss: 1.9699 - val_acc: 0.2233\n",
      "Epoch 394/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5108 - acc: 0.3786 - val_loss: 1.9598 - val_acc: 0.2333\n",
      "Epoch 395/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5107 - acc: 0.3829 - val_loss: 1.9592 - val_acc: 0.2233\n",
      "Epoch 396/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5101 - acc: 0.3771 - val_loss: 1.9636 - val_acc: 0.2267\n",
      "Epoch 397/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5090 - acc: 0.3857 - val_loss: 1.9769 - val_acc: 0.2300\n",
      "Epoch 398/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5103 - acc: 0.3657 - val_loss: 1.9617 - val_acc: 0.2300\n",
      "Epoch 399/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5091 - acc: 0.3729 - val_loss: 1.9813 - val_acc: 0.2300\n",
      "Epoch 400/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5092 - acc: 0.3714 - val_loss: 1.9617 - val_acc: 0.2233\n",
      "Epoch 401/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5092 - acc: 0.3686 - val_loss: 1.9661 - val_acc: 0.2367\n",
      "Epoch 402/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5070 - acc: 0.3771 - val_loss: 1.9685 - val_acc: 0.2333\n",
      "Epoch 403/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5056 - acc: 0.3843 - val_loss: 1.9935 - val_acc: 0.2233\n",
      "Epoch 404/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5067 - acc: 0.3786 - val_loss: 1.9659 - val_acc: 0.2367\n",
      "Epoch 405/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5057 - acc: 0.3800 - val_loss: 1.9853 - val_acc: 0.2267\n",
      "Epoch 406/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5069 - acc: 0.3843 - val_loss: 1.9847 - val_acc: 0.2200\n",
      "Epoch 407/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5069 - acc: 0.3843 - val_loss: 1.9735 - val_acc: 0.2400\n",
      "Epoch 408/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5066 - acc: 0.3829 - val_loss: 1.9717 - val_acc: 0.2300\n",
      "Epoch 409/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5026 - acc: 0.3757 - val_loss: 1.9815 - val_acc: 0.2367\n",
      "Epoch 410/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5052 - acc: 0.3800 - val_loss: 1.9640 - val_acc: 0.2233\n",
      "Epoch 411/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5050 - acc: 0.3771 - val_loss: 1.9847 - val_acc: 0.2267\n",
      "Epoch 412/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5047 - acc: 0.3957 - val_loss: 1.9759 - val_acc: 0.2267\n",
      "Epoch 413/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5028 - acc: 0.3771 - val_loss: 1.9786 - val_acc: 0.2367\n",
      "Epoch 414/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5042 - acc: 0.3829 - val_loss: 1.9843 - val_acc: 0.2333\n",
      "Epoch 415/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5014 - acc: 0.3714 - val_loss: 1.9845 - val_acc: 0.2400\n",
      "Epoch 416/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5026 - acc: 0.3900 - val_loss: 1.9952 - val_acc: 0.2167\n",
      "Epoch 417/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5033 - acc: 0.3771 - val_loss: 1.9844 - val_acc: 0.2267\n",
      "Epoch 418/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5017 - acc: 0.3886 - val_loss: 1.9841 - val_acc: 0.2400\n",
      "Epoch 419/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5019 - acc: 0.3800 - val_loss: 1.9728 - val_acc: 0.2267\n",
      "Epoch 420/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5018 - acc: 0.3829 - val_loss: 1.9946 - val_acc: 0.2300\n",
      "Epoch 421/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5016 - acc: 0.3886 - val_loss: 1.9742 - val_acc: 0.2400\n",
      "Epoch 422/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5006 - acc: 0.3786 - val_loss: 1.9724 - val_acc: 0.2400\n",
      "Epoch 423/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5004 - acc: 0.3857 - val_loss: 1.9872 - val_acc: 0.2400\n",
      "Epoch 424/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5003 - acc: 0.3743 - val_loss: 1.9638 - val_acc: 0.2233\n",
      "Epoch 425/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4996 - acc: 0.3943 - val_loss: 1.9660 - val_acc: 0.2367\n",
      "Epoch 426/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4988 - acc: 0.3971 - val_loss: 1.9811 - val_acc: 0.2167\n",
      "Epoch 427/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4986 - acc: 0.3843 - val_loss: 1.9892 - val_acc: 0.2600\n",
      "Epoch 428/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4990 - acc: 0.3857 - val_loss: 2.0068 - val_acc: 0.2300\n",
      "Epoch 429/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4989 - acc: 0.3786 - val_loss: 1.9715 - val_acc: 0.2333\n",
      "Epoch 430/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4983 - acc: 0.3871 - val_loss: 1.9813 - val_acc: 0.2467\n",
      "Epoch 431/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4995 - acc: 0.3943 - val_loss: 1.9891 - val_acc: 0.2367\n",
      "Epoch 432/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4982 - acc: 0.3843 - val_loss: 1.9831 - val_acc: 0.2267\n",
      "Epoch 433/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4975 - acc: 0.3800 - val_loss: 1.9859 - val_acc: 0.2400\n",
      "Epoch 434/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4982 - acc: 0.3757 - val_loss: 1.9856 - val_acc: 0.2267\n",
      "Epoch 435/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4966 - acc: 0.3929 - val_loss: 1.9850 - val_acc: 0.2333\n",
      "Epoch 436/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4980 - acc: 0.3871 - val_loss: 1.9688 - val_acc: 0.2267\n",
      "Epoch 437/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4965 - acc: 0.3900 - val_loss: 1.9875 - val_acc: 0.2300\n",
      "Epoch 438/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4957 - acc: 0.3914 - val_loss: 1.9655 - val_acc: 0.2300\n",
      "Epoch 439/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4955 - acc: 0.3786 - val_loss: 1.9810 - val_acc: 0.2233\n",
      "Epoch 440/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4967 - acc: 0.3800 - val_loss: 1.9787 - val_acc: 0.2300\n",
      "Epoch 441/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4959 - acc: 0.3886 - val_loss: 1.9830 - val_acc: 0.2267\n",
      "Epoch 442/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4955 - acc: 0.3857 - val_loss: 1.9889 - val_acc: 0.2233\n",
      "Epoch 443/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4947 - acc: 0.3857 - val_loss: 1.9832 - val_acc: 0.2400\n",
      "Epoch 444/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4956 - acc: 0.3843 - val_loss: 1.9947 - val_acc: 0.2233\n",
      "Epoch 445/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4951 - acc: 0.3814 - val_loss: 1.9873 - val_acc: 0.2333\n",
      "Epoch 446/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4940 - acc: 0.3786 - val_loss: 2.0044 - val_acc: 0.2233\n",
      "Epoch 447/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4939 - acc: 0.3857 - val_loss: 2.0150 - val_acc: 0.2267\n",
      "Epoch 448/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4936 - acc: 0.3857 - val_loss: 1.9854 - val_acc: 0.2233\n",
      "Epoch 449/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4932 - acc: 0.3971 - val_loss: 2.0029 - val_acc: 0.2367\n",
      "Epoch 450/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4942 - acc: 0.3871 - val_loss: 1.9935 - val_acc: 0.2267\n",
      "Epoch 451/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4932 - acc: 0.3829 - val_loss: 1.9980 - val_acc: 0.2233\n",
      "Epoch 452/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4925 - acc: 0.3871 - val_loss: 1.9917 - val_acc: 0.2233\n",
      "Epoch 453/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4920 - acc: 0.3843 - val_loss: 1.9878 - val_acc: 0.2367\n",
      "Epoch 454/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4922 - acc: 0.3886 - val_loss: 1.9986 - val_acc: 0.2267\n",
      "Epoch 455/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4920 - acc: 0.3786 - val_loss: 1.9842 - val_acc: 0.2300\n",
      "Epoch 456/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4913 - acc: 0.3857 - val_loss: 1.9926 - val_acc: 0.2467\n",
      "Epoch 457/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4910 - acc: 0.3900 - val_loss: 2.0000 - val_acc: 0.2300\n",
      "Epoch 458/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4917 - acc: 0.3800 - val_loss: 1.9917 - val_acc: 0.2267\n",
      "Epoch 459/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4896 - acc: 0.3957 - val_loss: 1.9946 - val_acc: 0.2433\n",
      "Epoch 460/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4915 - acc: 0.3857 - val_loss: 1.9933 - val_acc: 0.2300\n",
      "Epoch 461/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4897 - acc: 0.3929 - val_loss: 2.0020 - val_acc: 0.2300\n",
      "Epoch 462/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4898 - acc: 0.3871 - val_loss: 1.9904 - val_acc: 0.2267\n",
      "Epoch 463/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4888 - acc: 0.3843 - val_loss: 1.9986 - val_acc: 0.2433\n",
      "Epoch 464/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4886 - acc: 0.3900 - val_loss: 1.9872 - val_acc: 0.2267\n",
      "Epoch 465/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4890 - acc: 0.3871 - val_loss: 1.9907 - val_acc: 0.2300\n",
      "Epoch 466/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4884 - acc: 0.3900 - val_loss: 2.0000 - val_acc: 0.2300\n",
      "Epoch 467/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4887 - acc: 0.3829 - val_loss: 2.0014 - val_acc: 0.2300\n",
      "Epoch 468/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4881 - acc: 0.3829 - val_loss: 1.9929 - val_acc: 0.2267\n",
      "Epoch 469/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4871 - acc: 0.3914 - val_loss: 2.0190 - val_acc: 0.2333\n",
      "Epoch 470/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4886 - acc: 0.3843 - val_loss: 2.0083 - val_acc: 0.2267\n",
      "Epoch 471/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4876 - acc: 0.3914 - val_loss: 2.0066 - val_acc: 0.2233\n",
      "Epoch 472/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4862 - acc: 0.3943 - val_loss: 2.0039 - val_acc: 0.2400\n",
      "Epoch 473/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4865 - acc: 0.3829 - val_loss: 2.0037 - val_acc: 0.2367\n",
      "Epoch 474/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4855 - acc: 0.3929 - val_loss: 1.9957 - val_acc: 0.2300\n",
      "Epoch 475/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4860 - acc: 0.3900 - val_loss: 2.0028 - val_acc: 0.2200\n",
      "Epoch 476/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4850 - acc: 0.3871 - val_loss: 2.0017 - val_acc: 0.2267\n",
      "Epoch 477/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4858 - acc: 0.4000 - val_loss: 2.0076 - val_acc: 0.2233\n",
      "Epoch 478/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4854 - acc: 0.3957 - val_loss: 2.0057 - val_acc: 0.2233\n",
      "Epoch 479/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4850 - acc: 0.3843 - val_loss: 1.9958 - val_acc: 0.2267\n",
      "Epoch 480/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4844 - acc: 0.3943 - val_loss: 2.0079 - val_acc: 0.2367\n",
      "Epoch 481/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4838 - acc: 0.3914 - val_loss: 2.0149 - val_acc: 0.2300\n",
      "Epoch 482/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4827 - acc: 0.3986 - val_loss: 2.0160 - val_acc: 0.2333\n",
      "Epoch 483/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4839 - acc: 0.3971 - val_loss: 2.0107 - val_acc: 0.2400\n",
      "Epoch 484/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4831 - acc: 0.3986 - val_loss: 2.0130 - val_acc: 0.2400\n",
      "Epoch 485/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4830 - acc: 0.3971 - val_loss: 2.0139 - val_acc: 0.2267\n",
      "Epoch 486/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3886 - val_loss: 2.0041 - val_acc: 0.2433\n",
      "Epoch 487/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4831 - acc: 0.3943 - val_loss: 2.0077 - val_acc: 0.2433\n",
      "Epoch 488/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3886 - val_loss: 2.0105 - val_acc: 0.2500\n",
      "Epoch 489/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3857 - val_loss: 1.9973 - val_acc: 0.2333\n",
      "Epoch 490/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4830 - acc: 0.3900 - val_loss: 1.9968 - val_acc: 0.2333\n",
      "Epoch 491/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4826 - acc: 0.3886 - val_loss: 2.0158 - val_acc: 0.2300\n",
      "Epoch 492/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4819 - acc: 0.3914 - val_loss: 1.9973 - val_acc: 0.2367\n",
      "Epoch 493/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4819 - acc: 0.3929 - val_loss: 2.0048 - val_acc: 0.2300\n",
      "Epoch 494/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4821 - acc: 0.3943 - val_loss: 2.0025 - val_acc: 0.2200\n",
      "Epoch 495/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4815 - acc: 0.3914 - val_loss: 2.0132 - val_acc: 0.2300\n",
      "Epoch 496/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4809 - acc: 0.3914 - val_loss: 2.0099 - val_acc: 0.2433\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4793 - acc: 0.3900 - val_loss: 2.0080 - val_acc: 0.2467\n",
      "Epoch 498/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4789 - acc: 0.3829 - val_loss: 2.0401 - val_acc: 0.2333\n",
      "Epoch 499/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4797 - acc: 0.3986 - val_loss: 2.0209 - val_acc: 0.2367\n",
      "Epoch 500/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4803 - acc: 0.3929 - val_loss: 2.0186 - val_acc: 0.2367\n",
      "Epoch 501/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4802 - acc: 0.3957 - val_loss: 2.0268 - val_acc: 0.2333\n",
      "Epoch 502/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4783 - acc: 0.3886 - val_loss: 2.0152 - val_acc: 0.2267\n",
      "Epoch 503/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4788 - acc: 0.4000 - val_loss: 2.0164 - val_acc: 0.2267\n",
      "Epoch 504/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4781 - acc: 0.3943 - val_loss: 2.0226 - val_acc: 0.2267\n",
      "Epoch 505/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4781 - acc: 0.3929 - val_loss: 2.0158 - val_acc: 0.2367\n",
      "Epoch 506/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4778 - acc: 0.3929 - val_loss: 2.0106 - val_acc: 0.2267\n",
      "Epoch 507/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4792 - acc: 0.3943 - val_loss: 2.0169 - val_acc: 0.2233\n",
      "Epoch 508/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4772 - acc: 0.3871 - val_loss: 2.0207 - val_acc: 0.2267\n",
      "Epoch 509/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4766 - acc: 0.3900 - val_loss: 2.0346 - val_acc: 0.2333\n",
      "Epoch 510/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4769 - acc: 0.3900 - val_loss: 2.0191 - val_acc: 0.2267\n",
      "Epoch 511/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4766 - acc: 0.3957 - val_loss: 2.0120 - val_acc: 0.2233\n",
      "Epoch 512/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3929 - val_loss: 2.0100 - val_acc: 0.2467\n",
      "Epoch 513/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4757 - acc: 0.3929 - val_loss: 2.0171 - val_acc: 0.2433\n",
      "Epoch 514/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4754 - acc: 0.3929 - val_loss: 2.0256 - val_acc: 0.2267\n",
      "Epoch 515/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4759 - acc: 0.3943 - val_loss: 2.0262 - val_acc: 0.2300\n",
      "Epoch 516/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4768 - acc: 0.3814 - val_loss: 2.0216 - val_acc: 0.2233\n",
      "Epoch 517/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4732 - acc: 0.3943 - val_loss: 2.0330 - val_acc: 0.2533\n",
      "Epoch 518/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4767 - acc: 0.3957 - val_loss: 2.0241 - val_acc: 0.2233\n",
      "Epoch 519/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4751 - acc: 0.4014 - val_loss: 2.0205 - val_acc: 0.2300\n",
      "Epoch 520/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4745 - acc: 0.3914 - val_loss: 2.0293 - val_acc: 0.2333\n",
      "Epoch 521/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4740 - acc: 0.4086 - val_loss: 2.0080 - val_acc: 0.2367\n",
      "Epoch 522/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4753 - acc: 0.3971 - val_loss: 2.0252 - val_acc: 0.2267\n",
      "Epoch 523/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4749 - acc: 0.3914 - val_loss: 2.0276 - val_acc: 0.2267\n",
      "Epoch 524/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4739 - acc: 0.4000 - val_loss: 2.0192 - val_acc: 0.2300\n",
      "Epoch 525/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4723 - acc: 0.3943 - val_loss: 2.0238 - val_acc: 0.2433\n",
      "Epoch 526/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4739 - acc: 0.3957 - val_loss: 2.0241 - val_acc: 0.2300\n",
      "Epoch 527/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4729 - acc: 0.3914 - val_loss: 2.0220 - val_acc: 0.2300\n",
      "Epoch 528/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4721 - acc: 0.3943 - val_loss: 2.0256 - val_acc: 0.2267\n",
      "Epoch 529/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4710 - acc: 0.3857 - val_loss: 2.0271 - val_acc: 0.2300\n",
      "Epoch 530/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4714 - acc: 0.3986 - val_loss: 2.0395 - val_acc: 0.2333\n",
      "Epoch 531/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4716 - acc: 0.3886 - val_loss: 2.0214 - val_acc: 0.2433\n",
      "Epoch 532/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4720 - acc: 0.4000 - val_loss: 2.0264 - val_acc: 0.2433\n",
      "Epoch 533/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4711 - acc: 0.4000 - val_loss: 2.0228 - val_acc: 0.2267\n",
      "Epoch 534/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4718 - acc: 0.3929 - val_loss: 2.0227 - val_acc: 0.2333\n",
      "Epoch 535/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4704 - acc: 0.3943 - val_loss: 2.0433 - val_acc: 0.2367\n",
      "Epoch 536/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4699 - acc: 0.3971 - val_loss: 2.0464 - val_acc: 0.2233\n",
      "Epoch 537/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4707 - acc: 0.3957 - val_loss: 2.0248 - val_acc: 0.2300\n",
      "Epoch 538/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4699 - acc: 0.3886 - val_loss: 2.0356 - val_acc: 0.2333\n",
      "Epoch 539/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4722 - acc: 0.3886 - val_loss: 2.0221 - val_acc: 0.2233\n",
      "Epoch 540/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4706 - acc: 0.3929 - val_loss: 2.0373 - val_acc: 0.2267\n",
      "Epoch 541/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3971 - val_loss: 2.0339 - val_acc: 0.2267\n",
      "Epoch 542/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4697 - acc: 0.4043 - val_loss: 2.0381 - val_acc: 0.2367\n",
      "Epoch 543/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4701 - acc: 0.3957 - val_loss: 2.0309 - val_acc: 0.2333\n",
      "Epoch 544/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4687 - acc: 0.3929 - val_loss: 2.0185 - val_acc: 0.2367\n",
      "Epoch 545/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3971 - val_loss: 2.0245 - val_acc: 0.2333\n",
      "Epoch 546/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4667 - acc: 0.3957 - val_loss: 2.0415 - val_acc: 0.2400\n",
      "Epoch 547/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4688 - acc: 0.3986 - val_loss: 2.0481 - val_acc: 0.2333\n",
      "Epoch 548/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4675 - acc: 0.3929 - val_loss: 2.0227 - val_acc: 0.2267\n",
      "Epoch 549/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.4014 - val_loss: 2.0206 - val_acc: 0.2267\n",
      "Epoch 550/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4671 - acc: 0.4000 - val_loss: 2.0328 - val_acc: 0.2300\n",
      "Epoch 551/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.3886 - val_loss: 2.0333 - val_acc: 0.2233\n",
      "Epoch 552/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4663 - acc: 0.4057 - val_loss: 2.0291 - val_acc: 0.2300\n",
      "Epoch 553/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4688 - acc: 0.3929 - val_loss: 2.0339 - val_acc: 0.2300\n",
      "Epoch 554/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4657 - acc: 0.3986 - val_loss: 2.0403 - val_acc: 0.2333\n",
      "Epoch 555/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4672 - acc: 0.3929 - val_loss: 2.0387 - val_acc: 0.2333\n",
      "Epoch 556/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4654 - acc: 0.3943 - val_loss: 2.0345 - val_acc: 0.2433\n",
      "Epoch 557/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4647 - acc: 0.4071 - val_loss: 2.0512 - val_acc: 0.2567\n",
      "Epoch 558/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4664 - acc: 0.3986 - val_loss: 2.0337 - val_acc: 0.2333\n",
      "Epoch 559/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4645 - acc: 0.4043 - val_loss: 2.0338 - val_acc: 0.2500\n",
      "Epoch 560/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.4043 - val_loss: 2.0381 - val_acc: 0.2567\n",
      "Epoch 561/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4656 - acc: 0.3971 - val_loss: 2.0294 - val_acc: 0.2300\n",
      "Epoch 562/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4652 - acc: 0.3986 - val_loss: 2.0285 - val_acc: 0.2233\n",
      "Epoch 563/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4645 - acc: 0.3943 - val_loss: 2.0287 - val_acc: 0.2267\n",
      "Epoch 564/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4643 - acc: 0.4086 - val_loss: 2.0459 - val_acc: 0.2267\n",
      "Epoch 565/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4638 - acc: 0.3971 - val_loss: 2.0321 - val_acc: 0.2333\n",
      "Epoch 566/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4655 - acc: 0.4000 - val_loss: 2.0354 - val_acc: 0.2300\n",
      "Epoch 567/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4631 - acc: 0.4100 - val_loss: 2.0474 - val_acc: 0.2233\n",
      "Epoch 568/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4634 - acc: 0.4029 - val_loss: 2.0423 - val_acc: 0.2333\n",
      "Epoch 569/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4626 - acc: 0.4029 - val_loss: 2.0365 - val_acc: 0.2400\n",
      "Epoch 570/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4623 - acc: 0.4029 - val_loss: 2.0444 - val_acc: 0.2367\n",
      "Epoch 571/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3986 - val_loss: 2.0453 - val_acc: 0.2267\n",
      "Epoch 572/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4612 - acc: 0.3929 - val_loss: 2.0384 - val_acc: 0.2400\n",
      "Epoch 573/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4071 - val_loss: 2.0497 - val_acc: 0.2433\n",
      "Epoch 574/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.3986 - val_loss: 2.0365 - val_acc: 0.2500\n",
      "Epoch 575/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4633 - acc: 0.4057 - val_loss: 2.0369 - val_acc: 0.2267\n",
      "Epoch 576/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3957 - val_loss: 2.0411 - val_acc: 0.2233\n",
      "Epoch 577/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4617 - acc: 0.3986 - val_loss: 2.0384 - val_acc: 0.2267\n",
      "Epoch 578/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4613 - acc: 0.4029 - val_loss: 2.0397 - val_acc: 0.2333\n",
      "Epoch 579/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4618 - acc: 0.4057 - val_loss: 2.0361 - val_acc: 0.2267\n",
      "Epoch 580/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4610 - acc: 0.4057 - val_loss: 2.0444 - val_acc: 0.2367\n",
      "Epoch 581/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4604 - acc: 0.3971 - val_loss: 2.0383 - val_acc: 0.2533\n",
      "Epoch 582/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4604 - acc: 0.4029 - val_loss: 2.0492 - val_acc: 0.2300\n",
      "Epoch 583/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4598 - acc: 0.3957 - val_loss: 2.0472 - val_acc: 0.2267\n",
      "Epoch 584/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4014 - val_loss: 2.0433 - val_acc: 0.2300\n",
      "Epoch 585/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4590 - acc: 0.4086 - val_loss: 2.0506 - val_acc: 0.2367\n",
      "Epoch 586/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4585 - acc: 0.3943 - val_loss: 2.0499 - val_acc: 0.2467\n",
      "Epoch 587/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4057 - val_loss: 2.0497 - val_acc: 0.2333\n",
      "Epoch 588/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4596 - acc: 0.4014 - val_loss: 2.0522 - val_acc: 0.2333\n",
      "Epoch 589/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4587 - acc: 0.3986 - val_loss: 2.0517 - val_acc: 0.2267\n",
      "Epoch 590/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4590 - acc: 0.3971 - val_loss: 2.0437 - val_acc: 0.2267\n",
      "Epoch 591/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4588 - acc: 0.4043 - val_loss: 2.0536 - val_acc: 0.2267\n",
      "Epoch 592/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4580 - acc: 0.4029 - val_loss: 2.0501 - val_acc: 0.2367\n",
      "Epoch 593/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4581 - acc: 0.4014 - val_loss: 2.0421 - val_acc: 0.2267\n",
      "Epoch 594/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4576 - acc: 0.4143 - val_loss: 2.0428 - val_acc: 0.2333\n",
      "Epoch 595/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4577 - acc: 0.4014 - val_loss: 2.0489 - val_acc: 0.2367\n",
      "Epoch 596/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4573 - acc: 0.4057 - val_loss: 2.0384 - val_acc: 0.2267\n",
      "Epoch 597/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4114 - val_loss: 2.0426 - val_acc: 0.2500\n",
      "Epoch 598/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4572 - acc: 0.3971 - val_loss: 2.0458 - val_acc: 0.2300\n",
      "Epoch 599/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4579 - acc: 0.4000 - val_loss: 2.0415 - val_acc: 0.2333\n",
      "Epoch 600/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4564 - acc: 0.4129 - val_loss: 2.0538 - val_acc: 0.2300\n",
      "Epoch 601/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4559 - acc: 0.4129 - val_loss: 2.0529 - val_acc: 0.2333\n",
      "Epoch 602/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4561 - acc: 0.3971 - val_loss: 2.0532 - val_acc: 0.2333\n",
      "Epoch 603/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4565 - acc: 0.4071 - val_loss: 2.0421 - val_acc: 0.2433\n",
      "Epoch 604/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4000 - val_loss: 2.0420 - val_acc: 0.2400\n",
      "Epoch 605/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4551 - acc: 0.4000 - val_loss: 2.0486 - val_acc: 0.2367\n",
      "Epoch 606/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4554 - acc: 0.4086 - val_loss: 2.0414 - val_acc: 0.2333\n",
      "Epoch 607/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4556 - acc: 0.4100 - val_loss: 2.0478 - val_acc: 0.2300\n",
      "Epoch 608/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4553 - acc: 0.4086 - val_loss: 2.0408 - val_acc: 0.2400\n",
      "Epoch 609/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4536 - acc: 0.4086 - val_loss: 2.0567 - val_acc: 0.2567\n",
      "Epoch 610/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4546 - acc: 0.4086 - val_loss: 2.0572 - val_acc: 0.2300\n",
      "Epoch 611/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4544 - acc: 0.4043 - val_loss: 2.0579 - val_acc: 0.2300\n",
      "Epoch 612/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4545 - acc: 0.4000 - val_loss: 2.0523 - val_acc: 0.2400\n",
      "Epoch 613/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4542 - acc: 0.4086 - val_loss: 2.0612 - val_acc: 0.2267\n",
      "Epoch 614/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4531 - acc: 0.4114 - val_loss: 2.0606 - val_acc: 0.2433\n",
      "Epoch 615/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4540 - acc: 0.4029 - val_loss: 2.0482 - val_acc: 0.2467\n",
      "Epoch 616/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4534 - acc: 0.4143 - val_loss: 2.0650 - val_acc: 0.2267\n",
      "Epoch 617/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4100 - val_loss: 2.0586 - val_acc: 0.2567\n",
      "Epoch 618/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4531 - acc: 0.4043 - val_loss: 2.0525 - val_acc: 0.2233\n",
      "Epoch 619/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4114 - val_loss: 2.0606 - val_acc: 0.2367\n",
      "Epoch 620/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4524 - acc: 0.4086 - val_loss: 2.0519 - val_acc: 0.2333\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4525 - acc: 0.4143 - val_loss: 2.0658 - val_acc: 0.2333\n",
      "Epoch 622/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4086 - val_loss: 2.0651 - val_acc: 0.2367\n",
      "Epoch 623/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4143 - val_loss: 2.0704 - val_acc: 0.2300\n",
      "Epoch 624/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4519 - acc: 0.4114 - val_loss: 2.0634 - val_acc: 0.2333\n",
      "Epoch 625/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4043 - val_loss: 2.0651 - val_acc: 0.2433\n",
      "Epoch 626/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4014 - val_loss: 2.0456 - val_acc: 0.2333\n",
      "Epoch 627/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4186 - val_loss: 2.0544 - val_acc: 0.2333\n",
      "Epoch 628/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4057 - val_loss: 2.0459 - val_acc: 0.2400\n",
      "Epoch 629/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4514 - acc: 0.3986 - val_loss: 2.0618 - val_acc: 0.2367\n",
      "Epoch 630/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4086 - val_loss: 2.0468 - val_acc: 0.2333\n",
      "Epoch 631/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4114 - val_loss: 2.0740 - val_acc: 0.2400\n",
      "Epoch 632/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4057 - val_loss: 2.0663 - val_acc: 0.2367\n",
      "Epoch 633/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4498 - acc: 0.4157 - val_loss: 2.0732 - val_acc: 0.2400\n",
      "Epoch 634/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4143 - val_loss: 2.0592 - val_acc: 0.2333\n",
      "Epoch 635/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4483 - acc: 0.4000 - val_loss: 2.0570 - val_acc: 0.2467\n",
      "Epoch 636/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4503 - acc: 0.4143 - val_loss: 2.0609 - val_acc: 0.2333\n",
      "Epoch 637/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4492 - acc: 0.4057 - val_loss: 2.0605 - val_acc: 0.2333\n",
      "Epoch 638/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4492 - acc: 0.4143 - val_loss: 2.0728 - val_acc: 0.2300\n",
      "Epoch 639/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4489 - acc: 0.4086 - val_loss: 2.0723 - val_acc: 0.2300\n",
      "Epoch 640/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4114 - val_loss: 2.0513 - val_acc: 0.2333\n",
      "Epoch 641/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4476 - acc: 0.4100 - val_loss: 2.0674 - val_acc: 0.2367\n",
      "Epoch 642/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4490 - acc: 0.4071 - val_loss: 2.0734 - val_acc: 0.2400\n",
      "Epoch 643/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4477 - acc: 0.4086 - val_loss: 2.0773 - val_acc: 0.2333\n",
      "Epoch 644/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4071 - val_loss: 2.0660 - val_acc: 0.2400\n",
      "Epoch 645/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4471 - acc: 0.4114 - val_loss: 2.0762 - val_acc: 0.2400\n",
      "Epoch 646/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4451 - acc: 0.4086 - val_loss: 2.0647 - val_acc: 0.2600\n",
      "Epoch 647/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4465 - acc: 0.4143 - val_loss: 2.0648 - val_acc: 0.2367\n",
      "Epoch 648/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4086 - val_loss: 2.0775 - val_acc: 0.2367\n",
      "Epoch 649/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4455 - acc: 0.4000 - val_loss: 2.0784 - val_acc: 0.2367\n",
      "Epoch 650/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4473 - acc: 0.4100 - val_loss: 2.0645 - val_acc: 0.2367\n",
      "Epoch 651/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4460 - acc: 0.4086 - val_loss: 2.0841 - val_acc: 0.2433\n",
      "Epoch 652/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4470 - acc: 0.4129 - val_loss: 2.0657 - val_acc: 0.2300\n",
      "Epoch 653/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4454 - acc: 0.4157 - val_loss: 2.0683 - val_acc: 0.2467\n",
      "Epoch 654/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4457 - acc: 0.4100 - val_loss: 2.0579 - val_acc: 0.2467\n",
      "Epoch 655/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4114 - val_loss: 2.0689 - val_acc: 0.2467\n",
      "Epoch 656/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4467 - acc: 0.4071 - val_loss: 2.0813 - val_acc: 0.2400\n",
      "Epoch 657/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4100 - val_loss: 2.0706 - val_acc: 0.2300\n",
      "Epoch 658/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4449 - acc: 0.4086 - val_loss: 2.0695 - val_acc: 0.2400\n",
      "Epoch 659/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4445 - acc: 0.4129 - val_loss: 2.0709 - val_acc: 0.2433\n",
      "Epoch 660/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4057 - val_loss: 2.0697 - val_acc: 0.2333\n",
      "Epoch 661/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4459 - acc: 0.4086 - val_loss: 2.0675 - val_acc: 0.2300\n",
      "Epoch 662/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4451 - acc: 0.4043 - val_loss: 2.0718 - val_acc: 0.2300\n",
      "Epoch 663/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4100 - val_loss: 2.0853 - val_acc: 0.2367\n",
      "Epoch 664/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4436 - acc: 0.4129 - val_loss: 2.0736 - val_acc: 0.2400\n",
      "Epoch 665/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4439 - acc: 0.4129 - val_loss: 2.0806 - val_acc: 0.2400\n",
      "Epoch 666/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4431 - acc: 0.4143 - val_loss: 2.0717 - val_acc: 0.2467\n",
      "Epoch 667/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4447 - acc: 0.4100 - val_loss: 2.0655 - val_acc: 0.2333\n",
      "Epoch 668/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4414 - acc: 0.4114 - val_loss: 2.0825 - val_acc: 0.2600\n",
      "Epoch 669/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4423 - acc: 0.4171 - val_loss: 2.0850 - val_acc: 0.2367\n",
      "Epoch 670/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4436 - acc: 0.4143 - val_loss: 2.0735 - val_acc: 0.2333\n",
      "Epoch 671/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4419 - acc: 0.4143 - val_loss: 2.0747 - val_acc: 0.2467\n",
      "Epoch 672/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4426 - acc: 0.4057 - val_loss: 2.0772 - val_acc: 0.2333\n",
      "Epoch 673/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4427 - acc: 0.4114 - val_loss: 2.0737 - val_acc: 0.2367\n",
      "Epoch 674/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4418 - acc: 0.4057 - val_loss: 2.0961 - val_acc: 0.2400\n",
      "Epoch 675/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4409 - acc: 0.4157 - val_loss: 2.0935 - val_acc: 0.2567\n",
      "Epoch 676/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4410 - acc: 0.4086 - val_loss: 2.0663 - val_acc: 0.2567\n",
      "Epoch 677/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4401 - acc: 0.4100 - val_loss: 2.0922 - val_acc: 0.2367\n",
      "Epoch 678/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4396 - acc: 0.4157 - val_loss: 2.0684 - val_acc: 0.2300\n",
      "Epoch 679/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4411 - acc: 0.4143 - val_loss: 2.0831 - val_acc: 0.2400\n",
      "Epoch 680/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4421 - acc: 0.4143 - val_loss: 2.0811 - val_acc: 0.2367\n",
      "Epoch 681/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4416 - acc: 0.4129 - val_loss: 2.0773 - val_acc: 0.2300\n",
      "Epoch 682/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4402 - acc: 0.4157 - val_loss: 2.0773 - val_acc: 0.2567\n",
      "Epoch 683/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4401 - acc: 0.4057 - val_loss: 2.0673 - val_acc: 0.2500\n",
      "Epoch 684/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4368 - acc: 0.4129 - val_loss: 2.0809 - val_acc: 0.2567\n",
      "Epoch 685/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4398 - acc: 0.4100 - val_loss: 2.0741 - val_acc: 0.2467\n",
      "Epoch 686/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4396 - acc: 0.4129 - val_loss: 2.0972 - val_acc: 0.2500\n",
      "Epoch 687/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4143 - val_loss: 2.0703 - val_acc: 0.2367\n",
      "Epoch 688/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4401 - acc: 0.4157 - val_loss: 2.0801 - val_acc: 0.2333\n",
      "Epoch 689/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4392 - acc: 0.4129 - val_loss: 2.0830 - val_acc: 0.2433\n",
      "Epoch 690/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4389 - acc: 0.4186 - val_loss: 2.0878 - val_acc: 0.2333\n",
      "Epoch 691/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4379 - acc: 0.4200 - val_loss: 2.0881 - val_acc: 0.2367\n",
      "Epoch 692/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4383 - acc: 0.4157 - val_loss: 2.0798 - val_acc: 0.2367\n",
      "Epoch 693/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4391 - acc: 0.4100 - val_loss: 2.0777 - val_acc: 0.2367\n",
      "Epoch 694/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4377 - acc: 0.4157 - val_loss: 2.1021 - val_acc: 0.2467\n",
      "Epoch 695/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4392 - acc: 0.4114 - val_loss: 2.0797 - val_acc: 0.2533\n",
      "Epoch 696/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4171 - val_loss: 2.0920 - val_acc: 0.2400\n",
      "Epoch 697/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4380 - acc: 0.4143 - val_loss: 2.0963 - val_acc: 0.2500\n",
      "Epoch 698/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4377 - acc: 0.4143 - val_loss: 2.0842 - val_acc: 0.2467\n",
      "Epoch 699/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4372 - acc: 0.4200 - val_loss: 2.0874 - val_acc: 0.2333\n",
      "Epoch 700/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4370 - acc: 0.4143 - val_loss: 2.0869 - val_acc: 0.2500\n",
      "Epoch 701/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4361 - acc: 0.4186 - val_loss: 2.0820 - val_acc: 0.2467\n",
      "Epoch 702/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4370 - acc: 0.4157 - val_loss: 2.0888 - val_acc: 0.2367\n",
      "Epoch 703/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4229 - val_loss: 2.0957 - val_acc: 0.2367\n",
      "Epoch 704/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4367 - acc: 0.4200 - val_loss: 2.0828 - val_acc: 0.2467\n",
      "Epoch 705/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4086 - val_loss: 2.0786 - val_acc: 0.2367\n",
      "Epoch 706/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4200 - val_loss: 2.0722 - val_acc: 0.2400\n",
      "Epoch 707/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4214 - val_loss: 2.0937 - val_acc: 0.2367\n",
      "Epoch 708/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4357 - acc: 0.4157 - val_loss: 2.0844 - val_acc: 0.2367\n",
      "Epoch 709/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4339 - acc: 0.4243 - val_loss: 2.1036 - val_acc: 0.2533\n",
      "Epoch 710/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4171 - val_loss: 2.0840 - val_acc: 0.2433\n",
      "Epoch 711/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4347 - acc: 0.4129 - val_loss: 2.0845 - val_acc: 0.2400\n",
      "Epoch 712/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4343 - acc: 0.4114 - val_loss: 2.0924 - val_acc: 0.2567\n",
      "Epoch 713/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4354 - acc: 0.4129 - val_loss: 2.0770 - val_acc: 0.2367\n",
      "Epoch 714/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4348 - acc: 0.4214 - val_loss: 2.0859 - val_acc: 0.2400\n",
      "Epoch 715/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4325 - acc: 0.4214 - val_loss: 2.0836 - val_acc: 0.2567\n",
      "Epoch 716/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4350 - acc: 0.4129 - val_loss: 2.0931 - val_acc: 0.2433\n",
      "Epoch 717/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4341 - acc: 0.4171 - val_loss: 2.0926 - val_acc: 0.2333\n",
      "Epoch 718/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4335 - acc: 0.4171 - val_loss: 2.1118 - val_acc: 0.2600\n",
      "Epoch 719/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4344 - acc: 0.4157 - val_loss: 2.1045 - val_acc: 0.2333\n",
      "Epoch 720/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4339 - acc: 0.4186 - val_loss: 2.0938 - val_acc: 0.2433\n",
      "Epoch 721/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4336 - acc: 0.4143 - val_loss: 2.0905 - val_acc: 0.2467\n",
      "Epoch 722/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4330 - acc: 0.4171 - val_loss: 2.0952 - val_acc: 0.2400\n",
      "Epoch 723/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4328 - acc: 0.4214 - val_loss: 2.0847 - val_acc: 0.2367\n",
      "Epoch 724/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4327 - acc: 0.4200 - val_loss: 2.0977 - val_acc: 0.2367\n",
      "Epoch 725/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4321 - acc: 0.4157 - val_loss: 2.1060 - val_acc: 0.2467\n",
      "Epoch 726/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4321 - acc: 0.4171 - val_loss: 2.0930 - val_acc: 0.2367\n",
      "Epoch 727/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4200 - val_loss: 2.1165 - val_acc: 0.2367\n",
      "Epoch 728/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4322 - acc: 0.4157 - val_loss: 2.0851 - val_acc: 0.2333\n",
      "Epoch 729/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4314 - acc: 0.4243 - val_loss: 2.1092 - val_acc: 0.2500\n",
      "Epoch 730/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4315 - acc: 0.4229 - val_loss: 2.1137 - val_acc: 0.2433\n",
      "Epoch 731/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4157 - val_loss: 2.0991 - val_acc: 0.2400\n",
      "Epoch 732/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4304 - acc: 0.4229 - val_loss: 2.0988 - val_acc: 0.2400\n",
      "Epoch 733/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4310 - acc: 0.4214 - val_loss: 2.0994 - val_acc: 0.2400\n",
      "Epoch 734/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4319 - acc: 0.4143 - val_loss: 2.1049 - val_acc: 0.2367\n",
      "Epoch 735/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4307 - acc: 0.4186 - val_loss: 2.0983 - val_acc: 0.2367\n",
      "Epoch 736/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4292 - acc: 0.4200 - val_loss: 2.0929 - val_acc: 0.2433\n",
      "Epoch 737/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4312 - acc: 0.4229 - val_loss: 2.1075 - val_acc: 0.2433\n",
      "Epoch 738/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4308 - acc: 0.4200 - val_loss: 2.1008 - val_acc: 0.2467\n",
      "Epoch 739/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4293 - acc: 0.4200 - val_loss: 2.1027 - val_acc: 0.2400\n",
      "Epoch 740/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4295 - acc: 0.4229 - val_loss: 2.1004 - val_acc: 0.2433\n",
      "Epoch 741/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4289 - acc: 0.4229 - val_loss: 2.1081 - val_acc: 0.2500\n",
      "Epoch 742/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4292 - acc: 0.4157 - val_loss: 2.0990 - val_acc: 0.2500\n",
      "Epoch 743/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4278 - acc: 0.4257 - val_loss: 2.0941 - val_acc: 0.2567\n",
      "Epoch 744/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4298 - acc: 0.4243 - val_loss: 2.0951 - val_acc: 0.2400\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4264 - acc: 0.4314 - val_loss: 2.1076 - val_acc: 0.2600\n",
      "Epoch 746/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4294 - acc: 0.4257 - val_loss: 2.1024 - val_acc: 0.2367\n",
      "Epoch 747/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4291 - acc: 0.4200 - val_loss: 2.1139 - val_acc: 0.2467\n",
      "Epoch 748/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4271 - acc: 0.4186 - val_loss: 2.1171 - val_acc: 0.2567\n",
      "Epoch 749/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4259 - acc: 0.4214 - val_loss: 2.0876 - val_acc: 0.2367\n",
      "Epoch 750/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4281 - acc: 0.4286 - val_loss: 2.0931 - val_acc: 0.2367\n",
      "Epoch 751/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4266 - acc: 0.4171 - val_loss: 2.1053 - val_acc: 0.2400\n",
      "Epoch 752/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4274 - acc: 0.4257 - val_loss: 2.1051 - val_acc: 0.2367\n",
      "Epoch 753/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4229 - val_loss: 2.1263 - val_acc: 0.2433\n",
      "Epoch 754/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4276 - acc: 0.4214 - val_loss: 2.1112 - val_acc: 0.2367\n",
      "Epoch 755/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4271 - val_loss: 2.1066 - val_acc: 0.2500\n",
      "Epoch 756/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4261 - acc: 0.4129 - val_loss: 2.1067 - val_acc: 0.2400\n",
      "Epoch 757/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4273 - acc: 0.4129 - val_loss: 2.1125 - val_acc: 0.2467\n",
      "Epoch 758/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4260 - acc: 0.4229 - val_loss: 2.1071 - val_acc: 0.2467\n",
      "Epoch 759/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4262 - acc: 0.4257 - val_loss: 2.1035 - val_acc: 0.2367\n",
      "Epoch 760/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4258 - acc: 0.4300 - val_loss: 2.1077 - val_acc: 0.2400\n",
      "Epoch 761/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4186 - val_loss: 2.1246 - val_acc: 0.2467\n",
      "Epoch 762/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4264 - acc: 0.4271 - val_loss: 2.1156 - val_acc: 0.2400\n",
      "Epoch 763/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4214 - val_loss: 2.1019 - val_acc: 0.2367\n",
      "Epoch 764/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4257 - val_loss: 2.1193 - val_acc: 0.2467\n",
      "Epoch 765/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4157 - val_loss: 2.1291 - val_acc: 0.2367\n",
      "Epoch 766/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4157 - val_loss: 2.1049 - val_acc: 0.2400\n",
      "Epoch 767/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4257 - val_loss: 2.1073 - val_acc: 0.2400\n",
      "Epoch 768/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4243 - acc: 0.4300 - val_loss: 2.1002 - val_acc: 0.2400\n",
      "Epoch 769/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4244 - acc: 0.4214 - val_loss: 2.1080 - val_acc: 0.2367\n",
      "Epoch 770/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4241 - acc: 0.4229 - val_loss: 2.0993 - val_acc: 0.2367\n",
      "Epoch 771/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4233 - acc: 0.4186 - val_loss: 2.1200 - val_acc: 0.2400\n",
      "Epoch 772/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4214 - val_loss: 2.1164 - val_acc: 0.2367\n",
      "Epoch 773/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4239 - acc: 0.4171 - val_loss: 2.1264 - val_acc: 0.2500\n",
      "Epoch 774/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4229 - acc: 0.4171 - val_loss: 2.1181 - val_acc: 0.2433\n",
      "Epoch 775/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4219 - acc: 0.4286 - val_loss: 2.1198 - val_acc: 0.2533\n",
      "Epoch 776/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4229 - acc: 0.4214 - val_loss: 2.1192 - val_acc: 0.2433\n",
      "Epoch 777/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4218 - acc: 0.4243 - val_loss: 2.1287 - val_acc: 0.2400\n",
      "Epoch 778/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4231 - acc: 0.4186 - val_loss: 2.1158 - val_acc: 0.2367\n",
      "Epoch 779/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4232 - acc: 0.4271 - val_loss: 2.1161 - val_acc: 0.2400\n",
      "Epoch 780/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4226 - acc: 0.4214 - val_loss: 2.1069 - val_acc: 0.2433\n",
      "Epoch 781/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4235 - acc: 0.4200 - val_loss: 2.1128 - val_acc: 0.2467\n",
      "Epoch 782/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4223 - acc: 0.4186 - val_loss: 2.0994 - val_acc: 0.2433\n",
      "Epoch 783/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4226 - acc: 0.4271 - val_loss: 2.1163 - val_acc: 0.2433\n",
      "Epoch 784/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4220 - acc: 0.4271 - val_loss: 2.1207 - val_acc: 0.2400\n",
      "Epoch 785/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4219 - acc: 0.4271 - val_loss: 2.1296 - val_acc: 0.2400\n",
      "Epoch 786/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4218 - acc: 0.4243 - val_loss: 2.1164 - val_acc: 0.2500\n",
      "Epoch 787/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4223 - acc: 0.4200 - val_loss: 2.1036 - val_acc: 0.2433\n",
      "Epoch 788/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4215 - acc: 0.4271 - val_loss: 2.1158 - val_acc: 0.2400\n",
      "Epoch 789/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4212 - acc: 0.4243 - val_loss: 2.1131 - val_acc: 0.2433\n",
      "Epoch 790/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4215 - acc: 0.4257 - val_loss: 2.1175 - val_acc: 0.2433\n",
      "Epoch 791/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4204 - acc: 0.4214 - val_loss: 2.1179 - val_acc: 0.2500\n",
      "Epoch 792/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4210 - acc: 0.4200 - val_loss: 2.1080 - val_acc: 0.2400\n",
      "Epoch 793/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4202 - acc: 0.4286 - val_loss: 2.1242 - val_acc: 0.2500\n",
      "Epoch 794/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4208 - acc: 0.4229 - val_loss: 2.1238 - val_acc: 0.2400\n",
      "Epoch 795/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4205 - acc: 0.4271 - val_loss: 2.1174 - val_acc: 0.2400\n",
      "Epoch 796/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4257 - val_loss: 2.1183 - val_acc: 0.2367\n",
      "Epoch 797/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4201 - acc: 0.4214 - val_loss: 2.1247 - val_acc: 0.2367\n",
      "Epoch 798/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4197 - acc: 0.4329 - val_loss: 2.1252 - val_acc: 0.2400\n",
      "Epoch 799/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4286 - val_loss: 2.1161 - val_acc: 0.2533\n",
      "Epoch 800/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4196 - acc: 0.4200 - val_loss: 2.1271 - val_acc: 0.2400\n",
      "Epoch 801/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4243 - val_loss: 2.1237 - val_acc: 0.2367\n",
      "Epoch 802/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4199 - acc: 0.4257 - val_loss: 2.1559 - val_acc: 0.2433\n",
      "Epoch 803/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4182 - acc: 0.4286 - val_loss: 2.1178 - val_acc: 0.2500\n",
      "Epoch 804/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4185 - acc: 0.4271 - val_loss: 2.1195 - val_acc: 0.2433\n",
      "Epoch 805/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4180 - acc: 0.4286 - val_loss: 2.1141 - val_acc: 0.2467\n",
      "Epoch 806/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4188 - acc: 0.4286 - val_loss: 2.1187 - val_acc: 0.2433\n",
      "Epoch 807/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4188 - acc: 0.4314 - val_loss: 2.1217 - val_acc: 0.2400\n",
      "Epoch 808/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4181 - acc: 0.4214 - val_loss: 2.1173 - val_acc: 0.2433\n",
      "Epoch 809/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4214 - val_loss: 2.1297 - val_acc: 0.2467\n",
      "Epoch 810/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4176 - acc: 0.4314 - val_loss: 2.1293 - val_acc: 0.2433\n",
      "Epoch 811/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4179 - acc: 0.4300 - val_loss: 2.1247 - val_acc: 0.2467\n",
      "Epoch 812/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4178 - acc: 0.4243 - val_loss: 2.1166 - val_acc: 0.2400\n",
      "Epoch 813/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4179 - acc: 0.4329 - val_loss: 2.1310 - val_acc: 0.2367\n",
      "Epoch 814/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4166 - acc: 0.4243 - val_loss: 2.1230 - val_acc: 0.2600\n",
      "Epoch 815/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4177 - acc: 0.4243 - val_loss: 2.1263 - val_acc: 0.2467\n",
      "Epoch 816/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4171 - acc: 0.4314 - val_loss: 2.1186 - val_acc: 0.2433\n",
      "Epoch 817/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4156 - acc: 0.4229 - val_loss: 2.1238 - val_acc: 0.2400\n",
      "Epoch 818/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4161 - acc: 0.4271 - val_loss: 2.1343 - val_acc: 0.2400\n",
      "Epoch 819/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4371 - val_loss: 2.1379 - val_acc: 0.2633\n",
      "Epoch 820/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4168 - acc: 0.4300 - val_loss: 2.1307 - val_acc: 0.2600\n",
      "Epoch 821/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4169 - acc: 0.4243 - val_loss: 2.1418 - val_acc: 0.2433\n",
      "Epoch 822/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4168 - acc: 0.4257 - val_loss: 2.1265 - val_acc: 0.2467\n",
      "Epoch 823/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4152 - acc: 0.4243 - val_loss: 2.1346 - val_acc: 0.2400\n",
      "Epoch 824/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4257 - val_loss: 2.1324 - val_acc: 0.2467\n",
      "Epoch 825/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4150 - acc: 0.4329 - val_loss: 2.1432 - val_acc: 0.2467\n",
      "Epoch 826/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4160 - acc: 0.4229 - val_loss: 2.1383 - val_acc: 0.2467\n",
      "Epoch 827/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4300 - val_loss: 2.1398 - val_acc: 0.2400\n",
      "Epoch 828/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4155 - acc: 0.4343 - val_loss: 2.1256 - val_acc: 0.2400\n",
      "Epoch 829/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4146 - acc: 0.4271 - val_loss: 2.1324 - val_acc: 0.2433\n",
      "Epoch 830/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4143 - acc: 0.4314 - val_loss: 2.1344 - val_acc: 0.2433\n",
      "Epoch 831/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4141 - acc: 0.4386 - val_loss: 2.1453 - val_acc: 0.2467\n",
      "Epoch 832/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4149 - acc: 0.4329 - val_loss: 2.1489 - val_acc: 0.2400\n",
      "Epoch 833/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4148 - acc: 0.4257 - val_loss: 2.1525 - val_acc: 0.2333\n",
      "Epoch 834/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4271 - val_loss: 2.1612 - val_acc: 0.2400\n",
      "Epoch 835/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4133 - acc: 0.4243 - val_loss: 2.1333 - val_acc: 0.2400\n",
      "Epoch 836/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4134 - acc: 0.4343 - val_loss: 2.1374 - val_acc: 0.2467\n",
      "Epoch 837/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4140 - acc: 0.4243 - val_loss: 2.1284 - val_acc: 0.2367\n",
      "Epoch 838/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4271 - val_loss: 2.1371 - val_acc: 0.2467\n",
      "Epoch 839/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4124 - acc: 0.4314 - val_loss: 2.1338 - val_acc: 0.2367\n",
      "Epoch 840/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4117 - acc: 0.4314 - val_loss: 2.1294 - val_acc: 0.2467\n",
      "Epoch 841/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4343 - val_loss: 2.1353 - val_acc: 0.2433\n",
      "Epoch 842/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4134 - acc: 0.4329 - val_loss: 2.1531 - val_acc: 0.2467\n",
      "Epoch 843/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4129 - acc: 0.4300 - val_loss: 2.1340 - val_acc: 0.2400\n",
      "Epoch 844/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4119 - acc: 0.4286 - val_loss: 2.1497 - val_acc: 0.2433\n",
      "Epoch 845/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4257 - val_loss: 2.1321 - val_acc: 0.2467\n",
      "Epoch 846/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4128 - acc: 0.4300 - val_loss: 2.1377 - val_acc: 0.2433\n",
      "Epoch 847/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4257 - val_loss: 2.1429 - val_acc: 0.2433\n",
      "Epoch 848/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4117 - acc: 0.4286 - val_loss: 2.1432 - val_acc: 0.2600\n",
      "Epoch 849/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4121 - acc: 0.4286 - val_loss: 2.1436 - val_acc: 0.2467\n",
      "Epoch 850/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4108 - acc: 0.4257 - val_loss: 2.1331 - val_acc: 0.2467\n",
      "Epoch 851/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4107 - acc: 0.4214 - val_loss: 2.1349 - val_acc: 0.2367\n",
      "Epoch 852/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4124 - acc: 0.4286 - val_loss: 2.1361 - val_acc: 0.2400\n",
      "Epoch 853/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4103 - acc: 0.4300 - val_loss: 2.1393 - val_acc: 0.2400\n",
      "Epoch 854/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4085 - acc: 0.4329 - val_loss: 2.1587 - val_acc: 0.2633\n",
      "Epoch 855/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4110 - acc: 0.4286 - val_loss: 2.1357 - val_acc: 0.2433\n",
      "Epoch 856/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4314 - val_loss: 2.1537 - val_acc: 0.2467\n",
      "Epoch 857/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4108 - acc: 0.4286 - val_loss: 2.1481 - val_acc: 0.2467\n",
      "Epoch 858/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4105 - acc: 0.4286 - val_loss: 2.1421 - val_acc: 0.2400\n",
      "Epoch 859/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4088 - acc: 0.4371 - val_loss: 2.1437 - val_acc: 0.2367\n",
      "Epoch 860/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4095 - acc: 0.4300 - val_loss: 2.1630 - val_acc: 0.2500\n",
      "Epoch 861/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4101 - acc: 0.4329 - val_loss: 2.1514 - val_acc: 0.2533\n",
      "Epoch 862/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4108 - acc: 0.4229 - val_loss: 2.1572 - val_acc: 0.2467\n",
      "Epoch 863/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4093 - acc: 0.4314 - val_loss: 2.1538 - val_acc: 0.2533\n",
      "Epoch 864/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4091 - acc: 0.4314 - val_loss: 2.1476 - val_acc: 0.2533\n",
      "Epoch 865/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4343 - val_loss: 2.1564 - val_acc: 0.2667\n",
      "Epoch 866/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4086 - acc: 0.4314 - val_loss: 2.1250 - val_acc: 0.2467\n",
      "Epoch 867/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4089 - acc: 0.4343 - val_loss: 2.1550 - val_acc: 0.2400\n",
      "Epoch 868/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4094 - acc: 0.4300 - val_loss: 2.1476 - val_acc: 0.2467\n",
      "Epoch 869/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4088 - acc: 0.4329 - val_loss: 2.1464 - val_acc: 0.2433\n",
      "Epoch 870/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4071 - acc: 0.4343 - val_loss: 2.1586 - val_acc: 0.2467\n",
      "Epoch 871/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4086 - acc: 0.4300 - val_loss: 2.1658 - val_acc: 0.2433\n",
      "Epoch 872/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4077 - acc: 0.4329 - val_loss: 2.1470 - val_acc: 0.2600\n",
      "Epoch 873/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4079 - acc: 0.4286 - val_loss: 2.1412 - val_acc: 0.2467\n",
      "Epoch 874/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4271 - val_loss: 2.1410 - val_acc: 0.2433\n",
      "Epoch 875/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4072 - acc: 0.4329 - val_loss: 2.1593 - val_acc: 0.2467\n",
      "Epoch 876/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4076 - acc: 0.4286 - val_loss: 2.1393 - val_acc: 0.2500\n",
      "Epoch 877/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4073 - acc: 0.4314 - val_loss: 2.1700 - val_acc: 0.2433\n",
      "Epoch 878/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4081 - acc: 0.4257 - val_loss: 2.1477 - val_acc: 0.2433\n",
      "Epoch 879/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4371 - val_loss: 2.1574 - val_acc: 0.2333\n",
      "Epoch 880/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4088 - acc: 0.4314 - val_loss: 2.1453 - val_acc: 0.2400\n",
      "Epoch 881/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4070 - acc: 0.4329 - val_loss: 2.1746 - val_acc: 0.2467\n",
      "Epoch 882/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4073 - acc: 0.4343 - val_loss: 2.1607 - val_acc: 0.2533\n",
      "Epoch 883/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4081 - acc: 0.4200 - val_loss: 2.1390 - val_acc: 0.2433\n",
      "Epoch 884/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4065 - acc: 0.4314 - val_loss: 2.1689 - val_acc: 0.2633\n",
      "Epoch 885/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4068 - acc: 0.4271 - val_loss: 2.1328 - val_acc: 0.2433\n",
      "Epoch 886/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4073 - acc: 0.4343 - val_loss: 2.1545 - val_acc: 0.2533\n",
      "Epoch 887/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4064 - acc: 0.4357 - val_loss: 2.1538 - val_acc: 0.2467\n",
      "Epoch 888/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4065 - acc: 0.4314 - val_loss: 2.1548 - val_acc: 0.2467\n",
      "Epoch 889/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4048 - acc: 0.4386 - val_loss: 2.1651 - val_acc: 0.2600\n",
      "Epoch 890/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4066 - acc: 0.4343 - val_loss: 2.1523 - val_acc: 0.2633\n",
      "Epoch 891/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4062 - acc: 0.4286 - val_loss: 2.1608 - val_acc: 0.2533\n",
      "Epoch 892/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4062 - acc: 0.4386 - val_loss: 2.1626 - val_acc: 0.2533\n",
      "Epoch 893/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4056 - acc: 0.4329 - val_loss: 2.1669 - val_acc: 0.2533\n",
      "Epoch 894/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4058 - acc: 0.4371 - val_loss: 2.1548 - val_acc: 0.2433\n",
      "Epoch 895/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4051 - acc: 0.4286 - val_loss: 2.1655 - val_acc: 0.2533\n",
      "Epoch 896/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4329 - val_loss: 2.1643 - val_acc: 0.2667\n",
      "Epoch 897/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4056 - acc: 0.4300 - val_loss: 2.1665 - val_acc: 0.2600\n",
      "Epoch 898/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4400 - val_loss: 2.1646 - val_acc: 0.2500\n",
      "Epoch 899/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4329 - val_loss: 2.1485 - val_acc: 0.2467\n",
      "Epoch 900/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4429 - val_loss: 2.1541 - val_acc: 0.2533\n",
      "Epoch 901/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4035 - acc: 0.4371 - val_loss: 2.1698 - val_acc: 0.2667\n",
      "Epoch 902/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4041 - acc: 0.4329 - val_loss: 2.1599 - val_acc: 0.2533\n",
      "Epoch 903/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4030 - acc: 0.4343 - val_loss: 2.1740 - val_acc: 0.2600\n",
      "Epoch 904/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4357 - val_loss: 2.1525 - val_acc: 0.2467\n",
      "Epoch 905/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4046 - acc: 0.4386 - val_loss: 2.1482 - val_acc: 0.2433\n",
      "Epoch 906/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4038 - acc: 0.4329 - val_loss: 2.1463 - val_acc: 0.2467\n",
      "Epoch 907/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4042 - acc: 0.4343 - val_loss: 2.1688 - val_acc: 0.2533\n",
      "Epoch 908/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4035 - acc: 0.4329 - val_loss: 2.1621 - val_acc: 0.2467\n",
      "Epoch 909/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4033 - acc: 0.4314 - val_loss: 2.1684 - val_acc: 0.2533\n",
      "Epoch 910/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4040 - acc: 0.4314 - val_loss: 2.1494 - val_acc: 0.2433\n",
      "Epoch 911/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4371 - val_loss: 2.1677 - val_acc: 0.2433\n",
      "Epoch 912/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4034 - acc: 0.4343 - val_loss: 2.1557 - val_acc: 0.2400\n",
      "Epoch 913/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4038 - acc: 0.4400 - val_loss: 2.1743 - val_acc: 0.2467\n",
      "Epoch 914/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4023 - acc: 0.4357 - val_loss: 2.1758 - val_acc: 0.2633\n",
      "Epoch 915/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4343 - val_loss: 2.1637 - val_acc: 0.2533\n",
      "Epoch 916/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4357 - val_loss: 2.1731 - val_acc: 0.2433\n",
      "Epoch 917/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4017 - acc: 0.4429 - val_loss: 2.1643 - val_acc: 0.2433\n",
      "Epoch 918/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4314 - val_loss: 2.1680 - val_acc: 0.2600\n",
      "Epoch 919/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4022 - acc: 0.4314 - val_loss: 2.1743 - val_acc: 0.2467\n",
      "Epoch 920/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4300 - val_loss: 2.1575 - val_acc: 0.2500\n",
      "Epoch 921/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4021 - acc: 0.4314 - val_loss: 2.1595 - val_acc: 0.2433\n",
      "Epoch 922/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4371 - val_loss: 2.1721 - val_acc: 0.2533\n",
      "Epoch 923/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4003 - acc: 0.4371 - val_loss: 2.1585 - val_acc: 0.2600\n",
      "Epoch 924/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4414 - val_loss: 2.1678 - val_acc: 0.2533\n",
      "Epoch 925/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4314 - val_loss: 2.1720 - val_acc: 0.2433\n",
      "Epoch 926/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4010 - acc: 0.4429 - val_loss: 2.1634 - val_acc: 0.2533\n",
      "Epoch 927/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4014 - acc: 0.4271 - val_loss: 2.1680 - val_acc: 0.2467\n",
      "Epoch 928/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4400 - val_loss: 2.1808 - val_acc: 0.2500\n",
      "Epoch 929/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4343 - val_loss: 2.1813 - val_acc: 0.2467\n",
      "Epoch 930/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4011 - acc: 0.4371 - val_loss: 2.1722 - val_acc: 0.2533\n",
      "Epoch 931/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4006 - acc: 0.4357 - val_loss: 2.1657 - val_acc: 0.2433\n",
      "Epoch 932/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4001 - acc: 0.4414 - val_loss: 2.1566 - val_acc: 0.2500\n",
      "Epoch 933/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4300 - val_loss: 2.1691 - val_acc: 0.2500\n",
      "Epoch 934/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3997 - acc: 0.4386 - val_loss: 2.1735 - val_acc: 0.2533\n",
      "Epoch 935/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4007 - acc: 0.4371 - val_loss: 2.1726 - val_acc: 0.2500\n",
      "Epoch 936/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4002 - acc: 0.4400 - val_loss: 2.1797 - val_acc: 0.2567\n",
      "Epoch 937/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3999 - acc: 0.4314 - val_loss: 2.1733 - val_acc: 0.2433\n",
      "Epoch 938/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3991 - acc: 0.4414 - val_loss: 2.1766 - val_acc: 0.2533\n",
      "Epoch 939/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3990 - acc: 0.4357 - val_loss: 2.1868 - val_acc: 0.2533\n",
      "Epoch 940/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3999 - acc: 0.4343 - val_loss: 2.1655 - val_acc: 0.2433\n",
      "Epoch 941/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3993 - acc: 0.4343 - val_loss: 2.1697 - val_acc: 0.2467\n",
      "Epoch 942/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4386 - val_loss: 2.1552 - val_acc: 0.2367\n",
      "Epoch 943/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3993 - acc: 0.4386 - val_loss: 2.1800 - val_acc: 0.2467\n",
      "Epoch 944/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3981 - acc: 0.4371 - val_loss: 2.1822 - val_acc: 0.2533\n",
      "Epoch 945/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3984 - acc: 0.4329 - val_loss: 2.1822 - val_acc: 0.2533\n",
      "Epoch 946/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4329 - val_loss: 2.1719 - val_acc: 0.2567\n",
      "Epoch 947/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4371 - val_loss: 2.1865 - val_acc: 0.2633\n",
      "Epoch 948/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3979 - acc: 0.4286 - val_loss: 2.1632 - val_acc: 0.2400\n",
      "Epoch 949/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4400 - val_loss: 2.1654 - val_acc: 0.2433\n",
      "Epoch 950/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4386 - val_loss: 2.1728 - val_acc: 0.2433\n",
      "Epoch 951/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4343 - val_loss: 2.1818 - val_acc: 0.2567\n",
      "Epoch 952/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4386 - val_loss: 2.1802 - val_acc: 0.2467\n",
      "Epoch 953/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4400 - val_loss: 2.1801 - val_acc: 0.2600\n",
      "Epoch 954/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4386 - val_loss: 2.1750 - val_acc: 0.2567\n",
      "Epoch 955/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4414 - val_loss: 2.1881 - val_acc: 0.2500\n",
      "Epoch 956/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3969 - acc: 0.4400 - val_loss: 2.1929 - val_acc: 0.2433\n",
      "Epoch 957/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3964 - acc: 0.4371 - val_loss: 2.1735 - val_acc: 0.2467\n",
      "Epoch 958/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4343 - val_loss: 2.2062 - val_acc: 0.2433\n",
      "Epoch 959/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4371 - val_loss: 2.1677 - val_acc: 0.2433\n",
      "Epoch 960/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4329 - val_loss: 2.1856 - val_acc: 0.2467\n",
      "Epoch 961/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3960 - acc: 0.4386 - val_loss: 2.1568 - val_acc: 0.2400\n",
      "Epoch 962/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3969 - acc: 0.4357 - val_loss: 2.1819 - val_acc: 0.2467\n",
      "Epoch 963/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3962 - acc: 0.4371 - val_loss: 2.1782 - val_acc: 0.2500\n",
      "Epoch 964/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3960 - acc: 0.4414 - val_loss: 2.1776 - val_acc: 0.2533\n",
      "Epoch 965/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3948 - acc: 0.4443 - val_loss: 2.1721 - val_acc: 0.2467\n",
      "Epoch 966/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3962 - acc: 0.4429 - val_loss: 2.1992 - val_acc: 0.2500\n",
      "Epoch 967/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3963 - acc: 0.4343 - val_loss: 2.1883 - val_acc: 0.2467\n",
      "Epoch 968/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3952 - acc: 0.4343 - val_loss: 2.1757 - val_acc: 0.2400\n",
      "Epoch 969/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3955 - acc: 0.4443 - val_loss: 2.1844 - val_acc: 0.2500\n",
      "Epoch 970/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3953 - acc: 0.4386 - val_loss: 2.1952 - val_acc: 0.2400\n",
      "Epoch 971/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3955 - acc: 0.4414 - val_loss: 2.1764 - val_acc: 0.2433\n",
      "Epoch 972/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3951 - acc: 0.4400 - val_loss: 2.1862 - val_acc: 0.2433\n",
      "Epoch 973/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4371 - val_loss: 2.1855 - val_acc: 0.2533\n",
      "Epoch 974/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3949 - acc: 0.4314 - val_loss: 2.1888 - val_acc: 0.2500\n",
      "Epoch 975/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3948 - acc: 0.4400 - val_loss: 2.1972 - val_acc: 0.2533\n",
      "Epoch 976/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4414 - val_loss: 2.1802 - val_acc: 0.2467\n",
      "Epoch 977/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3944 - acc: 0.4386 - val_loss: 2.1760 - val_acc: 0.2367\n",
      "Epoch 978/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3936 - acc: 0.4471 - val_loss: 2.1859 - val_acc: 0.2500\n",
      "Epoch 979/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3945 - acc: 0.4357 - val_loss: 2.1874 - val_acc: 0.2433\n",
      "Epoch 980/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3944 - acc: 0.4400 - val_loss: 2.2000 - val_acc: 0.2500\n",
      "Epoch 981/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3932 - acc: 0.4343 - val_loss: 2.2070 - val_acc: 0.2500\n",
      "Epoch 982/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4400 - val_loss: 2.1905 - val_acc: 0.2533\n",
      "Epoch 983/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4343 - val_loss: 2.1962 - val_acc: 0.2500\n",
      "Epoch 984/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4343 - val_loss: 2.1870 - val_acc: 0.2533\n",
      "Epoch 985/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4371 - val_loss: 2.1973 - val_acc: 0.2433\n",
      "Epoch 986/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4414 - val_loss: 2.1911 - val_acc: 0.2567\n",
      "Epoch 987/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4414 - val_loss: 2.1833 - val_acc: 0.2400\n",
      "Epoch 988/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3925 - acc: 0.4457 - val_loss: 2.2109 - val_acc: 0.2600\n",
      "Epoch 989/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4371 - val_loss: 2.2076 - val_acc: 0.2500\n",
      "Epoch 990/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4429 - val_loss: 2.2208 - val_acc: 0.2467\n",
      "Epoch 991/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3929 - acc: 0.4443 - val_loss: 2.1873 - val_acc: 0.2467\n",
      "Epoch 992/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4386 - val_loss: 2.2007 - val_acc: 0.2467\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3924 - acc: 0.4500 - val_loss: 2.1931 - val_acc: 0.2433\n",
      "Epoch 994/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4343 - val_loss: 2.1745 - val_acc: 0.2333\n",
      "Epoch 995/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4386 - val_loss: 2.1976 - val_acc: 0.2467\n",
      "Epoch 996/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3917 - acc: 0.4429 - val_loss: 2.1987 - val_acc: 0.2400\n",
      "Epoch 997/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3916 - acc: 0.4443 - val_loss: 2.1984 - val_acc: 0.2500\n",
      "Epoch 998/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4443 - val_loss: 2.2108 - val_acc: 0.2500\n",
      "Epoch 999/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3921 - acc: 0.4386 - val_loss: 2.1951 - val_acc: 0.2467\n",
      "Epoch 1000/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3918 - acc: 0.4414 - val_loss: 2.2071 - val_acc: 0.2467\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNBrNgEEbLY1Go9EMGP4/RpQQKB0G/lgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272cf742518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(X_train, Y_train, epochs=1000, batch_size=10, validation_data=(X_val, Y_val))\n",
    "\n",
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2072 - acc: 0.1657 - val_loss: 2.1908 - val_acc: 0.1800\n",
      "Epoch 3/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1730 - acc: 0.1729 - val_loss: 2.1631 - val_acc: 0.1867\n",
      "Epoch 4/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1441 - acc: 0.1786 - val_loss: 2.1372 - val_acc: 0.1867\n",
      "Epoch 5/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1177 - acc: 0.1900 - val_loss: 2.1141 - val_acc: 0.1867\n",
      "Epoch 6/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0940 - acc: 0.2029 - val_loss: 2.0931 - val_acc: 0.2033\n",
      "Epoch 7/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.2086 - val_loss: 2.0727 - val_acc: 0.2067\n",
      "Epoch 8/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0521 - acc: 0.2129 - val_loss: 2.0563 - val_acc: 0.2067\n",
      "Epoch 9/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0342 - acc: 0.2157 - val_loss: 2.0409 - val_acc: 0.2033\n",
      "Epoch 10/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0188 - acc: 0.2129 - val_loss: 2.0271 - val_acc: 0.2067\n",
      "Epoch 11/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0042 - acc: 0.2200 - val_loss: 2.0125 - val_acc: 0.2100\n",
      "Epoch 12/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9912 - acc: 0.2186 - val_loss: 2.0036 - val_acc: 0.2100\n",
      "Epoch 13/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9788 - acc: 0.2271 - val_loss: 1.9953 - val_acc: 0.2100\n",
      "Epoch 14/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9684 - acc: 0.2329 - val_loss: 1.9833 - val_acc: 0.2033\n",
      "Epoch 15/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9582 - acc: 0.2214 - val_loss: 1.9753 - val_acc: 0.2067\n",
      "Epoch 16/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9484 - acc: 0.2357 - val_loss: 1.9686 - val_acc: 0.2000\n",
      "Epoch 17/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9394 - acc: 0.2343 - val_loss: 1.9613 - val_acc: 0.2033\n",
      "Epoch 18/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9309 - acc: 0.2314 - val_loss: 1.9537 - val_acc: 0.2100\n",
      "Epoch 19/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.2271 - val_loss: 1.9452 - val_acc: 0.2100\n",
      "Epoch 20/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9156 - acc: 0.2386 - val_loss: 1.9392 - val_acc: 0.2100\n",
      "Epoch 21/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9085 - acc: 0.2329 - val_loss: 1.9359 - val_acc: 0.2067\n",
      "Epoch 22/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9011 - acc: 0.2386 - val_loss: 1.9289 - val_acc: 0.2033\n",
      "Epoch 23/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8953 - acc: 0.2357 - val_loss: 1.9233 - val_acc: 0.2100\n",
      "Epoch 24/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8895 - acc: 0.2314 - val_loss: 1.9200 - val_acc: 0.2067\n",
      "Epoch 25/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8831 - acc: 0.2371 - val_loss: 1.9166 - val_acc: 0.2167\n",
      "Epoch 26/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8771 - acc: 0.2329 - val_loss: 1.9112 - val_acc: 0.2167\n",
      "Epoch 27/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8715 - acc: 0.2400 - val_loss: 1.9101 - val_acc: 0.2167\n",
      "Epoch 28/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8663 - acc: 0.2386 - val_loss: 1.9095 - val_acc: 0.2000\n",
      "Epoch 29/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2414 - val_loss: 1.9055 - val_acc: 0.1900\n",
      "Epoch 30/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8567 - acc: 0.2271 - val_loss: 1.8978 - val_acc: 0.2167\n",
      "Epoch 31/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8514 - acc: 0.2471 - val_loss: 1.8972 - val_acc: 0.1900\n",
      "Epoch 32/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8464 - acc: 0.2371 - val_loss: 1.8928 - val_acc: 0.1867\n",
      "Epoch 33/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8425 - acc: 0.2243 - val_loss: 1.8873 - val_acc: 0.2100\n",
      "Epoch 34/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8384 - acc: 0.2314 - val_loss: 1.8811 - val_acc: 0.1967\n",
      "Epoch 35/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8336 - acc: 0.2471 - val_loss: 1.8836 - val_acc: 0.1933\n",
      "Epoch 36/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8300 - acc: 0.2386 - val_loss: 1.8756 - val_acc: 0.1900\n",
      "Epoch 37/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8258 - acc: 0.2514 - val_loss: 1.8744 - val_acc: 0.1767\n",
      "Epoch 38/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8221 - acc: 0.2357 - val_loss: 1.8698 - val_acc: 0.1967\n",
      "Epoch 39/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8185 - acc: 0.2443 - val_loss: 1.8706 - val_acc: 0.1767\n",
      "Epoch 40/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8145 - acc: 0.2371 - val_loss: 1.8681 - val_acc: 0.1967\n",
      "Epoch 41/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8105 - acc: 0.2414 - val_loss: 1.8669 - val_acc: 0.1833\n",
      "Epoch 42/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8076 - acc: 0.2471 - val_loss: 1.8636 - val_acc: 0.1800\n",
      "Epoch 43/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8044 - acc: 0.2443 - val_loss: 1.8609 - val_acc: 0.1733\n",
      "Epoch 44/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8002 - acc: 0.2371 - val_loss: 1.8573 - val_acc: 0.1967\n",
      "Epoch 45/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7970 - acc: 0.2471 - val_loss: 1.8566 - val_acc: 0.1733\n",
      "Epoch 46/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7939 - acc: 0.2229 - val_loss: 1.8528 - val_acc: 0.1933\n",
      "Epoch 47/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7914 - acc: 0.2600 - val_loss: 1.8552 - val_acc: 0.1800\n",
      "Epoch 48/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7886 - acc: 0.2486 - val_loss: 1.8546 - val_acc: 0.1867\n",
      "Epoch 49/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7859 - acc: 0.2471 - val_loss: 1.8503 - val_acc: 0.1833\n",
      "Epoch 50/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7819 - acc: 0.2471 - val_loss: 1.8445 - val_acc: 0.2333\n",
      "Epoch 51/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7794 - acc: 0.2614 - val_loss: 1.8469 - val_acc: 0.1900\n",
      "Epoch 52/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7762 - acc: 0.2500 - val_loss: 1.8413 - val_acc: 0.1967\n",
      "Epoch 53/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7743 - acc: 0.2543 - val_loss: 1.8489 - val_acc: 0.2067\n",
      "Epoch 54/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7714 - acc: 0.2671 - val_loss: 1.8482 - val_acc: 0.1867\n",
      "Epoch 55/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7685 - acc: 0.2514 - val_loss: 1.8357 - val_acc: 0.2067\n",
      "Epoch 56/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7671 - acc: 0.2529 - val_loss: 1.8422 - val_acc: 0.2200\n",
      "Epoch 57/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7639 - acc: 0.2757 - val_loss: 1.8388 - val_acc: 0.2167\n",
      "Epoch 58/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7613 - acc: 0.2586 - val_loss: 1.8353 - val_acc: 0.2233\n",
      "Epoch 59/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7588 - acc: 0.2657 - val_loss: 1.8336 - val_acc: 0.2233\n",
      "Epoch 60/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7549 - acc: 0.2629 - val_loss: 1.8262 - val_acc: 0.2533\n",
      "Epoch 61/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7563 - acc: 0.2843 - val_loss: 1.8339 - val_acc: 0.2367\n",
      "Epoch 62/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7528 - acc: 0.2714 - val_loss: 1.8319 - val_acc: 0.2233\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7501 - acc: 0.2800 - val_loss: 1.8305 - val_acc: 0.2000\n",
      "Epoch 64/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7481 - acc: 0.2800 - val_loss: 1.8280 - val_acc: 0.2167\n",
      "Epoch 65/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7455 - acc: 0.2843 - val_loss: 1.8307 - val_acc: 0.2000\n",
      "Epoch 66/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7436 - acc: 0.2800 - val_loss: 1.8306 - val_acc: 0.2067\n",
      "Epoch 67/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7417 - acc: 0.2671 - val_loss: 1.8301 - val_acc: 0.2033\n",
      "Epoch 68/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7401 - acc: 0.2757 - val_loss: 1.8244 - val_acc: 0.2000\n",
      "Epoch 69/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7373 - acc: 0.2843 - val_loss: 1.8291 - val_acc: 0.2167\n",
      "Epoch 70/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7353 - acc: 0.2843 - val_loss: 1.8274 - val_acc: 0.2433\n",
      "Epoch 71/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7342 - acc: 0.2814 - val_loss: 1.8221 - val_acc: 0.2167\n",
      "Epoch 72/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7324 - acc: 0.2843 - val_loss: 1.8233 - val_acc: 0.2133\n",
      "Epoch 73/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7294 - acc: 0.2829 - val_loss: 1.8265 - val_acc: 0.2467\n",
      "Epoch 74/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7280 - acc: 0.2843 - val_loss: 1.8264 - val_acc: 0.1967\n",
      "Epoch 75/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7265 - acc: 0.2771 - val_loss: 1.8202 - val_acc: 0.2267\n",
      "Epoch 76/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7252 - acc: 0.2857 - val_loss: 1.8204 - val_acc: 0.2200\n",
      "Epoch 77/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7225 - acc: 0.3086 - val_loss: 1.8236 - val_acc: 0.2033\n",
      "Epoch 78/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7208 - acc: 0.2857 - val_loss: 1.8176 - val_acc: 0.1933\n",
      "Epoch 79/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7195 - acc: 0.2814 - val_loss: 1.8202 - val_acc: 0.2433\n",
      "Epoch 80/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7188 - acc: 0.2929 - val_loss: 1.8203 - val_acc: 0.2067\n",
      "Epoch 81/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7162 - acc: 0.2800 - val_loss: 1.8261 - val_acc: 0.2067\n",
      "Epoch 82/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7138 - acc: 0.2829 - val_loss: 1.8159 - val_acc: 0.2667\n",
      "Epoch 83/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7130 - acc: 0.2857 - val_loss: 1.8207 - val_acc: 0.2400\n",
      "Epoch 84/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7124 - acc: 0.2971 - val_loss: 1.8229 - val_acc: 0.2133\n",
      "Epoch 85/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7096 - acc: 0.2986 - val_loss: 1.8170 - val_acc: 0.2267\n",
      "Epoch 86/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7079 - acc: 0.2800 - val_loss: 1.8159 - val_acc: 0.2633\n",
      "Epoch 87/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7048 - acc: 0.3071 - val_loss: 1.8201 - val_acc: 0.2733\n",
      "Epoch 88/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7058 - acc: 0.3043 - val_loss: 1.8131 - val_acc: 0.2433\n",
      "Epoch 89/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7040 - acc: 0.3029 - val_loss: 1.8180 - val_acc: 0.2200\n",
      "Epoch 90/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7024 - acc: 0.2829 - val_loss: 1.8168 - val_acc: 0.2233\n",
      "Epoch 91/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7015 - acc: 0.3071 - val_loss: 1.8202 - val_acc: 0.2167\n",
      "Epoch 92/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6985 - acc: 0.3129 - val_loss: 1.8233 - val_acc: 0.2067\n",
      "Epoch 93/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6978 - acc: 0.3014 - val_loss: 1.8178 - val_acc: 0.2733\n",
      "Epoch 94/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6962 - acc: 0.3186 - val_loss: 1.8198 - val_acc: 0.2100\n",
      "Epoch 95/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6936 - acc: 0.3043 - val_loss: 1.8252 - val_acc: 0.2833\n",
      "Epoch 96/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6942 - acc: 0.3071 - val_loss: 1.8119 - val_acc: 0.2200\n",
      "Epoch 97/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6923 - acc: 0.3057 - val_loss: 1.8264 - val_acc: 0.2200\n",
      "Epoch 98/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6915 - acc: 0.3057 - val_loss: 1.8124 - val_acc: 0.2167\n",
      "Epoch 99/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6894 - acc: 0.3129 - val_loss: 1.8251 - val_acc: 0.2200\n",
      "Epoch 100/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6882 - acc: 0.3043 - val_loss: 1.8225 - val_acc: 0.2300\n",
      "Epoch 101/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6869 - acc: 0.3157 - val_loss: 1.8240 - val_acc: 0.2233\n",
      "Epoch 102/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6869 - acc: 0.3071 - val_loss: 1.8230 - val_acc: 0.2300\n",
      "Epoch 103/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6834 - acc: 0.3043 - val_loss: 1.8117 - val_acc: 0.2500\n",
      "Epoch 104/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6834 - acc: 0.3171 - val_loss: 1.8144 - val_acc: 0.2167\n",
      "Epoch 105/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6813 - acc: 0.3100 - val_loss: 1.8088 - val_acc: 0.2100\n",
      "Epoch 106/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6805 - acc: 0.3186 - val_loss: 1.8200 - val_acc: 0.2167\n",
      "Epoch 107/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6795 - acc: 0.3143 - val_loss: 1.8255 - val_acc: 0.2367\n",
      "Epoch 108/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6775 - acc: 0.3143 - val_loss: 1.8198 - val_acc: 0.2767\n",
      "Epoch 109/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6771 - acc: 0.3143 - val_loss: 1.8200 - val_acc: 0.2333\n",
      "Epoch 110/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6764 - acc: 0.3214 - val_loss: 1.8168 - val_acc: 0.2200\n",
      "Epoch 111/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6765 - acc: 0.3071 - val_loss: 1.8224 - val_acc: 0.2233\n",
      "Epoch 112/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6734 - acc: 0.3186 - val_loss: 1.8218 - val_acc: 0.2267\n",
      "Epoch 113/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6725 - acc: 0.3143 - val_loss: 1.8204 - val_acc: 0.2367\n",
      "Epoch 114/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6714 - acc: 0.3086 - val_loss: 1.8225 - val_acc: 0.2200\n",
      "Epoch 115/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6699 - acc: 0.3171 - val_loss: 1.8156 - val_acc: 0.2367\n",
      "Epoch 116/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6689 - acc: 0.3243 - val_loss: 1.8308 - val_acc: 0.2300\n",
      "Epoch 117/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6685 - acc: 0.3143 - val_loss: 1.8231 - val_acc: 0.2300\n",
      "Epoch 118/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6670 - acc: 0.3171 - val_loss: 1.8280 - val_acc: 0.2300\n",
      "Epoch 119/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6657 - acc: 0.3286 - val_loss: 1.8260 - val_acc: 0.2267\n",
      "Epoch 120/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6647 - acc: 0.3186 - val_loss: 1.8245 - val_acc: 0.2200\n",
      "Epoch 121/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6634 - acc: 0.3186 - val_loss: 1.8151 - val_acc: 0.2267\n",
      "Epoch 122/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6622 - acc: 0.3200 - val_loss: 1.8209 - val_acc: 0.2267\n",
      "Epoch 123/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6614 - acc: 0.3229 - val_loss: 1.8174 - val_acc: 0.2233\n",
      "Epoch 124/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6601 - acc: 0.3186 - val_loss: 1.8245 - val_acc: 0.2367\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6591 - acc: 0.3157 - val_loss: 1.8290 - val_acc: 0.2367\n",
      "Epoch 126/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6573 - acc: 0.3186 - val_loss: 1.8291 - val_acc: 0.2267\n",
      "Epoch 127/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6565 - acc: 0.3171 - val_loss: 1.8171 - val_acc: 0.2433\n",
      "Epoch 128/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6546 - acc: 0.3357 - val_loss: 1.8327 - val_acc: 0.2233\n",
      "Epoch 129/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6550 - acc: 0.3143 - val_loss: 1.8267 - val_acc: 0.2233\n",
      "Epoch 130/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6528 - acc: 0.3271 - val_loss: 1.8176 - val_acc: 0.2567\n",
      "Epoch 131/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6524 - acc: 0.3229 - val_loss: 1.8281 - val_acc: 0.2533\n",
      "Epoch 132/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6522 - acc: 0.3286 - val_loss: 1.8165 - val_acc: 0.2233\n",
      "Epoch 133/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6505 - acc: 0.3257 - val_loss: 1.8352 - val_acc: 0.2500\n",
      "Epoch 134/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6490 - acc: 0.3186 - val_loss: 1.8271 - val_acc: 0.2733\n",
      "Epoch 135/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6489 - acc: 0.3314 - val_loss: 1.8235 - val_acc: 0.2267\n",
      "Epoch 136/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6472 - acc: 0.3214 - val_loss: 1.8214 - val_acc: 0.2333\n",
      "Epoch 137/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6462 - acc: 0.3214 - val_loss: 1.8225 - val_acc: 0.2300\n",
      "Epoch 138/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6466 - acc: 0.3229 - val_loss: 1.8266 - val_acc: 0.2367\n",
      "Epoch 139/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6438 - acc: 0.3300 - val_loss: 1.8513 - val_acc: 0.2467\n",
      "Epoch 140/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6440 - acc: 0.3286 - val_loss: 1.8181 - val_acc: 0.2300\n",
      "Epoch 141/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6422 - acc: 0.3271 - val_loss: 1.8329 - val_acc: 0.2367\n",
      "Epoch 142/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6411 - acc: 0.3314 - val_loss: 1.8215 - val_acc: 0.2633\n",
      "Epoch 143/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6410 - acc: 0.3300 - val_loss: 1.8378 - val_acc: 0.2500\n",
      "Epoch 144/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6403 - acc: 0.3443 - val_loss: 1.8326 - val_acc: 0.2333\n",
      "Epoch 145/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6393 - acc: 0.3314 - val_loss: 1.8231 - val_acc: 0.2200\n",
      "Epoch 146/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6369 - acc: 0.3400 - val_loss: 1.8373 - val_acc: 0.2267\n",
      "Epoch 147/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6389 - acc: 0.3157 - val_loss: 1.8364 - val_acc: 0.2367\n",
      "Epoch 148/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6370 - acc: 0.3357 - val_loss: 1.8304 - val_acc: 0.2533\n",
      "Epoch 149/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6357 - acc: 0.3357 - val_loss: 1.8218 - val_acc: 0.2133\n",
      "Epoch 150/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6349 - acc: 0.3314 - val_loss: 1.8307 - val_acc: 0.2267\n",
      "Epoch 151/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6346 - acc: 0.3300 - val_loss: 1.8259 - val_acc: 0.2267\n",
      "Epoch 152/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6338 - acc: 0.3329 - val_loss: 1.8308 - val_acc: 0.2333\n",
      "Epoch 153/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6320 - acc: 0.3429 - val_loss: 1.8302 - val_acc: 0.2333\n",
      "Epoch 154/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6302 - acc: 0.3514 - val_loss: 1.8375 - val_acc: 0.2233\n",
      "Epoch 155/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6304 - acc: 0.3257 - val_loss: 1.8238 - val_acc: 0.2333\n",
      "Epoch 156/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6308 - acc: 0.3343 - val_loss: 1.8352 - val_acc: 0.2267\n",
      "Epoch 157/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6291 - acc: 0.3386 - val_loss: 1.8276 - val_acc: 0.2200\n",
      "Epoch 158/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6293 - acc: 0.3371 - val_loss: 1.8354 - val_acc: 0.2267\n",
      "Epoch 159/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6272 - acc: 0.3314 - val_loss: 1.8359 - val_acc: 0.2467\n",
      "Epoch 160/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6265 - acc: 0.3329 - val_loss: 1.8336 - val_acc: 0.2267\n",
      "Epoch 161/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6258 - acc: 0.3314 - val_loss: 1.8427 - val_acc: 0.2300\n",
      "Epoch 162/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6252 - acc: 0.3314 - val_loss: 1.8391 - val_acc: 0.2233\n",
      "Epoch 163/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6240 - acc: 0.3357 - val_loss: 1.8381 - val_acc: 0.2267\n",
      "Epoch 164/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6227 - acc: 0.3386 - val_loss: 1.8281 - val_acc: 0.2133\n",
      "Epoch 165/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6224 - acc: 0.3243 - val_loss: 1.8414 - val_acc: 0.2400\n",
      "Epoch 166/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6218 - acc: 0.3343 - val_loss: 1.8361 - val_acc: 0.2233\n",
      "Epoch 167/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6209 - acc: 0.3357 - val_loss: 1.8406 - val_acc: 0.2567\n",
      "Epoch 168/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6196 - acc: 0.3457 - val_loss: 1.8464 - val_acc: 0.2200\n",
      "Epoch 169/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6186 - acc: 0.3443 - val_loss: 1.8540 - val_acc: 0.2233\n",
      "Epoch 170/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6182 - acc: 0.3371 - val_loss: 1.8352 - val_acc: 0.2267\n",
      "Epoch 171/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6160 - acc: 0.3243 - val_loss: 1.8516 - val_acc: 0.2633\n",
      "Epoch 172/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6151 - acc: 0.3429 - val_loss: 1.8404 - val_acc: 0.2200\n",
      "Epoch 173/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6171 - acc: 0.3400 - val_loss: 1.8403 - val_acc: 0.2433\n",
      "Epoch 174/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6157 - acc: 0.3343 - val_loss: 1.8443 - val_acc: 0.2167\n",
      "Epoch 175/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6156 - acc: 0.3400 - val_loss: 1.8432 - val_acc: 0.2200\n",
      "Epoch 176/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6134 - acc: 0.3286 - val_loss: 1.8422 - val_acc: 0.2633\n",
      "Epoch 177/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6124 - acc: 0.3414 - val_loss: 1.8399 - val_acc: 0.2200\n",
      "Epoch 178/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6130 - acc: 0.3400 - val_loss: 1.8437 - val_acc: 0.2300\n",
      "Epoch 179/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6125 - acc: 0.3443 - val_loss: 1.8383 - val_acc: 0.2233\n",
      "Epoch 180/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6102 - acc: 0.3429 - val_loss: 1.8428 - val_acc: 0.2667\n",
      "Epoch 181/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.3471 - val_loss: 1.8358 - val_acc: 0.2567\n",
      "Epoch 182/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6098 - acc: 0.3457 - val_loss: 1.8391 - val_acc: 0.2167\n",
      "Epoch 183/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.3329 - val_loss: 1.8443 - val_acc: 0.2300\n",
      "Epoch 184/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6083 - acc: 0.3514 - val_loss: 1.8470 - val_acc: 0.2267\n",
      "Epoch 185/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6066 - acc: 0.3429 - val_loss: 1.8662 - val_acc: 0.2200\n",
      "Epoch 186/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6077 - acc: 0.3414 - val_loss: 1.8462 - val_acc: 0.2267\n",
      "Epoch 187/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6057 - acc: 0.3543 - val_loss: 1.8476 - val_acc: 0.2200\n",
      "Epoch 188/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6066 - acc: 0.3400 - val_loss: 1.8436 - val_acc: 0.2200\n",
      "Epoch 189/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6037 - acc: 0.3543 - val_loss: 1.8559 - val_acc: 0.2200\n",
      "Epoch 190/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6057 - acc: 0.3314 - val_loss: 1.8426 - val_acc: 0.2300\n",
      "Epoch 191/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6034 - acc: 0.3571 - val_loss: 1.8621 - val_acc: 0.2233\n",
      "Epoch 192/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6037 - acc: 0.3486 - val_loss: 1.8528 - val_acc: 0.2233\n",
      "Epoch 193/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6021 - acc: 0.3429 - val_loss: 1.8571 - val_acc: 0.2300\n",
      "Epoch 194/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6018 - acc: 0.3414 - val_loss: 1.8503 - val_acc: 0.2300\n",
      "Epoch 195/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6012 - acc: 0.3471 - val_loss: 1.8486 - val_acc: 0.2467\n",
      "Epoch 196/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6008 - acc: 0.3486 - val_loss: 1.8510 - val_acc: 0.2200\n",
      "Epoch 197/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5994 - acc: 0.3500 - val_loss: 1.8513 - val_acc: 0.2300\n",
      "Epoch 198/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5998 - acc: 0.3400 - val_loss: 1.8613 - val_acc: 0.2233\n",
      "Epoch 199/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5996 - acc: 0.3400 - val_loss: 1.8579 - val_acc: 0.2233\n",
      "Epoch 200/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5967 - acc: 0.3457 - val_loss: 1.8624 - val_acc: 0.2633\n",
      "Epoch 201/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5979 - acc: 0.3443 - val_loss: 1.8496 - val_acc: 0.2133\n",
      "Epoch 202/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5972 - acc: 0.3471 - val_loss: 1.8499 - val_acc: 0.2100\n",
      "Epoch 203/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5959 - acc: 0.3357 - val_loss: 1.8599 - val_acc: 0.2300\n",
      "Epoch 204/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5947 - acc: 0.3629 - val_loss: 1.8489 - val_acc: 0.2167\n",
      "Epoch 205/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5946 - acc: 0.3443 - val_loss: 1.8532 - val_acc: 0.2233\n",
      "Epoch 206/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5939 - acc: 0.3543 - val_loss: 1.8498 - val_acc: 0.2133\n",
      "Epoch 207/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5940 - acc: 0.3543 - val_loss: 1.8574 - val_acc: 0.2267\n",
      "Epoch 208/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5914 - acc: 0.3471 - val_loss: 1.8582 - val_acc: 0.2233\n",
      "Epoch 209/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5913 - acc: 0.3514 - val_loss: 1.8553 - val_acc: 0.2367\n",
      "Epoch 210/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5908 - acc: 0.3471 - val_loss: 1.8557 - val_acc: 0.2300\n",
      "Epoch 211/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5897 - acc: 0.3414 - val_loss: 1.8479 - val_acc: 0.2600\n",
      "Epoch 212/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5907 - acc: 0.3514 - val_loss: 1.8596 - val_acc: 0.2233\n",
      "Epoch 213/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5888 - acc: 0.3514 - val_loss: 1.8556 - val_acc: 0.2300\n",
      "Epoch 214/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5899 - acc: 0.3443 - val_loss: 1.8616 - val_acc: 0.2300\n",
      "Epoch 215/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5880 - acc: 0.3457 - val_loss: 1.8609 - val_acc: 0.2667\n",
      "Epoch 216/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5890 - acc: 0.3514 - val_loss: 1.8658 - val_acc: 0.2300\n",
      "Epoch 217/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5874 - acc: 0.3457 - val_loss: 1.8674 - val_acc: 0.2267\n",
      "Epoch 218/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5870 - acc: 0.3457 - val_loss: 1.8633 - val_acc: 0.2300\n",
      "Epoch 219/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5860 - acc: 0.3471 - val_loss: 1.8689 - val_acc: 0.2267\n",
      "Epoch 220/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5850 - acc: 0.3557 - val_loss: 1.8623 - val_acc: 0.2267\n",
      "Epoch 221/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5851 - acc: 0.3471 - val_loss: 1.8648 - val_acc: 0.2267\n",
      "Epoch 222/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5838 - acc: 0.3486 - val_loss: 1.8800 - val_acc: 0.2467\n",
      "Epoch 223/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3443 - val_loss: 1.8650 - val_acc: 0.2600\n",
      "Epoch 224/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3543 - val_loss: 1.8793 - val_acc: 0.2433\n",
      "Epoch 225/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5828 - acc: 0.3443 - val_loss: 1.8599 - val_acc: 0.2200\n",
      "Epoch 226/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5830 - acc: 0.3500 - val_loss: 1.8725 - val_acc: 0.2267\n",
      "Epoch 227/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5820 - acc: 0.3486 - val_loss: 1.8668 - val_acc: 0.2167\n",
      "Epoch 228/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5819 - acc: 0.3443 - val_loss: 1.8712 - val_acc: 0.2267\n",
      "Epoch 229/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5796 - acc: 0.3571 - val_loss: 1.8748 - val_acc: 0.2233\n",
      "Epoch 230/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5813 - acc: 0.3414 - val_loss: 1.8648 - val_acc: 0.2167\n",
      "Epoch 231/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5794 - acc: 0.3543 - val_loss: 1.8656 - val_acc: 0.2233\n",
      "Epoch 232/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5804 - acc: 0.3514 - val_loss: 1.8780 - val_acc: 0.2200\n",
      "Epoch 233/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5792 - acc: 0.3486 - val_loss: 1.8839 - val_acc: 0.2300\n",
      "Epoch 234/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5786 - acc: 0.3457 - val_loss: 1.8718 - val_acc: 0.2200\n",
      "Epoch 235/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5772 - acc: 0.3586 - val_loss: 1.8739 - val_acc: 0.2100\n",
      "Epoch 236/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5774 - acc: 0.3457 - val_loss: 1.8692 - val_acc: 0.2167\n",
      "Epoch 237/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3471 - val_loss: 1.8763 - val_acc: 0.2300\n",
      "Epoch 238/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5769 - acc: 0.3457 - val_loss: 1.8660 - val_acc: 0.2133\n",
      "Epoch 239/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3543 - val_loss: 1.8732 - val_acc: 0.2300\n",
      "Epoch 240/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5745 - acc: 0.3529 - val_loss: 1.8775 - val_acc: 0.2467\n",
      "Epoch 241/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5743 - acc: 0.3571 - val_loss: 1.8799 - val_acc: 0.2433\n",
      "Epoch 242/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5739 - acc: 0.3543 - val_loss: 1.8785 - val_acc: 0.2067\n",
      "Epoch 243/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5738 - acc: 0.3600 - val_loss: 1.8845 - val_acc: 0.2167\n",
      "Epoch 244/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5707 - acc: 0.3600 - val_loss: 1.8761 - val_acc: 0.2200\n",
      "Epoch 245/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5711 - acc: 0.3586 - val_loss: 1.8890 - val_acc: 0.2267\n",
      "Epoch 246/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5707 - acc: 0.3543 - val_loss: 1.8755 - val_acc: 0.2633\n",
      "Epoch 247/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5718 - acc: 0.3557 - val_loss: 1.8654 - val_acc: 0.2200\n",
      "Epoch 248/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5711 - acc: 0.3557 - val_loss: 1.8729 - val_acc: 0.2300\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5708 - acc: 0.3471 - val_loss: 1.8870 - val_acc: 0.2267\n",
      "Epoch 250/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5675 - acc: 0.3629 - val_loss: 1.9024 - val_acc: 0.2167\n",
      "Epoch 251/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5699 - acc: 0.3471 - val_loss: 1.8845 - val_acc: 0.2333\n",
      "Epoch 252/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3657 - val_loss: 1.8804 - val_acc: 0.2267\n",
      "Epoch 253/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5685 - acc: 0.3543 - val_loss: 1.8866 - val_acc: 0.2167\n",
      "Epoch 254/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5674 - acc: 0.3557 - val_loss: 1.8858 - val_acc: 0.2267\n",
      "Epoch 255/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3471 - val_loss: 1.8745 - val_acc: 0.2300\n",
      "Epoch 256/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5654 - acc: 0.3529 - val_loss: 1.8798 - val_acc: 0.2533\n",
      "Epoch 257/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5663 - acc: 0.3486 - val_loss: 1.8948 - val_acc: 0.2367\n",
      "Epoch 258/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5644 - acc: 0.3600 - val_loss: 1.9016 - val_acc: 0.2567\n",
      "Epoch 259/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5643 - acc: 0.3629 - val_loss: 1.8976 - val_acc: 0.2233\n",
      "Epoch 260/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5653 - acc: 0.3614 - val_loss: 1.9022 - val_acc: 0.2233\n",
      "Epoch 261/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5632 - acc: 0.3586 - val_loss: 1.8783 - val_acc: 0.2167\n",
      "Epoch 262/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3600 - val_loss: 1.8894 - val_acc: 0.2133\n",
      "Epoch 263/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5627 - acc: 0.3643 - val_loss: 1.8926 - val_acc: 0.2200\n",
      "Epoch 264/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5635 - acc: 0.3557 - val_loss: 1.8869 - val_acc: 0.2100\n",
      "Epoch 265/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3657 - val_loss: 1.8964 - val_acc: 0.2267\n",
      "Epoch 266/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5609 - acc: 0.3629 - val_loss: 1.9027 - val_acc: 0.2467\n",
      "Epoch 267/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5610 - acc: 0.3657 - val_loss: 1.8957 - val_acc: 0.2267\n",
      "Epoch 268/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5608 - acc: 0.3629 - val_loss: 1.8805 - val_acc: 0.2167\n",
      "Epoch 269/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5596 - acc: 0.3657 - val_loss: 1.8884 - val_acc: 0.2167\n",
      "Epoch 270/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5592 - acc: 0.3686 - val_loss: 1.8748 - val_acc: 0.2333\n",
      "Epoch 271/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5605 - acc: 0.3643 - val_loss: 1.8974 - val_acc: 0.2300\n",
      "Epoch 272/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5591 - acc: 0.3586 - val_loss: 1.9103 - val_acc: 0.2267\n",
      "Epoch 273/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5587 - acc: 0.3557 - val_loss: 1.9021 - val_acc: 0.2133\n",
      "Epoch 274/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5585 - acc: 0.3471 - val_loss: 1.8900 - val_acc: 0.2233\n",
      "Epoch 275/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5585 - acc: 0.3586 - val_loss: 1.9002 - val_acc: 0.2233\n",
      "Epoch 276/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5575 - acc: 0.3543 - val_loss: 1.9019 - val_acc: 0.2567\n",
      "Epoch 277/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5577 - acc: 0.3743 - val_loss: 1.8968 - val_acc: 0.2167\n",
      "Epoch 278/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5575 - acc: 0.3600 - val_loss: 1.9086 - val_acc: 0.2300\n",
      "Epoch 279/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5554 - acc: 0.3586 - val_loss: 1.8917 - val_acc: 0.2200\n",
      "Epoch 280/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5555 - acc: 0.3643 - val_loss: 1.8934 - val_acc: 0.2167\n",
      "Epoch 281/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5537 - acc: 0.3557 - val_loss: 1.8959 - val_acc: 0.2333\n",
      "Epoch 282/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5550 - acc: 0.3700 - val_loss: 1.8886 - val_acc: 0.2200\n",
      "Epoch 283/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5538 - acc: 0.3571 - val_loss: 1.8993 - val_acc: 0.2233\n",
      "Epoch 284/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5538 - acc: 0.3586 - val_loss: 1.8984 - val_acc: 0.2600\n",
      "Epoch 285/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5531 - acc: 0.3586 - val_loss: 1.8939 - val_acc: 0.2200\n",
      "Epoch 286/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5532 - acc: 0.3557 - val_loss: 1.9055 - val_acc: 0.2233\n",
      "Epoch 287/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5524 - acc: 0.3700 - val_loss: 1.9065 - val_acc: 0.2133\n",
      "Epoch 288/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5520 - acc: 0.3586 - val_loss: 1.8990 - val_acc: 0.2100\n",
      "Epoch 289/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5515 - acc: 0.3600 - val_loss: 1.9082 - val_acc: 0.2100\n",
      "Epoch 290/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5511 - acc: 0.3657 - val_loss: 1.9100 - val_acc: 0.2200\n",
      "Epoch 291/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5509 - acc: 0.3586 - val_loss: 1.9091 - val_acc: 0.2200\n",
      "Epoch 292/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5504 - acc: 0.3514 - val_loss: 1.9040 - val_acc: 0.2400\n",
      "Epoch 293/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5503 - acc: 0.3557 - val_loss: 1.9065 - val_acc: 0.2333\n",
      "Epoch 294/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5494 - acc: 0.3571 - val_loss: 1.9066 - val_acc: 0.2533\n",
      "Epoch 295/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5495 - acc: 0.3729 - val_loss: 1.9167 - val_acc: 0.2133\n",
      "Epoch 296/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5490 - acc: 0.3571 - val_loss: 1.9153 - val_acc: 0.2067\n",
      "Epoch 297/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5475 - acc: 0.3586 - val_loss: 1.9185 - val_acc: 0.2167\n",
      "Epoch 298/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5473 - acc: 0.3486 - val_loss: 1.9017 - val_acc: 0.2300\n",
      "Epoch 299/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5480 - acc: 0.3671 - val_loss: 1.9235 - val_acc: 0.2300\n",
      "Epoch 300/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5470 - acc: 0.3643 - val_loss: 1.9094 - val_acc: 0.2300\n",
      "Epoch 301/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5466 - acc: 0.3629 - val_loss: 1.8992 - val_acc: 0.2200\n",
      "Epoch 302/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5476 - acc: 0.3686 - val_loss: 1.9138 - val_acc: 0.2200\n",
      "Epoch 303/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5461 - acc: 0.3543 - val_loss: 1.9060 - val_acc: 0.2367\n",
      "Epoch 304/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5448 - acc: 0.3700 - val_loss: 1.9080 - val_acc: 0.2200\n",
      "Epoch 305/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3671 - val_loss: 1.9226 - val_acc: 0.2233\n",
      "Epoch 306/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5455 - acc: 0.3671 - val_loss: 1.9151 - val_acc: 0.2333\n",
      "Epoch 307/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3714 - val_loss: 1.9135 - val_acc: 0.2300\n",
      "Epoch 308/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5448 - acc: 0.3600 - val_loss: 1.9182 - val_acc: 0.2333\n",
      "Epoch 309/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3629 - val_loss: 1.9224 - val_acc: 0.2367\n",
      "Epoch 310/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5443 - acc: 0.3657 - val_loss: 1.9164 - val_acc: 0.2233\n",
      "Epoch 311/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5423 - acc: 0.3600 - val_loss: 1.9111 - val_acc: 0.2300\n",
      "Epoch 312/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5420 - acc: 0.3729 - val_loss: 1.9346 - val_acc: 0.2333\n",
      "Epoch 313/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5384 - acc: 0.3714 - val_loss: 1.9118 - val_acc: 0.2567\n",
      "Epoch 314/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5429 - acc: 0.3686 - val_loss: 1.9180 - val_acc: 0.2300\n",
      "Epoch 315/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3657 - val_loss: 1.9165 - val_acc: 0.2233\n",
      "Epoch 316/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3643 - val_loss: 1.9253 - val_acc: 0.2200\n",
      "Epoch 317/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5409 - acc: 0.3571 - val_loss: 1.9289 - val_acc: 0.2333\n",
      "Epoch 318/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5404 - acc: 0.3643 - val_loss: 1.9174 - val_acc: 0.2300\n",
      "Epoch 319/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5386 - acc: 0.3629 - val_loss: 1.9137 - val_acc: 0.2233\n",
      "Epoch 320/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5388 - acc: 0.3600 - val_loss: 1.9329 - val_acc: 0.2233\n",
      "Epoch 321/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5388 - acc: 0.3700 - val_loss: 1.9327 - val_acc: 0.2300\n",
      "Epoch 322/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5370 - acc: 0.3686 - val_loss: 1.9215 - val_acc: 0.2533\n",
      "Epoch 323/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5379 - acc: 0.3700 - val_loss: 1.9232 - val_acc: 0.2300\n",
      "Epoch 324/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5368 - acc: 0.3657 - val_loss: 1.9294 - val_acc: 0.2233\n",
      "Epoch 325/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3600 - val_loss: 1.9207 - val_acc: 0.2133\n",
      "Epoch 326/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5361 - acc: 0.3600 - val_loss: 1.9333 - val_acc: 0.2300\n",
      "Epoch 327/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5355 - acc: 0.3729 - val_loss: 1.9460 - val_acc: 0.2367\n",
      "Epoch 328/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5364 - acc: 0.3629 - val_loss: 1.9360 - val_acc: 0.2267\n",
      "Epoch 329/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5351 - acc: 0.3743 - val_loss: 1.9214 - val_acc: 0.2267\n",
      "Epoch 330/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5347 - acc: 0.3657 - val_loss: 1.9297 - val_acc: 0.2333\n",
      "Epoch 331/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.3657 - val_loss: 1.9213 - val_acc: 0.2133\n",
      "Epoch 332/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5341 - acc: 0.3600 - val_loss: 1.9377 - val_acc: 0.2567\n",
      "Epoch 333/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5355 - acc: 0.3714 - val_loss: 1.9296 - val_acc: 0.2300\n",
      "Epoch 334/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5346 - acc: 0.3614 - val_loss: 1.9332 - val_acc: 0.2300\n",
      "Epoch 335/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5323 - acc: 0.3786 - val_loss: 1.9317 - val_acc: 0.2567\n",
      "Epoch 336/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5331 - acc: 0.3800 - val_loss: 1.9245 - val_acc: 0.2267\n",
      "Epoch 337/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5328 - acc: 0.3771 - val_loss: 1.9370 - val_acc: 0.2233\n",
      "Epoch 338/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5319 - acc: 0.3814 - val_loss: 1.9303 - val_acc: 0.2133\n",
      "Epoch 339/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5308 - acc: 0.3729 - val_loss: 1.9233 - val_acc: 0.2133\n",
      "Epoch 340/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5324 - acc: 0.3743 - val_loss: 1.9475 - val_acc: 0.2200\n",
      "Epoch 341/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5312 - acc: 0.3786 - val_loss: 1.9290 - val_acc: 0.2333\n",
      "Epoch 342/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5304 - acc: 0.3657 - val_loss: 1.9489 - val_acc: 0.2200\n",
      "Epoch 343/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5303 - acc: 0.3700 - val_loss: 1.9401 - val_acc: 0.2233\n",
      "Epoch 344/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5297 - acc: 0.3700 - val_loss: 1.9494 - val_acc: 0.2200\n",
      "Epoch 345/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5282 - acc: 0.3671 - val_loss: 1.9510 - val_acc: 0.2467\n",
      "Epoch 346/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5289 - acc: 0.3714 - val_loss: 1.9370 - val_acc: 0.2267\n",
      "Epoch 347/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5267 - acc: 0.3814 - val_loss: 1.9222 - val_acc: 0.2233\n",
      "Epoch 348/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5270 - acc: 0.3629 - val_loss: 1.9375 - val_acc: 0.2133\n",
      "Epoch 349/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5270 - acc: 0.3729 - val_loss: 1.9358 - val_acc: 0.2167\n",
      "Epoch 350/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5265 - acc: 0.3714 - val_loss: 1.9369 - val_acc: 0.2300\n",
      "Epoch 351/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5248 - acc: 0.3771 - val_loss: 1.9368 - val_acc: 0.2367\n",
      "Epoch 352/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5264 - acc: 0.3743 - val_loss: 1.9367 - val_acc: 0.2300\n",
      "Epoch 353/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5240 - acc: 0.3671 - val_loss: 1.9490 - val_acc: 0.2267\n",
      "Epoch 354/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5260 - acc: 0.3786 - val_loss: 1.9393 - val_acc: 0.2167\n",
      "Epoch 355/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5241 - acc: 0.3629 - val_loss: 1.9581 - val_acc: 0.2300\n",
      "Epoch 356/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5241 - acc: 0.3729 - val_loss: 1.9496 - val_acc: 0.2400\n",
      "Epoch 357/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5238 - acc: 0.3757 - val_loss: 1.9388 - val_acc: 0.2300\n",
      "Epoch 358/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5230 - acc: 0.3729 - val_loss: 1.9579 - val_acc: 0.2167\n",
      "Epoch 359/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5233 - acc: 0.3786 - val_loss: 1.9501 - val_acc: 0.2200\n",
      "Epoch 360/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5214 - acc: 0.3700 - val_loss: 1.9545 - val_acc: 0.2167\n",
      "Epoch 361/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5209 - acc: 0.3714 - val_loss: 1.9496 - val_acc: 0.2300\n",
      "Epoch 362/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5220 - acc: 0.3657 - val_loss: 1.9511 - val_acc: 0.2200\n",
      "Epoch 363/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5194 - acc: 0.3743 - val_loss: 1.9580 - val_acc: 0.2333\n",
      "Epoch 364/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5205 - acc: 0.3643 - val_loss: 1.9375 - val_acc: 0.2300\n",
      "Epoch 365/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5203 - acc: 0.3829 - val_loss: 1.9534 - val_acc: 0.2167\n",
      "Epoch 366/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5200 - acc: 0.3757 - val_loss: 1.9425 - val_acc: 0.2133\n",
      "Epoch 367/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5202 - acc: 0.3786 - val_loss: 1.9535 - val_acc: 0.2267\n",
      "Epoch 368/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5186 - acc: 0.3829 - val_loss: 1.9549 - val_acc: 0.2300\n",
      "Epoch 369/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5185 - acc: 0.3729 - val_loss: 1.9363 - val_acc: 0.2367\n",
      "Epoch 370/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5177 - acc: 0.3714 - val_loss: 1.9571 - val_acc: 0.2333\n",
      "Epoch 371/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5185 - acc: 0.3657 - val_loss: 1.9572 - val_acc: 0.2200\n",
      "Epoch 372/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5171 - acc: 0.3757 - val_loss: 1.9619 - val_acc: 0.2300\n",
      "Epoch 373/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5181 - acc: 0.3714 - val_loss: 1.9596 - val_acc: 0.2200\n",
      "Epoch 374/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5173 - acc: 0.3657 - val_loss: 1.9547 - val_acc: 0.2333\n",
      "Epoch 375/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5158 - acc: 0.3857 - val_loss: 1.9442 - val_acc: 0.2267\n",
      "Epoch 376/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5159 - acc: 0.3786 - val_loss: 1.9532 - val_acc: 0.2200\n",
      "Epoch 377/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5150 - acc: 0.3729 - val_loss: 1.9741 - val_acc: 0.2233\n",
      "Epoch 378/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5157 - acc: 0.3729 - val_loss: 1.9803 - val_acc: 0.2367\n",
      "Epoch 379/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5165 - acc: 0.3686 - val_loss: 1.9545 - val_acc: 0.2400\n",
      "Epoch 380/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5141 - acc: 0.3686 - val_loss: 1.9621 - val_acc: 0.2567\n",
      "Epoch 381/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5154 - acc: 0.3800 - val_loss: 1.9751 - val_acc: 0.2133\n",
      "Epoch 382/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5138 - acc: 0.3743 - val_loss: 1.9589 - val_acc: 0.2200\n",
      "Epoch 383/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5130 - acc: 0.3814 - val_loss: 1.9701 - val_acc: 0.2433\n",
      "Epoch 384/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5138 - acc: 0.3829 - val_loss: 1.9513 - val_acc: 0.2233\n",
      "Epoch 385/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5139 - acc: 0.3829 - val_loss: 1.9593 - val_acc: 0.2200\n",
      "Epoch 386/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5130 - acc: 0.3757 - val_loss: 1.9527 - val_acc: 0.2367\n",
      "Epoch 387/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5129 - acc: 0.3829 - val_loss: 1.9620 - val_acc: 0.2333\n",
      "Epoch 388/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5117 - acc: 0.3757 - val_loss: 1.9701 - val_acc: 0.2500\n",
      "Epoch 389/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5135 - acc: 0.3829 - val_loss: 1.9538 - val_acc: 0.2267\n",
      "Epoch 390/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5099 - acc: 0.3686 - val_loss: 1.9576 - val_acc: 0.2400\n",
      "Epoch 391/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5105 - acc: 0.3871 - val_loss: 1.9589 - val_acc: 0.2367\n",
      "Epoch 392/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5112 - acc: 0.3757 - val_loss: 1.9601 - val_acc: 0.2267\n",
      "Epoch 393/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5112 - acc: 0.3800 - val_loss: 1.9739 - val_acc: 0.2233\n",
      "Epoch 394/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5105 - acc: 0.3800 - val_loss: 1.9582 - val_acc: 0.2267\n",
      "Epoch 395/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5103 - acc: 0.3857 - val_loss: 1.9586 - val_acc: 0.2233\n",
      "Epoch 396/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5098 - acc: 0.3771 - val_loss: 1.9631 - val_acc: 0.2267\n",
      "Epoch 397/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5086 - acc: 0.3871 - val_loss: 1.9770 - val_acc: 0.2300\n",
      "Epoch 398/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5100 - acc: 0.3643 - val_loss: 1.9610 - val_acc: 0.2300\n",
      "Epoch 399/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5088 - acc: 0.3757 - val_loss: 1.9809 - val_acc: 0.2367\n",
      "Epoch 400/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5089 - acc: 0.3743 - val_loss: 1.9580 - val_acc: 0.2233\n",
      "Epoch 401/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5090 - acc: 0.3686 - val_loss: 1.9651 - val_acc: 0.2333\n",
      "Epoch 402/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5067 - acc: 0.3800 - val_loss: 1.9686 - val_acc: 0.2333\n",
      "Epoch 403/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5053 - acc: 0.3857 - val_loss: 1.9915 - val_acc: 0.2200\n",
      "Epoch 404/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5063 - acc: 0.3786 - val_loss: 1.9658 - val_acc: 0.2367\n",
      "Epoch 405/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5055 - acc: 0.3829 - val_loss: 1.9850 - val_acc: 0.2333\n",
      "Epoch 406/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5066 - acc: 0.3857 - val_loss: 1.9874 - val_acc: 0.2200\n",
      "Epoch 407/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5067 - acc: 0.3843 - val_loss: 1.9742 - val_acc: 0.2367\n",
      "Epoch 408/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5063 - acc: 0.3829 - val_loss: 1.9733 - val_acc: 0.2300\n",
      "Epoch 409/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5025 - acc: 0.3757 - val_loss: 1.9827 - val_acc: 0.2400\n",
      "Epoch 410/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5047 - acc: 0.3800 - val_loss: 1.9636 - val_acc: 0.2233\n",
      "Epoch 411/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5049 - acc: 0.3771 - val_loss: 1.9806 - val_acc: 0.2300\n",
      "Epoch 412/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5044 - acc: 0.3943 - val_loss: 1.9758 - val_acc: 0.2267\n",
      "Epoch 413/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5027 - acc: 0.3771 - val_loss: 1.9723 - val_acc: 0.2367\n",
      "Epoch 414/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5038 - acc: 0.3843 - val_loss: 1.9853 - val_acc: 0.2333\n",
      "Epoch 415/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5012 - acc: 0.3729 - val_loss: 1.9849 - val_acc: 0.2400\n",
      "Epoch 416/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5024 - acc: 0.3900 - val_loss: 1.9951 - val_acc: 0.2167\n",
      "Epoch 417/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5031 - acc: 0.3786 - val_loss: 1.9829 - val_acc: 0.2267\n",
      "Epoch 418/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5015 - acc: 0.3900 - val_loss: 1.9853 - val_acc: 0.2400\n",
      "Epoch 419/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5017 - acc: 0.3800 - val_loss: 1.9723 - val_acc: 0.2267\n",
      "Epoch 420/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5017 - acc: 0.3829 - val_loss: 1.9959 - val_acc: 0.2300\n",
      "Epoch 421/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5014 - acc: 0.3886 - val_loss: 1.9737 - val_acc: 0.2400\n",
      "Epoch 422/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5005 - acc: 0.3786 - val_loss: 1.9725 - val_acc: 0.2367\n",
      "Epoch 423/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5003 - acc: 0.3871 - val_loss: 1.9867 - val_acc: 0.2400\n",
      "Epoch 424/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5000 - acc: 0.3771 - val_loss: 1.9638 - val_acc: 0.2233\n",
      "Epoch 425/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4993 - acc: 0.3957 - val_loss: 1.9668 - val_acc: 0.2367\n",
      "Epoch 426/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4986 - acc: 0.3986 - val_loss: 1.9819 - val_acc: 0.2167\n",
      "Epoch 427/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4985 - acc: 0.3843 - val_loss: 1.9860 - val_acc: 0.2600\n",
      "Epoch 428/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4988 - acc: 0.3871 - val_loss: 2.0070 - val_acc: 0.2267\n",
      "Epoch 429/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4987 - acc: 0.3786 - val_loss: 1.9713 - val_acc: 0.2333\n",
      "Epoch 430/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4981 - acc: 0.3871 - val_loss: 1.9831 - val_acc: 0.2500\n",
      "Epoch 431/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4994 - acc: 0.3914 - val_loss: 1.9894 - val_acc: 0.2333\n",
      "Epoch 432/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4983 - acc: 0.3843 - val_loss: 1.9835 - val_acc: 0.2267\n",
      "Epoch 433/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4973 - acc: 0.3814 - val_loss: 1.9861 - val_acc: 0.2400\n",
      "Epoch 434/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4982 - acc: 0.3757 - val_loss: 1.9854 - val_acc: 0.2267\n",
      "Epoch 435/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4964 - acc: 0.3943 - val_loss: 1.9864 - val_acc: 0.2367\n",
      "Epoch 436/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4980 - acc: 0.3886 - val_loss: 1.9704 - val_acc: 0.2267\n",
      "Epoch 437/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4964 - acc: 0.3943 - val_loss: 1.9879 - val_acc: 0.2300\n",
      "Epoch 438/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4955 - acc: 0.3900 - val_loss: 1.9661 - val_acc: 0.2300\n",
      "Epoch 439/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.3771 - val_loss: 1.9776 - val_acc: 0.2267\n",
      "Epoch 440/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4966 - acc: 0.3800 - val_loss: 1.9787 - val_acc: 0.2300\n",
      "Epoch 441/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4958 - acc: 0.3857 - val_loss: 1.9831 - val_acc: 0.2267\n",
      "Epoch 442/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.3857 - val_loss: 1.9890 - val_acc: 0.2233\n",
      "Epoch 443/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4946 - acc: 0.3843 - val_loss: 1.9835 - val_acc: 0.2400\n",
      "Epoch 444/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4954 - acc: 0.3857 - val_loss: 1.9939 - val_acc: 0.2233\n",
      "Epoch 445/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4949 - acc: 0.3829 - val_loss: 1.9871 - val_acc: 0.2333\n",
      "Epoch 446/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4939 - acc: 0.3814 - val_loss: 1.9950 - val_acc: 0.2300\n",
      "Epoch 447/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4937 - acc: 0.3814 - val_loss: 2.0150 - val_acc: 0.2267\n",
      "Epoch 448/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4934 - acc: 0.3857 - val_loss: 1.9854 - val_acc: 0.2267\n",
      "Epoch 449/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4930 - acc: 0.3971 - val_loss: 2.0027 - val_acc: 0.2333\n",
      "Epoch 450/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4940 - acc: 0.3871 - val_loss: 1.9939 - val_acc: 0.2267\n",
      "Epoch 451/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4931 - acc: 0.3814 - val_loss: 1.9959 - val_acc: 0.2233\n",
      "Epoch 452/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4924 - acc: 0.3857 - val_loss: 1.9909 - val_acc: 0.2233\n",
      "Epoch 453/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4920 - acc: 0.3871 - val_loss: 1.9948 - val_acc: 0.2367\n",
      "Epoch 454/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4919 - acc: 0.3900 - val_loss: 1.9987 - val_acc: 0.2267\n",
      "Epoch 455/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4920 - acc: 0.3786 - val_loss: 1.9859 - val_acc: 0.2300\n",
      "Epoch 456/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4912 - acc: 0.3857 - val_loss: 1.9932 - val_acc: 0.2467\n",
      "Epoch 457/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4910 - acc: 0.3900 - val_loss: 1.9975 - val_acc: 0.2300\n",
      "Epoch 458/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4915 - acc: 0.3829 - val_loss: 1.9907 - val_acc: 0.2267\n",
      "Epoch 459/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4895 - acc: 0.3957 - val_loss: 1.9936 - val_acc: 0.2433\n",
      "Epoch 460/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4913 - acc: 0.3886 - val_loss: 1.9933 - val_acc: 0.2333\n",
      "Epoch 461/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4897 - acc: 0.3929 - val_loss: 2.0052 - val_acc: 0.2300\n",
      "Epoch 462/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4898 - acc: 0.3843 - val_loss: 1.9914 - val_acc: 0.2267\n",
      "Epoch 463/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4888 - acc: 0.3843 - val_loss: 1.9987 - val_acc: 0.2400\n",
      "Epoch 464/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4885 - acc: 0.3900 - val_loss: 1.9885 - val_acc: 0.2267\n",
      "Epoch 465/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4889 - acc: 0.3871 - val_loss: 1.9908 - val_acc: 0.2300\n",
      "Epoch 466/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4883 - acc: 0.3886 - val_loss: 2.0003 - val_acc: 0.2300\n",
      "Epoch 467/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4886 - acc: 0.3829 - val_loss: 2.0014 - val_acc: 0.2300\n",
      "Epoch 468/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4880 - acc: 0.3814 - val_loss: 1.9927 - val_acc: 0.2267\n",
      "Epoch 469/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4871 - acc: 0.3929 - val_loss: 2.0189 - val_acc: 0.2367\n",
      "Epoch 470/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4885 - acc: 0.3843 - val_loss: 2.0092 - val_acc: 0.2233\n",
      "Epoch 471/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4875 - acc: 0.3914 - val_loss: 2.0069 - val_acc: 0.2233\n",
      "Epoch 472/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4861 - acc: 0.3943 - val_loss: 2.0041 - val_acc: 0.2400\n",
      "Epoch 473/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4866 - acc: 0.3814 - val_loss: 2.0056 - val_acc: 0.2367\n",
      "Epoch 474/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4853 - acc: 0.3914 - val_loss: 1.9926 - val_acc: 0.2300\n",
      "Epoch 475/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4859 - acc: 0.3886 - val_loss: 2.0037 - val_acc: 0.2200\n",
      "Epoch 476/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4849 - acc: 0.3871 - val_loss: 2.0031 - val_acc: 0.2233\n",
      "Epoch 477/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4856 - acc: 0.3986 - val_loss: 2.0059 - val_acc: 0.2233\n",
      "Epoch 478/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4853 - acc: 0.3957 - val_loss: 2.0055 - val_acc: 0.2233\n",
      "Epoch 479/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4850 - acc: 0.3871 - val_loss: 1.9969 - val_acc: 0.2233\n",
      "Epoch 480/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4843 - acc: 0.3943 - val_loss: 2.0085 - val_acc: 0.2367\n",
      "Epoch 481/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4839 - acc: 0.3871 - val_loss: 2.0121 - val_acc: 0.2300\n",
      "Epoch 482/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3986 - val_loss: 2.0167 - val_acc: 0.2333\n",
      "Epoch 483/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4838 - acc: 0.3971 - val_loss: 2.0131 - val_acc: 0.2367\n",
      "Epoch 484/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4830 - acc: 0.3971 - val_loss: 2.0100 - val_acc: 0.2367\n",
      "Epoch 485/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4828 - acc: 0.3943 - val_loss: 2.0073 - val_acc: 0.2233\n",
      "Epoch 486/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4826 - acc: 0.3886 - val_loss: 2.0043 - val_acc: 0.2433\n",
      "Epoch 487/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4829 - acc: 0.3929 - val_loss: 2.0045 - val_acc: 0.2433\n",
      "Epoch 488/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3914 - val_loss: 2.0111 - val_acc: 0.2500\n",
      "Epoch 489/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4823 - acc: 0.3857 - val_loss: 1.9993 - val_acc: 0.2300\n",
      "Epoch 490/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4829 - acc: 0.3900 - val_loss: 1.9969 - val_acc: 0.2333\n",
      "Epoch 491/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3886 - val_loss: 2.0160 - val_acc: 0.2267\n",
      "Epoch 492/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4818 - acc: 0.3914 - val_loss: 1.9975 - val_acc: 0.2367\n",
      "Epoch 493/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4818 - acc: 0.3914 - val_loss: 2.0060 - val_acc: 0.2300\n",
      "Epoch 494/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4822 - acc: 0.3957 - val_loss: 2.0025 - val_acc: 0.2200\n",
      "Epoch 495/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4814 - acc: 0.3943 - val_loss: 2.0115 - val_acc: 0.2267\n",
      "Epoch 496/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4809 - acc: 0.3914 - val_loss: 2.0094 - val_acc: 0.2433\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4791 - acc: 0.3900 - val_loss: 2.0083 - val_acc: 0.2467\n",
      "Epoch 498/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4789 - acc: 0.3829 - val_loss: 2.0400 - val_acc: 0.2333\n",
      "Epoch 499/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4795 - acc: 0.3971 - val_loss: 2.0216 - val_acc: 0.2367\n",
      "Epoch 500/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4802 - acc: 0.3914 - val_loss: 2.0197 - val_acc: 0.2367\n",
      "Epoch 501/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4802 - acc: 0.3943 - val_loss: 2.0239 - val_acc: 0.2333\n",
      "Epoch 502/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4782 - acc: 0.3857 - val_loss: 2.0197 - val_acc: 0.2267\n",
      "Epoch 503/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4787 - acc: 0.4000 - val_loss: 2.0164 - val_acc: 0.2267\n",
      "Epoch 504/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4780 - acc: 0.3957 - val_loss: 2.0225 - val_acc: 0.2300\n",
      "Epoch 505/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4781 - acc: 0.3957 - val_loss: 2.0160 - val_acc: 0.2367\n",
      "Epoch 506/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4778 - acc: 0.3929 - val_loss: 2.0122 - val_acc: 0.2267\n",
      "Epoch 507/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4791 - acc: 0.3957 - val_loss: 2.0176 - val_acc: 0.2233\n",
      "Epoch 508/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4771 - acc: 0.3871 - val_loss: 2.0212 - val_acc: 0.2267\n",
      "Epoch 509/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3900 - val_loss: 2.0354 - val_acc: 0.2333\n",
      "Epoch 510/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4769 - acc: 0.3886 - val_loss: 2.0209 - val_acc: 0.2267\n",
      "Epoch 511/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4766 - acc: 0.3943 - val_loss: 2.0118 - val_acc: 0.2233\n",
      "Epoch 512/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3929 - val_loss: 2.0107 - val_acc: 0.2467\n",
      "Epoch 513/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4757 - acc: 0.3929 - val_loss: 2.0175 - val_acc: 0.2433\n",
      "Epoch 514/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4753 - acc: 0.3929 - val_loss: 2.0245 - val_acc: 0.2267\n",
      "Epoch 515/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4758 - acc: 0.3943 - val_loss: 2.0252 - val_acc: 0.2300\n",
      "Epoch 516/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4768 - acc: 0.3814 - val_loss: 2.0210 - val_acc: 0.2233\n",
      "Epoch 517/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4731 - acc: 0.3943 - val_loss: 2.0334 - val_acc: 0.2533\n",
      "Epoch 518/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4767 - acc: 0.3943 - val_loss: 2.0246 - val_acc: 0.2233\n",
      "Epoch 519/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4750 - acc: 0.4014 - val_loss: 2.0207 - val_acc: 0.2300\n",
      "Epoch 520/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4744 - acc: 0.3914 - val_loss: 2.0298 - val_acc: 0.2333\n",
      "Epoch 521/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4739 - acc: 0.4086 - val_loss: 2.0077 - val_acc: 0.2367\n",
      "Epoch 522/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4752 - acc: 0.3986 - val_loss: 2.0317 - val_acc: 0.2267\n",
      "Epoch 523/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4748 - acc: 0.3971 - val_loss: 2.0275 - val_acc: 0.2267\n",
      "Epoch 524/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4738 - acc: 0.4000 - val_loss: 2.0195 - val_acc: 0.2300\n",
      "Epoch 525/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4723 - acc: 0.3957 - val_loss: 2.0243 - val_acc: 0.2433\n",
      "Epoch 526/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4739 - acc: 0.3943 - val_loss: 2.0249 - val_acc: 0.2300\n",
      "Epoch 527/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4729 - acc: 0.3914 - val_loss: 2.0279 - val_acc: 0.2300\n",
      "Epoch 528/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4720 - acc: 0.3929 - val_loss: 2.0264 - val_acc: 0.2300\n",
      "Epoch 529/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4709 - acc: 0.3857 - val_loss: 2.0281 - val_acc: 0.2333\n",
      "Epoch 530/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4714 - acc: 0.3986 - val_loss: 2.0404 - val_acc: 0.2333\n",
      "Epoch 531/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4717 - acc: 0.3871 - val_loss: 2.0220 - val_acc: 0.2433\n",
      "Epoch 532/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4719 - acc: 0.3971 - val_loss: 2.0278 - val_acc: 0.2433\n",
      "Epoch 533/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4711 - acc: 0.4014 - val_loss: 2.0229 - val_acc: 0.2267\n",
      "Epoch 534/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4718 - acc: 0.3929 - val_loss: 2.0229 - val_acc: 0.2333\n",
      "Epoch 535/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4703 - acc: 0.3957 - val_loss: 2.0438 - val_acc: 0.2367\n",
      "Epoch 536/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4699 - acc: 0.3971 - val_loss: 2.0469 - val_acc: 0.2267\n",
      "Epoch 537/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4707 - acc: 0.3957 - val_loss: 2.0252 - val_acc: 0.2300\n",
      "Epoch 538/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3886 - val_loss: 2.0354 - val_acc: 0.2333\n",
      "Epoch 539/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4721 - acc: 0.3914 - val_loss: 2.0221 - val_acc: 0.2233\n",
      "Epoch 540/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4706 - acc: 0.3929 - val_loss: 2.0374 - val_acc: 0.2267\n",
      "Epoch 541/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3971 - val_loss: 2.0332 - val_acc: 0.2267\n",
      "Epoch 542/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4696 - acc: 0.4029 - val_loss: 2.0388 - val_acc: 0.2367\n",
      "Epoch 543/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4702 - acc: 0.3943 - val_loss: 2.0315 - val_acc: 0.2333\n",
      "Epoch 544/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4687 - acc: 0.3929 - val_loss: 2.0206 - val_acc: 0.2400\n",
      "Epoch 545/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4696 - acc: 0.3971 - val_loss: 2.0244 - val_acc: 0.2333\n",
      "Epoch 546/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4667 - acc: 0.3957 - val_loss: 2.0416 - val_acc: 0.2400\n",
      "Epoch 547/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4688 - acc: 0.3943 - val_loss: 2.0474 - val_acc: 0.2333\n",
      "Epoch 548/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4675 - acc: 0.3914 - val_loss: 2.0228 - val_acc: 0.2267\n",
      "Epoch 549/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.4014 - val_loss: 2.0209 - val_acc: 0.2267\n",
      "Epoch 550/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4671 - acc: 0.4000 - val_loss: 2.0328 - val_acc: 0.2300\n",
      "Epoch 551/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.3886 - val_loss: 2.0336 - val_acc: 0.2233\n",
      "Epoch 552/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4662 - acc: 0.4057 - val_loss: 2.0288 - val_acc: 0.2300\n",
      "Epoch 553/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4688 - acc: 0.3929 - val_loss: 2.0340 - val_acc: 0.2300\n",
      "Epoch 554/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4656 - acc: 0.3986 - val_loss: 2.0402 - val_acc: 0.2333\n",
      "Epoch 555/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4672 - acc: 0.3929 - val_loss: 2.0409 - val_acc: 0.2333\n",
      "Epoch 556/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4654 - acc: 0.3929 - val_loss: 2.0352 - val_acc: 0.2433\n",
      "Epoch 557/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.4071 - val_loss: 2.0511 - val_acc: 0.2567\n",
      "Epoch 558/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4664 - acc: 0.3986 - val_loss: 2.0338 - val_acc: 0.2333\n",
      "Epoch 559/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4645 - acc: 0.4043 - val_loss: 2.0342 - val_acc: 0.2533\n",
      "Epoch 560/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.4043 - val_loss: 2.0385 - val_acc: 0.2567\n",
      "Epoch 561/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4656 - acc: 0.3971 - val_loss: 2.0298 - val_acc: 0.2300\n",
      "Epoch 562/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4652 - acc: 0.3986 - val_loss: 2.0289 - val_acc: 0.2233\n",
      "Epoch 563/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4645 - acc: 0.3957 - val_loss: 2.0290 - val_acc: 0.2267\n",
      "Epoch 564/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4643 - acc: 0.4086 - val_loss: 2.0464 - val_acc: 0.2267\n",
      "Epoch 565/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4638 - acc: 0.3986 - val_loss: 2.0328 - val_acc: 0.2333\n",
      "Epoch 566/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4655 - acc: 0.4000 - val_loss: 2.0359 - val_acc: 0.2267\n",
      "Epoch 567/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4631 - acc: 0.4100 - val_loss: 2.0478 - val_acc: 0.2233\n",
      "Epoch 568/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4634 - acc: 0.4029 - val_loss: 2.0424 - val_acc: 0.2333\n",
      "Epoch 569/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4625 - acc: 0.4014 - val_loss: 2.0367 - val_acc: 0.2400\n",
      "Epoch 570/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.4029 - val_loss: 2.0452 - val_acc: 0.2367\n",
      "Epoch 571/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3986 - val_loss: 2.0460 - val_acc: 0.2233\n",
      "Epoch 572/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4612 - acc: 0.3929 - val_loss: 2.0320 - val_acc: 0.2367\n",
      "Epoch 573/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4607 - acc: 0.4071 - val_loss: 2.0507 - val_acc: 0.2433\n",
      "Epoch 574/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.4000 - val_loss: 2.0376 - val_acc: 0.2500\n",
      "Epoch 575/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.4071 - val_loss: 2.0383 - val_acc: 0.2267\n",
      "Epoch 576/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3957 - val_loss: 2.0421 - val_acc: 0.2233\n",
      "Epoch 577/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4617 - acc: 0.3986 - val_loss: 2.0395 - val_acc: 0.2267\n",
      "Epoch 578/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4613 - acc: 0.4014 - val_loss: 2.0404 - val_acc: 0.2333\n",
      "Epoch 579/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4618 - acc: 0.4043 - val_loss: 2.0367 - val_acc: 0.2267\n",
      "Epoch 580/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4610 - acc: 0.4043 - val_loss: 2.0449 - val_acc: 0.2400\n",
      "Epoch 581/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4604 - acc: 0.3971 - val_loss: 2.0392 - val_acc: 0.2533\n",
      "Epoch 582/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4605 - acc: 0.4029 - val_loss: 2.0497 - val_acc: 0.2300\n",
      "Epoch 583/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4598 - acc: 0.3957 - val_loss: 2.0478 - val_acc: 0.2267\n",
      "Epoch 584/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4014 - val_loss: 2.0461 - val_acc: 0.2300\n",
      "Epoch 585/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4591 - acc: 0.4086 - val_loss: 2.0510 - val_acc: 0.2367\n",
      "Epoch 586/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4585 - acc: 0.3943 - val_loss: 2.0506 - val_acc: 0.2467\n",
      "Epoch 587/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4609 - acc: 0.4057 - val_loss: 2.0503 - val_acc: 0.2333\n",
      "Epoch 588/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4595 - acc: 0.4014 - val_loss: 2.0526 - val_acc: 0.2333\n",
      "Epoch 589/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4587 - acc: 0.4000 - val_loss: 2.0523 - val_acc: 0.2267\n",
      "Epoch 590/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4591 - acc: 0.3971 - val_loss: 2.0442 - val_acc: 0.2267\n",
      "Epoch 591/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4588 - acc: 0.4043 - val_loss: 2.0577 - val_acc: 0.2267\n",
      "Epoch 592/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4581 - acc: 0.4029 - val_loss: 2.0519 - val_acc: 0.2333\n",
      "Epoch 593/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4581 - acc: 0.4014 - val_loss: 2.0436 - val_acc: 0.2267\n",
      "Epoch 594/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4575 - acc: 0.4171 - val_loss: 2.0450 - val_acc: 0.2333\n",
      "Epoch 595/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4576 - acc: 0.4043 - val_loss: 2.0488 - val_acc: 0.2367\n",
      "Epoch 596/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4573 - acc: 0.4043 - val_loss: 2.0384 - val_acc: 0.2267\n",
      "Epoch 597/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4071 - val_loss: 2.0432 - val_acc: 0.2500\n",
      "Epoch 598/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4572 - acc: 0.3971 - val_loss: 2.0432 - val_acc: 0.2267\n",
      "Epoch 599/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4579 - acc: 0.3986 - val_loss: 2.0420 - val_acc: 0.2333\n",
      "Epoch 600/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4100 - val_loss: 2.0577 - val_acc: 0.2300\n",
      "Epoch 601/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4560 - acc: 0.4129 - val_loss: 2.0533 - val_acc: 0.2333\n",
      "Epoch 602/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4562 - acc: 0.3986 - val_loss: 2.0521 - val_acc: 0.2333\n",
      "Epoch 603/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4566 - acc: 0.4057 - val_loss: 2.0396 - val_acc: 0.2400\n",
      "Epoch 604/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4000 - val_loss: 2.0416 - val_acc: 0.2400\n",
      "Epoch 605/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4549 - acc: 0.4014 - val_loss: 2.0490 - val_acc: 0.2367\n",
      "Epoch 606/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4555 - acc: 0.4071 - val_loss: 2.0435 - val_acc: 0.2333\n",
      "Epoch 607/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4556 - acc: 0.4086 - val_loss: 2.0465 - val_acc: 0.2333\n",
      "Epoch 608/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4552 - acc: 0.4100 - val_loss: 2.0416 - val_acc: 0.2367\n",
      "Epoch 609/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4536 - acc: 0.4071 - val_loss: 2.0576 - val_acc: 0.2567\n",
      "Epoch 610/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4546 - acc: 0.4086 - val_loss: 2.0575 - val_acc: 0.2300\n",
      "Epoch 611/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4544 - acc: 0.4029 - val_loss: 2.0581 - val_acc: 0.2300\n",
      "Epoch 612/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4545 - acc: 0.3986 - val_loss: 2.0522 - val_acc: 0.2400\n",
      "Epoch 613/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4542 - acc: 0.4043 - val_loss: 2.0624 - val_acc: 0.2267\n",
      "Epoch 614/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4533 - acc: 0.4100 - val_loss: 2.0598 - val_acc: 0.2433\n",
      "Epoch 615/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4539 - acc: 0.4029 - val_loss: 2.0534 - val_acc: 0.2467\n",
      "Epoch 616/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4534 - acc: 0.4143 - val_loss: 2.0666 - val_acc: 0.2267\n",
      "Epoch 617/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4086 - val_loss: 2.0603 - val_acc: 0.2567\n",
      "Epoch 618/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4531 - acc: 0.4014 - val_loss: 2.0535 - val_acc: 0.2233\n",
      "Epoch 619/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4518 - acc: 0.4129 - val_loss: 2.0618 - val_acc: 0.2367\n",
      "Epoch 620/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4524 - acc: 0.4100 - val_loss: 2.0531 - val_acc: 0.2333\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4525 - acc: 0.4157 - val_loss: 2.0670 - val_acc: 0.2367\n",
      "Epoch 622/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4086 - val_loss: 2.0651 - val_acc: 0.2367\n",
      "Epoch 623/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4143 - val_loss: 2.0684 - val_acc: 0.2300\n",
      "Epoch 624/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4518 - acc: 0.4114 - val_loss: 2.0629 - val_acc: 0.2333\n",
      "Epoch 625/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4057 - val_loss: 2.0662 - val_acc: 0.2433\n",
      "Epoch 626/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4509 - acc: 0.4043 - val_loss: 2.0456 - val_acc: 0.2333\n",
      "Epoch 627/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4186 - val_loss: 2.0548 - val_acc: 0.2333\n",
      "Epoch 628/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4071 - val_loss: 2.0463 - val_acc: 0.2400\n",
      "Epoch 629/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4514 - acc: 0.3986 - val_loss: 2.0621 - val_acc: 0.2367\n",
      "Epoch 630/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4086 - val_loss: 2.0471 - val_acc: 0.2333\n",
      "Epoch 631/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4114 - val_loss: 2.0739 - val_acc: 0.2400\n",
      "Epoch 632/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4057 - val_loss: 2.0651 - val_acc: 0.2367\n",
      "Epoch 633/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4495 - acc: 0.4143 - val_loss: 2.0757 - val_acc: 0.2400\n",
      "Epoch 634/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4498 - acc: 0.4100 - val_loss: 2.0606 - val_acc: 0.2333\n",
      "Epoch 635/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4483 - acc: 0.3986 - val_loss: 2.0583 - val_acc: 0.2467\n",
      "Epoch 636/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4503 - acc: 0.4143 - val_loss: 2.0615 - val_acc: 0.2333\n",
      "Epoch 637/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4492 - acc: 0.4057 - val_loss: 2.0699 - val_acc: 0.2400\n",
      "Epoch 638/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4493 - acc: 0.4143 - val_loss: 2.0735 - val_acc: 0.2300\n",
      "Epoch 639/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4489 - acc: 0.4071 - val_loss: 2.0728 - val_acc: 0.2300\n",
      "Epoch 640/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4114 - val_loss: 2.0513 - val_acc: 0.2333\n",
      "Epoch 641/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4100 - val_loss: 2.0665 - val_acc: 0.2400\n",
      "Epoch 642/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4490 - acc: 0.4071 - val_loss: 2.0735 - val_acc: 0.2400\n",
      "Epoch 643/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4086 - val_loss: 2.0756 - val_acc: 0.2333\n",
      "Epoch 644/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4086 - val_loss: 2.0664 - val_acc: 0.2367\n",
      "Epoch 645/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4114 - val_loss: 2.0746 - val_acc: 0.2367\n",
      "Epoch 646/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4453 - acc: 0.4057 - val_loss: 2.0637 - val_acc: 0.2600\n",
      "Epoch 647/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4465 - acc: 0.4171 - val_loss: 2.0640 - val_acc: 0.2333\n",
      "Epoch 648/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4086 - val_loss: 2.0785 - val_acc: 0.2367\n",
      "Epoch 649/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4455 - acc: 0.3986 - val_loss: 2.0791 - val_acc: 0.2367\n",
      "Epoch 650/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4473 - acc: 0.4100 - val_loss: 2.0653 - val_acc: 0.2367\n",
      "Epoch 651/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4460 - acc: 0.4100 - val_loss: 2.0845 - val_acc: 0.2433\n",
      "Epoch 652/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4469 - acc: 0.4114 - val_loss: 2.0714 - val_acc: 0.2333\n",
      "Epoch 653/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4454 - acc: 0.4143 - val_loss: 2.0709 - val_acc: 0.2467\n",
      "Epoch 654/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4457 - acc: 0.4100 - val_loss: 2.0587 - val_acc: 0.2467\n",
      "Epoch 655/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4445 - acc: 0.4100 - val_loss: 2.0687 - val_acc: 0.2467\n",
      "Epoch 656/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4468 - acc: 0.4071 - val_loss: 2.0814 - val_acc: 0.2400\n",
      "Epoch 657/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4086 - val_loss: 2.0708 - val_acc: 0.2300\n",
      "Epoch 658/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4086 - val_loss: 2.0697 - val_acc: 0.2400\n",
      "Epoch 659/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4445 - acc: 0.4129 - val_loss: 2.0711 - val_acc: 0.2433\n",
      "Epoch 660/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4057 - val_loss: 2.0699 - val_acc: 0.2333\n",
      "Epoch 661/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4459 - acc: 0.4100 - val_loss: 2.0678 - val_acc: 0.2300\n",
      "Epoch 662/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4452 - acc: 0.4043 - val_loss: 2.0722 - val_acc: 0.2300\n",
      "Epoch 663/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4100 - val_loss: 2.0857 - val_acc: 0.2367\n",
      "Epoch 664/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4435 - acc: 0.4129 - val_loss: 2.0755 - val_acc: 0.2400\n",
      "Epoch 665/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4439 - acc: 0.4129 - val_loss: 2.0815 - val_acc: 0.2400\n",
      "Epoch 666/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4430 - acc: 0.4143 - val_loss: 2.0707 - val_acc: 0.2467\n",
      "Epoch 667/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4447 - acc: 0.4100 - val_loss: 2.0664 - val_acc: 0.2333\n",
      "Epoch 668/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4414 - acc: 0.4086 - val_loss: 2.0834 - val_acc: 0.2600\n",
      "Epoch 669/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4423 - acc: 0.4171 - val_loss: 2.0858 - val_acc: 0.2367\n",
      "Epoch 670/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4437 - acc: 0.4143 - val_loss: 2.0744 - val_acc: 0.2333\n",
      "Epoch 671/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4419 - acc: 0.4143 - val_loss: 2.0755 - val_acc: 0.2467\n",
      "Epoch 672/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4426 - acc: 0.4071 - val_loss: 2.0779 - val_acc: 0.2333\n",
      "Epoch 673/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4428 - acc: 0.4129 - val_loss: 2.0744 - val_acc: 0.2400\n",
      "Epoch 674/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4418 - acc: 0.4057 - val_loss: 2.0970 - val_acc: 0.2400\n",
      "Epoch 675/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4408 - acc: 0.4157 - val_loss: 2.0948 - val_acc: 0.2567\n",
      "Epoch 676/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4410 - acc: 0.4100 - val_loss: 2.0670 - val_acc: 0.2567\n",
      "Epoch 677/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4402 - acc: 0.4114 - val_loss: 2.0928 - val_acc: 0.2367\n",
      "Epoch 678/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4397 - acc: 0.4171 - val_loss: 2.0680 - val_acc: 0.2300\n",
      "Epoch 679/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4412 - acc: 0.4129 - val_loss: 2.0827 - val_acc: 0.2400\n",
      "Epoch 680/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4421 - acc: 0.4157 - val_loss: 2.0817 - val_acc: 0.2367\n",
      "Epoch 681/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4417 - acc: 0.4129 - val_loss: 2.0773 - val_acc: 0.2300\n",
      "Epoch 682/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4403 - acc: 0.4143 - val_loss: 2.0782 - val_acc: 0.2567\n",
      "Epoch 683/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4400 - acc: 0.4043 - val_loss: 2.0694 - val_acc: 0.2500\n",
      "Epoch 684/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4370 - acc: 0.4129 - val_loss: 2.0818 - val_acc: 0.2567\n",
      "Epoch 685/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4398 - acc: 0.4100 - val_loss: 2.0778 - val_acc: 0.2467\n",
      "Epoch 686/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4396 - acc: 0.4129 - val_loss: 2.0978 - val_acc: 0.2533\n",
      "Epoch 687/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4200 - val_loss: 2.0703 - val_acc: 0.2367\n",
      "Epoch 688/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4399 - acc: 0.4171 - val_loss: 2.0820 - val_acc: 0.2367\n",
      "Epoch 689/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4392 - acc: 0.4157 - val_loss: 2.0833 - val_acc: 0.2433\n",
      "Epoch 690/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4389 - acc: 0.4171 - val_loss: 2.0892 - val_acc: 0.2367\n",
      "Epoch 691/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4379 - acc: 0.4200 - val_loss: 2.0895 - val_acc: 0.2367\n",
      "Epoch 692/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4383 - acc: 0.4143 - val_loss: 2.0810 - val_acc: 0.2367\n",
      "Epoch 693/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4390 - acc: 0.4057 - val_loss: 2.0778 - val_acc: 0.2367\n",
      "Epoch 694/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4378 - acc: 0.4157 - val_loss: 2.1022 - val_acc: 0.2467\n",
      "Epoch 695/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4129 - val_loss: 2.0801 - val_acc: 0.2533\n",
      "Epoch 696/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4366 - acc: 0.4171 - val_loss: 2.0926 - val_acc: 0.2400\n",
      "Epoch 697/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4379 - acc: 0.4143 - val_loss: 2.0968 - val_acc: 0.2500\n",
      "Epoch 698/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4377 - acc: 0.4171 - val_loss: 2.0885 - val_acc: 0.2467\n",
      "Epoch 699/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4372 - acc: 0.4186 - val_loss: 2.0887 - val_acc: 0.2333\n",
      "Epoch 700/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4370 - acc: 0.4129 - val_loss: 2.0866 - val_acc: 0.2500\n",
      "Epoch 701/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4361 - acc: 0.4171 - val_loss: 2.0821 - val_acc: 0.2500\n",
      "Epoch 702/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4369 - acc: 0.4129 - val_loss: 2.0839 - val_acc: 0.2400\n",
      "Epoch 703/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4359 - acc: 0.4214 - val_loss: 2.1036 - val_acc: 0.2367\n",
      "Epoch 704/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4368 - acc: 0.4143 - val_loss: 2.0850 - val_acc: 0.2467\n",
      "Epoch 705/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4114 - val_loss: 2.0787 - val_acc: 0.2367\n",
      "Epoch 706/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4186 - val_loss: 2.0729 - val_acc: 0.2400\n",
      "Epoch 707/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4200 - val_loss: 2.0847 - val_acc: 0.2400\n",
      "Epoch 708/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4359 - acc: 0.4171 - val_loss: 2.0837 - val_acc: 0.2367\n",
      "Epoch 709/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4339 - acc: 0.4243 - val_loss: 2.1037 - val_acc: 0.2500\n",
      "Epoch 710/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4171 - val_loss: 2.0839 - val_acc: 0.2467\n",
      "Epoch 711/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4346 - acc: 0.4114 - val_loss: 2.0859 - val_acc: 0.2400\n",
      "Epoch 712/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4343 - acc: 0.4114 - val_loss: 2.0916 - val_acc: 0.2567\n",
      "Epoch 713/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4353 - acc: 0.4143 - val_loss: 2.0767 - val_acc: 0.2367\n",
      "Epoch 714/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4349 - acc: 0.4214 - val_loss: 2.0893 - val_acc: 0.2400\n",
      "Epoch 715/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4325 - acc: 0.4200 - val_loss: 2.0908 - val_acc: 0.2567\n",
      "Epoch 716/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4351 - acc: 0.4114 - val_loss: 2.0939 - val_acc: 0.2433\n",
      "Epoch 717/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4342 - acc: 0.4157 - val_loss: 2.0927 - val_acc: 0.2333\n",
      "Epoch 718/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4334 - acc: 0.4171 - val_loss: 2.1124 - val_acc: 0.2567\n",
      "Epoch 719/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4344 - acc: 0.4143 - val_loss: 2.1080 - val_acc: 0.2333\n",
      "Epoch 720/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4340 - acc: 0.4129 - val_loss: 2.0947 - val_acc: 0.2400\n",
      "Epoch 721/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4335 - acc: 0.4157 - val_loss: 2.0916 - val_acc: 0.2467\n",
      "Epoch 722/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4331 - acc: 0.4200 - val_loss: 2.0963 - val_acc: 0.2400\n",
      "Epoch 723/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4329 - acc: 0.4214 - val_loss: 2.0846 - val_acc: 0.2367\n",
      "Epoch 724/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4326 - acc: 0.4214 - val_loss: 2.0982 - val_acc: 0.2367\n",
      "Epoch 725/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4322 - acc: 0.4157 - val_loss: 2.1062 - val_acc: 0.2467\n",
      "Epoch 726/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4321 - acc: 0.4186 - val_loss: 2.0930 - val_acc: 0.2367\n",
      "Epoch 727/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4319 - acc: 0.4200 - val_loss: 2.1172 - val_acc: 0.2367\n",
      "Epoch 728/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4323 - acc: 0.4171 - val_loss: 2.0857 - val_acc: 0.2333\n",
      "Epoch 729/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4313 - acc: 0.4229 - val_loss: 2.1106 - val_acc: 0.2500\n",
      "Epoch 730/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4316 - acc: 0.4243 - val_loss: 2.1139 - val_acc: 0.2367\n",
      "Epoch 731/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4157 - val_loss: 2.0997 - val_acc: 0.2400\n",
      "Epoch 732/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4303 - acc: 0.4214 - val_loss: 2.0973 - val_acc: 0.2367\n",
      "Epoch 733/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4310 - acc: 0.4200 - val_loss: 2.1017 - val_acc: 0.2400\n",
      "Epoch 734/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4318 - acc: 0.4143 - val_loss: 2.1059 - val_acc: 0.2367\n",
      "Epoch 735/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4307 - acc: 0.4200 - val_loss: 2.0999 - val_acc: 0.2367\n",
      "Epoch 736/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4293 - acc: 0.4200 - val_loss: 2.0935 - val_acc: 0.2400\n",
      "Epoch 737/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4311 - acc: 0.4229 - val_loss: 2.1092 - val_acc: 0.2433\n",
      "Epoch 738/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4308 - acc: 0.4157 - val_loss: 2.1016 - val_acc: 0.2433\n",
      "Epoch 739/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4293 - acc: 0.4214 - val_loss: 2.1035 - val_acc: 0.2400\n",
      "Epoch 740/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4297 - acc: 0.4229 - val_loss: 2.1015 - val_acc: 0.2433\n",
      "Epoch 741/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4289 - acc: 0.4229 - val_loss: 2.1090 - val_acc: 0.2500\n",
      "Epoch 742/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4293 - acc: 0.4143 - val_loss: 2.1001 - val_acc: 0.2500\n",
      "Epoch 743/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4279 - acc: 0.4271 - val_loss: 2.0947 - val_acc: 0.2567\n",
      "Epoch 744/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4299 - acc: 0.4214 - val_loss: 2.0957 - val_acc: 0.2400\n",
      "Epoch 745/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4314 - val_loss: 2.1087 - val_acc: 0.2567\n",
      "Epoch 746/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4295 - acc: 0.4257 - val_loss: 2.1028 - val_acc: 0.2367\n",
      "Epoch 747/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4292 - acc: 0.4186 - val_loss: 2.1138 - val_acc: 0.2467\n",
      "Epoch 748/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4272 - acc: 0.4186 - val_loss: 2.1167 - val_acc: 0.2567\n",
      "Epoch 749/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4259 - acc: 0.4214 - val_loss: 2.0883 - val_acc: 0.2367\n",
      "Epoch 750/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4281 - acc: 0.4286 - val_loss: 2.0928 - val_acc: 0.2367\n",
      "Epoch 751/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4266 - acc: 0.4200 - val_loss: 2.1053 - val_acc: 0.2367\n",
      "Epoch 752/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4274 - acc: 0.4271 - val_loss: 2.1052 - val_acc: 0.2367\n",
      "Epoch 753/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4262 - acc: 0.4214 - val_loss: 2.1255 - val_acc: 0.2433\n",
      "Epoch 754/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4275 - acc: 0.4214 - val_loss: 2.1079 - val_acc: 0.2367\n",
      "Epoch 755/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4271 - val_loss: 2.1072 - val_acc: 0.2500\n",
      "Epoch 756/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4129 - val_loss: 2.1071 - val_acc: 0.2400\n",
      "Epoch 757/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4274 - acc: 0.4129 - val_loss: 2.1130 - val_acc: 0.2467\n",
      "Epoch 758/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4260 - acc: 0.4243 - val_loss: 2.1075 - val_acc: 0.2467\n",
      "Epoch 759/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4263 - acc: 0.4257 - val_loss: 2.1050 - val_acc: 0.2400\n",
      "Epoch 760/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4258 - acc: 0.4300 - val_loss: 2.1086 - val_acc: 0.2400\n",
      "Epoch 761/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4171 - val_loss: 2.1253 - val_acc: 0.2433\n",
      "Epoch 762/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4265 - acc: 0.4271 - val_loss: 2.1160 - val_acc: 0.2400\n",
      "Epoch 763/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4229 - val_loss: 2.1047 - val_acc: 0.2367\n",
      "Epoch 764/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4266 - acc: 0.4229 - val_loss: 2.1187 - val_acc: 0.2433\n",
      "Epoch 765/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4243 - acc: 0.4171 - val_loss: 2.1285 - val_acc: 0.2367\n",
      "Epoch 766/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4200 - val_loss: 2.1048 - val_acc: 0.2367\n",
      "Epoch 767/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4255 - acc: 0.4243 - val_loss: 2.1066 - val_acc: 0.2400\n",
      "Epoch 768/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4245 - acc: 0.4300 - val_loss: 2.1010 - val_acc: 0.2400\n",
      "Epoch 769/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4244 - acc: 0.4229 - val_loss: 2.1083 - val_acc: 0.2367\n",
      "Epoch 770/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4241 - acc: 0.4229 - val_loss: 2.0997 - val_acc: 0.2367\n",
      "Epoch 771/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4233 - acc: 0.4186 - val_loss: 2.1207 - val_acc: 0.2400\n",
      "Epoch 772/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4241 - acc: 0.4200 - val_loss: 2.1175 - val_acc: 0.2367\n",
      "Epoch 773/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4240 - acc: 0.4157 - val_loss: 2.1263 - val_acc: 0.2500\n",
      "Epoch 774/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4229 - acc: 0.4157 - val_loss: 2.1183 - val_acc: 0.2433\n",
      "Epoch 775/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4219 - acc: 0.4286 - val_loss: 2.1207 - val_acc: 0.2533\n",
      "Epoch 776/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4230 - acc: 0.4214 - val_loss: 2.1128 - val_acc: 0.2433\n",
      "Epoch 777/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4218 - acc: 0.4229 - val_loss: 2.1289 - val_acc: 0.2400\n",
      "Epoch 778/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4231 - acc: 0.4186 - val_loss: 2.1162 - val_acc: 0.2400\n",
      "Epoch 779/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4232 - acc: 0.4300 - val_loss: 2.1254 - val_acc: 0.2367\n",
      "Epoch 780/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4228 - acc: 0.4200 - val_loss: 2.1080 - val_acc: 0.2400\n",
      "Epoch 781/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4235 - acc: 0.4200 - val_loss: 2.1130 - val_acc: 0.2467\n",
      "Epoch 782/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4223 - acc: 0.4200 - val_loss: 2.0979 - val_acc: 0.2400\n",
      "Epoch 783/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4226 - acc: 0.4271 - val_loss: 2.1164 - val_acc: 0.2433\n",
      "Epoch 784/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4221 - acc: 0.4286 - val_loss: 2.1187 - val_acc: 0.2400\n",
      "Epoch 785/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4219 - acc: 0.4271 - val_loss: 2.1295 - val_acc: 0.2400\n",
      "Epoch 786/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4218 - acc: 0.4243 - val_loss: 2.1178 - val_acc: 0.2500\n",
      "Epoch 787/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4225 - acc: 0.4200 - val_loss: 2.1137 - val_acc: 0.2467\n",
      "Epoch 788/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4215 - acc: 0.4300 - val_loss: 2.1160 - val_acc: 0.2400\n",
      "Epoch 789/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4212 - acc: 0.4243 - val_loss: 2.1133 - val_acc: 0.2400\n",
      "Epoch 790/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4215 - acc: 0.4243 - val_loss: 2.1176 - val_acc: 0.2433\n",
      "Epoch 791/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4205 - acc: 0.4229 - val_loss: 2.1190 - val_acc: 0.2500\n",
      "Epoch 792/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4209 - acc: 0.4200 - val_loss: 2.1086 - val_acc: 0.2400\n",
      "Epoch 793/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4203 - acc: 0.4286 - val_loss: 2.1259 - val_acc: 0.2500\n",
      "Epoch 794/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4208 - acc: 0.4229 - val_loss: 2.1226 - val_acc: 0.2400\n",
      "Epoch 795/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4205 - acc: 0.4286 - val_loss: 2.1178 - val_acc: 0.2400\n",
      "Epoch 796/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4257 - val_loss: 2.1183 - val_acc: 0.2333\n",
      "Epoch 797/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4201 - acc: 0.4214 - val_loss: 2.1250 - val_acc: 0.2367\n",
      "Epoch 798/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4329 - val_loss: 2.1254 - val_acc: 0.2400\n",
      "Epoch 799/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4286 - val_loss: 2.1162 - val_acc: 0.2533\n",
      "Epoch 800/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4186 - val_loss: 2.1261 - val_acc: 0.2400\n",
      "Epoch 801/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4200 - acc: 0.4243 - val_loss: 2.1241 - val_acc: 0.2400\n",
      "Epoch 802/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4200 - acc: 0.4271 - val_loss: 2.1566 - val_acc: 0.2400\n",
      "Epoch 803/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4183 - acc: 0.4300 - val_loss: 2.1187 - val_acc: 0.2500\n",
      "Epoch 804/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.421 - ETA: 0s - loss: 1.4185 - acc: 0.4257 - val_loss: 2.1203 - val_acc: 0.2433\n",
      "Epoch 805/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4180 - acc: 0.4286 - val_loss: 2.1152 - val_acc: 0.2500\n",
      "Epoch 806/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4188 - acc: 0.4271 - val_loss: 2.1190 - val_acc: 0.2433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4314 - val_loss: 2.1173 - val_acc: 0.2400\n",
      "Epoch 808/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4181 - acc: 0.4214 - val_loss: 2.1241 - val_acc: 0.2467\n",
      "Epoch 809/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4200 - val_loss: 2.1313 - val_acc: 0.2467\n",
      "Epoch 810/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4177 - acc: 0.4314 - val_loss: 2.1302 - val_acc: 0.2433\n",
      "Epoch 811/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4180 - acc: 0.4300 - val_loss: 2.1257 - val_acc: 0.2467\n",
      "Epoch 812/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4178 - acc: 0.4243 - val_loss: 2.1162 - val_acc: 0.2400\n",
      "Epoch 813/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4179 - acc: 0.4314 - val_loss: 2.1317 - val_acc: 0.2367\n",
      "Epoch 814/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4167 - acc: 0.4243 - val_loss: 2.1246 - val_acc: 0.2600\n",
      "Epoch 815/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4178 - acc: 0.4243 - val_loss: 2.1273 - val_acc: 0.2467\n",
      "Epoch 816/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4171 - acc: 0.4329 - val_loss: 2.1193 - val_acc: 0.2433\n",
      "Epoch 817/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4156 - acc: 0.4229 - val_loss: 2.1248 - val_acc: 0.2400\n",
      "Epoch 818/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4162 - acc: 0.4271 - val_loss: 2.1352 - val_acc: 0.2400\n",
      "Epoch 819/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4148 - acc: 0.4371 - val_loss: 2.1390 - val_acc: 0.2633\n",
      "Epoch 820/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4169 - acc: 0.4300 - val_loss: 2.1305 - val_acc: 0.2600\n",
      "Epoch 821/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4170 - acc: 0.4214 - val_loss: 2.1416 - val_acc: 0.2433\n",
      "Epoch 822/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4168 - acc: 0.4257 - val_loss: 2.1268 - val_acc: 0.2467\n",
      "Epoch 823/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4155 - acc: 0.4271 - val_loss: 2.1293 - val_acc: 0.2433\n",
      "Epoch 824/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4257 - val_loss: 2.1326 - val_acc: 0.2467\n",
      "Epoch 825/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4151 - acc: 0.4329 - val_loss: 2.1430 - val_acc: 0.2467\n",
      "Epoch 826/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4161 - acc: 0.4214 - val_loss: 2.1383 - val_acc: 0.2467\n",
      "Epoch 827/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4146 - acc: 0.4300 - val_loss: 2.1391 - val_acc: 0.2400\n",
      "Epoch 828/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4156 - acc: 0.4343 - val_loss: 2.1273 - val_acc: 0.2400\n",
      "Epoch 829/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4271 - val_loss: 2.1350 - val_acc: 0.2433\n",
      "Epoch 830/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4144 - acc: 0.4300 - val_loss: 2.1349 - val_acc: 0.2433\n",
      "Epoch 831/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4141 - acc: 0.4386 - val_loss: 2.1467 - val_acc: 0.2467\n",
      "Epoch 832/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4150 - acc: 0.4329 - val_loss: 2.1501 - val_acc: 0.2400\n",
      "Epoch 833/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4257 - val_loss: 2.1552 - val_acc: 0.2333\n",
      "Epoch 834/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4146 - acc: 0.4271 - val_loss: 2.1622 - val_acc: 0.2400\n",
      "Epoch 835/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4133 - acc: 0.4271 - val_loss: 2.1342 - val_acc: 0.2400\n",
      "Epoch 836/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4134 - acc: 0.4343 - val_loss: 2.1388 - val_acc: 0.2467\n",
      "Epoch 837/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4141 - acc: 0.4243 - val_loss: 2.1291 - val_acc: 0.2367\n",
      "Epoch 838/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4271 - val_loss: 2.1382 - val_acc: 0.2467\n",
      "Epoch 839/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4125 - acc: 0.4300 - val_loss: 2.1359 - val_acc: 0.2367\n",
      "Epoch 840/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4118 - acc: 0.4314 - val_loss: 2.1303 - val_acc: 0.2467\n",
      "Epoch 841/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4131 - acc: 0.4343 - val_loss: 2.1368 - val_acc: 0.2433\n",
      "Epoch 842/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4135 - acc: 0.4329 - val_loss: 2.1541 - val_acc: 0.2467\n",
      "Epoch 843/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4300 - val_loss: 2.1356 - val_acc: 0.2400\n",
      "Epoch 844/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4120 - acc: 0.4300 - val_loss: 2.1507 - val_acc: 0.2433\n",
      "Epoch 845/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4271 - val_loss: 2.1315 - val_acc: 0.2467\n",
      "Epoch 846/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4127 - acc: 0.4286 - val_loss: 2.1377 - val_acc: 0.2433\n",
      "Epoch 847/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4243 - val_loss: 2.1440 - val_acc: 0.2433\n",
      "Epoch 848/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4118 - acc: 0.4314 - val_loss: 2.1451 - val_acc: 0.2567\n",
      "Epoch 849/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4314 - val_loss: 2.1434 - val_acc: 0.2467\n",
      "Epoch 850/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4109 - acc: 0.4243 - val_loss: 2.1356 - val_acc: 0.2467\n",
      "Epoch 851/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4107 - acc: 0.4186 - val_loss: 2.1361 - val_acc: 0.2367\n",
      "Epoch 852/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4125 - acc: 0.4286 - val_loss: 2.1363 - val_acc: 0.2400\n",
      "Epoch 853/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4101 - acc: 0.4286 - val_loss: 2.1317 - val_acc: 0.2367\n",
      "Epoch 854/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4085 - acc: 0.4314 - val_loss: 2.1582 - val_acc: 0.2633\n",
      "Epoch 855/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4110 - acc: 0.4286 - val_loss: 2.1354 - val_acc: 0.2433\n",
      "Epoch 856/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4314 - val_loss: 2.1539 - val_acc: 0.2467\n",
      "Epoch 857/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4108 - acc: 0.4271 - val_loss: 2.1481 - val_acc: 0.2467\n",
      "Epoch 858/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4106 - acc: 0.4271 - val_loss: 2.1422 - val_acc: 0.2400\n",
      "Epoch 859/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4088 - acc: 0.4371 - val_loss: 2.1440 - val_acc: 0.2367\n",
      "Epoch 860/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4098 - acc: 0.4300 - val_loss: 2.1637 - val_acc: 0.2400\n",
      "Epoch 861/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4101 - acc: 0.4314 - val_loss: 2.1551 - val_acc: 0.2533\n",
      "Epoch 862/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4111 - acc: 0.4229 - val_loss: 2.1559 - val_acc: 0.2467\n",
      "Epoch 863/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4092 - acc: 0.4329 - val_loss: 2.1578 - val_acc: 0.2533\n",
      "Epoch 864/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4093 - acc: 0.4314 - val_loss: 2.1494 - val_acc: 0.2533\n",
      "Epoch 865/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4083 - acc: 0.4329 - val_loss: 2.1566 - val_acc: 0.2667\n",
      "Epoch 866/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4087 - acc: 0.4329 - val_loss: 2.1246 - val_acc: 0.2467\n",
      "Epoch 867/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4343 - val_loss: 2.1551 - val_acc: 0.2400\n",
      "Epoch 868/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4094 - acc: 0.4286 - val_loss: 2.1458 - val_acc: 0.2467\n",
      "Epoch 869/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4089 - acc: 0.4343 - val_loss: 2.1472 - val_acc: 0.2433\n",
      "Epoch 870/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4071 - acc: 0.4329 - val_loss: 2.1579 - val_acc: 0.2467\n",
      "Epoch 871/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4087 - acc: 0.4314 - val_loss: 2.1659 - val_acc: 0.2433\n",
      "Epoch 872/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4077 - acc: 0.4343 - val_loss: 2.1481 - val_acc: 0.2600\n",
      "Epoch 873/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4080 - acc: 0.4271 - val_loss: 2.1412 - val_acc: 0.2467\n",
      "Epoch 874/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4083 - acc: 0.4271 - val_loss: 2.1433 - val_acc: 0.2433\n",
      "Epoch 875/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4076 - acc: 0.4371 - val_loss: 2.1579 - val_acc: 0.2467\n",
      "Epoch 876/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4075 - acc: 0.4314 - val_loss: 2.1382 - val_acc: 0.2500\n",
      "Epoch 877/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4069 - acc: 0.4314 - val_loss: 2.1698 - val_acc: 0.2433\n",
      "Epoch 878/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4271 - val_loss: 2.1569 - val_acc: 0.2433\n",
      "Epoch 879/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4371 - val_loss: 2.1563 - val_acc: 0.2333\n",
      "Epoch 880/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4300 - val_loss: 2.1441 - val_acc: 0.2367\n",
      "Epoch 881/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4071 - acc: 0.4314 - val_loss: 2.1735 - val_acc: 0.2467\n",
      "Epoch 882/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4072 - acc: 0.4329 - val_loss: 2.1595 - val_acc: 0.2500\n",
      "Epoch 883/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4081 - acc: 0.4186 - val_loss: 2.1389 - val_acc: 0.2433\n",
      "Epoch 884/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4066 - acc: 0.4329 - val_loss: 2.1679 - val_acc: 0.2633\n",
      "Epoch 885/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4068 - acc: 0.4271 - val_loss: 2.1327 - val_acc: 0.2433\n",
      "Epoch 886/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4074 - acc: 0.4329 - val_loss: 2.1536 - val_acc: 0.2533\n",
      "Epoch 887/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4064 - acc: 0.4329 - val_loss: 2.1489 - val_acc: 0.2500\n",
      "Epoch 888/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4063 - acc: 0.4329 - val_loss: 2.1556 - val_acc: 0.2467\n",
      "Epoch 889/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4371 - val_loss: 2.1645 - val_acc: 0.2600\n",
      "Epoch 890/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4066 - acc: 0.4343 - val_loss: 2.1524 - val_acc: 0.2633\n",
      "Epoch 891/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4061 - acc: 0.4286 - val_loss: 2.1607 - val_acc: 0.2533\n",
      "Epoch 892/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4064 - acc: 0.4371 - val_loss: 2.1633 - val_acc: 0.2533\n",
      "Epoch 893/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4057 - acc: 0.4286 - val_loss: 2.1678 - val_acc: 0.2533\n",
      "Epoch 894/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4386 - val_loss: 2.1537 - val_acc: 0.2433\n",
      "Epoch 895/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4051 - acc: 0.4243 - val_loss: 2.1625 - val_acc: 0.2500\n",
      "Epoch 896/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4343 - val_loss: 2.1679 - val_acc: 0.2667\n",
      "Epoch 897/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4055 - acc: 0.4300 - val_loss: 2.1667 - val_acc: 0.2600\n",
      "Epoch 898/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4371 - val_loss: 2.1714 - val_acc: 0.2533\n",
      "Epoch 899/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4052 - acc: 0.4300 - val_loss: 2.1501 - val_acc: 0.2500\n",
      "Epoch 900/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4429 - val_loss: 2.1544 - val_acc: 0.2533\n",
      "Epoch 901/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4357 - val_loss: 2.1689 - val_acc: 0.2667\n",
      "Epoch 902/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4041 - acc: 0.4343 - val_loss: 2.1560 - val_acc: 0.2533\n",
      "Epoch 903/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4031 - acc: 0.4343 - val_loss: 2.1733 - val_acc: 0.2567\n",
      "Epoch 904/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4329 - val_loss: 2.1624 - val_acc: 0.2367\n",
      "Epoch 905/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4046 - acc: 0.4371 - val_loss: 2.1470 - val_acc: 0.2433\n",
      "Epoch 906/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4037 - acc: 0.4314 - val_loss: 2.1468 - val_acc: 0.2433\n",
      "Epoch 907/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4042 - acc: 0.4343 - val_loss: 2.1732 - val_acc: 0.2533\n",
      "Epoch 908/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4300 - val_loss: 2.1625 - val_acc: 0.2467\n",
      "Epoch 909/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4033 - acc: 0.4329 - val_loss: 2.1684 - val_acc: 0.2533\n",
      "Epoch 910/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4040 - acc: 0.4329 - val_loss: 2.1493 - val_acc: 0.2467\n",
      "Epoch 911/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4371 - val_loss: 2.1680 - val_acc: 0.2433\n",
      "Epoch 912/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4034 - acc: 0.4343 - val_loss: 2.1562 - val_acc: 0.2433\n",
      "Epoch 913/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4400 - val_loss: 2.1748 - val_acc: 0.2500\n",
      "Epoch 914/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4024 - acc: 0.4371 - val_loss: 2.1751 - val_acc: 0.2633\n",
      "Epoch 915/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4343 - val_loss: 2.1639 - val_acc: 0.2533\n",
      "Epoch 916/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4357 - val_loss: 2.1734 - val_acc: 0.2433\n",
      "Epoch 917/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4017 - acc: 0.4429 - val_loss: 2.1643 - val_acc: 0.2433\n",
      "Epoch 918/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4314 - val_loss: 2.1685 - val_acc: 0.2600\n",
      "Epoch 919/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4023 - acc: 0.4314 - val_loss: 2.1746 - val_acc: 0.2467\n",
      "Epoch 920/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4300 - val_loss: 2.1579 - val_acc: 0.2500\n",
      "Epoch 921/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4022 - acc: 0.4314 - val_loss: 2.1600 - val_acc: 0.2433\n",
      "Epoch 922/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4371 - val_loss: 2.1725 - val_acc: 0.2533\n",
      "Epoch 923/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4003 - acc: 0.4371 - val_loss: 2.1593 - val_acc: 0.2600\n",
      "Epoch 924/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4429 - val_loss: 2.1683 - val_acc: 0.2533\n",
      "Epoch 925/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4300 - val_loss: 2.1721 - val_acc: 0.2433\n",
      "Epoch 926/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4010 - acc: 0.4414 - val_loss: 2.1640 - val_acc: 0.2533\n",
      "Epoch 927/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4015 - acc: 0.4271 - val_loss: 2.1699 - val_acc: 0.2467\n",
      "Epoch 928/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4400 - val_loss: 2.1871 - val_acc: 0.2433\n",
      "Epoch 929/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4357 - val_loss: 2.1807 - val_acc: 0.2467\n",
      "Epoch 930/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4010 - acc: 0.4371 - val_loss: 2.1737 - val_acc: 0.2533\n",
      "Epoch 931/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4006 - acc: 0.4357 - val_loss: 2.1665 - val_acc: 0.2467\n",
      "Epoch 932/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4001 - acc: 0.4429 - val_loss: 2.1571 - val_acc: 0.2500\n",
      "Epoch 933/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4314 - val_loss: 2.1697 - val_acc: 0.2500\n",
      "Epoch 934/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3998 - acc: 0.4386 - val_loss: 2.1744 - val_acc: 0.2533\n",
      "Epoch 935/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4006 - acc: 0.4371 - val_loss: 2.1715 - val_acc: 0.2500\n",
      "Epoch 936/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4004 - acc: 0.4400 - val_loss: 2.1774 - val_acc: 0.2567\n",
      "Epoch 937/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3999 - acc: 0.4314 - val_loss: 2.1737 - val_acc: 0.2433\n",
      "Epoch 938/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3993 - acc: 0.4386 - val_loss: 2.1766 - val_acc: 0.2533\n",
      "Epoch 939/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3991 - acc: 0.4357 - val_loss: 2.1887 - val_acc: 0.2533\n",
      "Epoch 940/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3999 - acc: 0.4329 - val_loss: 2.1668 - val_acc: 0.2433\n",
      "Epoch 941/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3994 - acc: 0.4314 - val_loss: 2.1707 - val_acc: 0.2467\n",
      "Epoch 942/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3986 - acc: 0.4371 - val_loss: 2.1558 - val_acc: 0.2367\n",
      "Epoch 943/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3993 - acc: 0.4386 - val_loss: 2.1808 - val_acc: 0.2467\n",
      "Epoch 944/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3981 - acc: 0.4371 - val_loss: 2.1822 - val_acc: 0.2533\n",
      "Epoch 945/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4314 - val_loss: 2.1826 - val_acc: 0.2533\n",
      "Epoch 946/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4314 - val_loss: 2.1723 - val_acc: 0.2567\n",
      "Epoch 947/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4371 - val_loss: 2.1866 - val_acc: 0.2633\n",
      "Epoch 948/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3978 - acc: 0.4257 - val_loss: 2.1636 - val_acc: 0.2400\n",
      "Epoch 949/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3981 - acc: 0.4386 - val_loss: 2.1657 - val_acc: 0.2433\n",
      "Epoch 950/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3966 - acc: 0.4400 - val_loss: 2.1731 - val_acc: 0.2433\n",
      "Epoch 951/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3978 - acc: 0.4343 - val_loss: 2.1819 - val_acc: 0.2533\n",
      "Epoch 952/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4386 - val_loss: 2.1806 - val_acc: 0.2467\n",
      "Epoch 953/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4400 - val_loss: 2.1803 - val_acc: 0.2600\n",
      "Epoch 954/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4429 - val_loss: 2.1738 - val_acc: 0.2567\n",
      "Epoch 955/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4400 - val_loss: 2.1877 - val_acc: 0.2500\n",
      "Epoch 956/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3968 - acc: 0.4386 - val_loss: 2.1945 - val_acc: 0.2433\n",
      "Epoch 957/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3965 - acc: 0.4343 - val_loss: 2.1742 - val_acc: 0.2433\n",
      "Epoch 958/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4329 - val_loss: 2.2074 - val_acc: 0.2433\n",
      "Epoch 959/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3974 - acc: 0.4357 - val_loss: 2.1702 - val_acc: 0.2400\n",
      "Epoch 960/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4329 - val_loss: 2.1853 - val_acc: 0.2467\n",
      "Epoch 961/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3963 - acc: 0.4400 - val_loss: 2.1565 - val_acc: 0.2433\n",
      "Epoch 962/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3970 - acc: 0.4371 - val_loss: 2.1810 - val_acc: 0.2467\n",
      "Epoch 963/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3963 - acc: 0.4371 - val_loss: 2.1791 - val_acc: 0.2500\n",
      "Epoch 964/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3959 - acc: 0.4414 - val_loss: 2.1665 - val_acc: 0.2400\n",
      "Epoch 965/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3950 - acc: 0.4414 - val_loss: 2.1735 - val_acc: 0.2467\n",
      "Epoch 966/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3964 - acc: 0.4414 - val_loss: 2.1958 - val_acc: 0.2467\n",
      "Epoch 967/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3962 - acc: 0.4329 - val_loss: 2.1886 - val_acc: 0.2467\n",
      "Epoch 968/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3953 - acc: 0.4343 - val_loss: 2.1761 - val_acc: 0.2400\n",
      "Epoch 969/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3955 - acc: 0.4443 - val_loss: 2.1839 - val_acc: 0.2500\n",
      "Epoch 970/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3954 - acc: 0.4371 - val_loss: 2.1948 - val_acc: 0.2400\n",
      "Epoch 971/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3957 - acc: 0.4414 - val_loss: 2.1757 - val_acc: 0.2433\n",
      "Epoch 972/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3952 - acc: 0.4386 - val_loss: 2.1858 - val_acc: 0.2433\n",
      "Epoch 973/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4357 - val_loss: 2.1847 - val_acc: 0.2533\n",
      "Epoch 974/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3948 - acc: 0.4329 - val_loss: 2.1881 - val_acc: 0.2500\n",
      "Epoch 975/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3948 - acc: 0.4400 - val_loss: 2.1968 - val_acc: 0.2533\n",
      "Epoch 976/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4400 - val_loss: 2.1805 - val_acc: 0.2467\n",
      "Epoch 977/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4386 - val_loss: 2.1774 - val_acc: 0.2367\n",
      "Epoch 978/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4514 - val_loss: 2.1873 - val_acc: 0.2500\n",
      "Epoch 979/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3945 - acc: 0.4357 - val_loss: 2.1828 - val_acc: 0.2433\n",
      "Epoch 980/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3946 - acc: 0.4386 - val_loss: 2.1999 - val_acc: 0.2533\n",
      "Epoch 981/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3933 - acc: 0.4343 - val_loss: 2.2078 - val_acc: 0.2500\n",
      "Epoch 982/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4386 - val_loss: 2.1901 - val_acc: 0.2500\n",
      "Epoch 983/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3937 - acc: 0.4343 - val_loss: 2.1966 - val_acc: 0.2500\n",
      "Epoch 984/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4343 - val_loss: 2.1872 - val_acc: 0.2567\n",
      "Epoch 985/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3931 - acc: 0.4371 - val_loss: 2.1978 - val_acc: 0.2433\n",
      "Epoch 986/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3931 - acc: 0.4414 - val_loss: 2.1912 - val_acc: 0.2567\n",
      "Epoch 987/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3927 - acc: 0.4414 - val_loss: 2.1830 - val_acc: 0.2367\n",
      "Epoch 988/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4457 - val_loss: 2.2107 - val_acc: 0.2633\n",
      "Epoch 989/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4371 - val_loss: 2.2073 - val_acc: 0.2500\n",
      "Epoch 990/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3927 - acc: 0.4471 - val_loss: 2.2131 - val_acc: 0.2500\n",
      "Epoch 991/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3928 - acc: 0.4429 - val_loss: 2.1889 - val_acc: 0.2467\n",
      "Epoch 992/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3927 - acc: 0.4371 - val_loss: 2.1989 - val_acc: 0.2467\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3924 - acc: 0.4457 - val_loss: 2.1960 - val_acc: 0.2433\n",
      "Epoch 994/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3924 - acc: 0.4343 - val_loss: 2.1748 - val_acc: 0.2333\n",
      "Epoch 995/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4371 - val_loss: 2.1982 - val_acc: 0.2500\n",
      "Epoch 996/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3918 - acc: 0.4429 - val_loss: 2.2006 - val_acc: 0.2467\n",
      "Epoch 997/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3917 - acc: 0.4443 - val_loss: 2.1968 - val_acc: 0.2467\n",
      "Epoch 998/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3925 - acc: 0.4400 - val_loss: 2.2103 - val_acc: 0.2500\n",
      "Epoch 999/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3921 - acc: 0.4400 - val_loss: 2.1957 - val_acc: 0.2467\n",
      "Epoch 1000/1000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3919 - acc: 0.4386 - val_loss: 2.2078 - val_acc: 0.2433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x272d3b22358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "tb_hist = keras.callbacks.TensorBoard(log_dir='./graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[tb_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs : 0\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "epochs : 1\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2067 - acc: 0.1786 - val_loss: 2.1906 - val_acc: 0.1800\n",
      "epochs : 2\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1733 - acc: 0.1829 - val_loss: 2.1641 - val_acc: 0.1800\n",
      "epochs : 3\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1446 - acc: 0.1957 - val_loss: 2.1409 - val_acc: 0.1833\n",
      "epochs : 4\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1187 - acc: 0.1929 - val_loss: 2.1151 - val_acc: 0.2000\n",
      "epochs : 5\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0937 - acc: 0.2043 - val_loss: 2.0953 - val_acc: 0.1967\n",
      "epochs : 6\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0732 - acc: 0.2043 - val_loss: 2.0756 - val_acc: 0.2000\n",
      "epochs : 7\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0532 - acc: 0.2071 - val_loss: 2.0568 - val_acc: 0.2067\n",
      "epochs : 8\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0351 - acc: 0.2200 - val_loss: 2.0399 - val_acc: 0.2033\n",
      "epochs : 9\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0193 - acc: 0.2157 - val_loss: 2.0281 - val_acc: 0.2033\n",
      "epochs : 10\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0048 - acc: 0.2200 - val_loss: 2.0135 - val_acc: 0.2100\n",
      "epochs : 11\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9920 - acc: 0.2271 - val_loss: 2.0033 - val_acc: 0.2100\n",
      "epochs : 12\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9799 - acc: 0.2257 - val_loss: 1.9929 - val_acc: 0.2100\n",
      "epochs : 13\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9688 - acc: 0.2257 - val_loss: 1.9852 - val_acc: 0.2067\n",
      "epochs : 14\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9581 - acc: 0.2314 - val_loss: 1.9783 - val_acc: 0.2100\n",
      "epochs : 15\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9491 - acc: 0.2286 - val_loss: 1.9694 - val_acc: 0.2133\n",
      "epochs : 16\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9400 - acc: 0.2314 - val_loss: 1.9627 - val_acc: 0.2133\n",
      "epochs : 17\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9316 - acc: 0.2314 - val_loss: 1.9534 - val_acc: 0.2100\n",
      "epochs : 18\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9236 - acc: 0.2300 - val_loss: 1.9468 - val_acc: 0.2033\n",
      "epochs : 19\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9154 - acc: 0.2371 - val_loss: 1.9391 - val_acc: 0.2000\n",
      "epochs : 20\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9095 - acc: 0.2371 - val_loss: 1.9342 - val_acc: 0.2067\n",
      "epochs : 21\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9020 - acc: 0.2386 - val_loss: 1.9288 - val_acc: 0.2100\n",
      "epochs : 22\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8953 - acc: 0.2329 - val_loss: 1.9244 - val_acc: 0.1900\n",
      "epochs : 23\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8897 - acc: 0.2386 - val_loss: 1.9188 - val_acc: 0.2067\n",
      "epochs : 24\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8831 - acc: 0.2357 - val_loss: 1.9142 - val_acc: 0.2133\n",
      "epochs : 25\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8780 - acc: 0.2471 - val_loss: 1.9122 - val_acc: 0.2000\n",
      "epochs : 26\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8721 - acc: 0.2300 - val_loss: 1.9138 - val_acc: 0.2167\n",
      "epochs : 27\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8668 - acc: 0.2329 - val_loss: 1.9020 - val_acc: 0.2033\n",
      "epochs : 28\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2400 - val_loss: 1.9006 - val_acc: 0.1867\n",
      "epochs : 29\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8562 - acc: 0.2386 - val_loss: 1.8952 - val_acc: 0.2033\n",
      "epochs : 30\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8516 - acc: 0.2400 - val_loss: 1.8934 - val_acc: 0.2167\n",
      "epochs : 31\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8470 - acc: 0.2443 - val_loss: 1.8893 - val_acc: 0.1867\n",
      "epochs : 32\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8429 - acc: 0.2371 - val_loss: 1.8867 - val_acc: 0.1867\n",
      "epochs : 33\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8382 - acc: 0.2343 - val_loss: 1.8848 - val_acc: 0.2000\n",
      "epochs : 34\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8334 - acc: 0.2457 - val_loss: 1.8788 - val_acc: 0.1800\n",
      "epochs : 35\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8294 - acc: 0.2300 - val_loss: 1.8828 - val_acc: 0.2067\n",
      "epochs : 36\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8261 - acc: 0.2400 - val_loss: 1.8759 - val_acc: 0.2033\n",
      "epochs : 37\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8217 - acc: 0.2386 - val_loss: 1.8755 - val_acc: 0.2000\n",
      "epochs : 38\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8185 - acc: 0.2400 - val_loss: 1.8699 - val_acc: 0.2033\n",
      "epochs : 39\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8151 - acc: 0.2343 - val_loss: 1.8662 - val_acc: 0.1933\n",
      "epochs : 40\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8107 - acc: 0.2443 - val_loss: 1.8681 - val_acc: 0.1967\n",
      "epochs : 41\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8077 - acc: 0.2429 - val_loss: 1.8597 - val_acc: 0.1900\n",
      "epochs : 42\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8032 - acc: 0.2586 - val_loss: 1.8687 - val_acc: 0.1867\n",
      "epochs : 43\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8008 - acc: 0.2371 - val_loss: 1.8574 - val_acc: 0.1967\n",
      "epochs : 44\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7976 - acc: 0.2486 - val_loss: 1.8582 - val_acc: 0.1933\n",
      "epochs : 45\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7944 - acc: 0.2471 - val_loss: 1.8557 - val_acc: 0.1733\n",
      "epochs : 46\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7915 - acc: 0.2400 - val_loss: 1.8556 - val_acc: 0.1833\n",
      "epochs : 47\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7887 - acc: 0.2429 - val_loss: 1.8547 - val_acc: 0.2067\n",
      "epochs : 48\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7855 - acc: 0.2629 - val_loss: 1.8514 - val_acc: 0.1833\n",
      "epochs : 49\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7828 - acc: 0.2486 - val_loss: 1.8466 - val_acc: 0.2167\n",
      "epochs : 50\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7797 - acc: 0.2629 - val_loss: 1.8444 - val_acc: 0.1833\n",
      "epochs : 51\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7766 - acc: 0.2629 - val_loss: 1.8459 - val_acc: 0.2233\n",
      "epochs : 52\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7747 - acc: 0.2557 - val_loss: 1.8450 - val_acc: 0.2267\n",
      "epochs : 53\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7719 - acc: 0.2386 - val_loss: 1.8381 - val_acc: 0.2267\n",
      "epochs : 54\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7690 - acc: 0.2629 - val_loss: 1.8395 - val_acc: 0.2000\n",
      "epochs : 55\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7669 - acc: 0.2557 - val_loss: 1.8411 - val_acc: 0.2267\n",
      "epochs : 56\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7645 - acc: 0.2614 - val_loss: 1.8399 - val_acc: 0.2133\n",
      "epochs : 57\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7615 - acc: 0.2657 - val_loss: 1.8394 - val_acc: 0.2133\n",
      "epochs : 58\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7587 - acc: 0.2586 - val_loss: 1.8324 - val_acc: 0.2100\n",
      "epochs : 59\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7574 - acc: 0.2714 - val_loss: 1.8376 - val_acc: 0.2167\n",
      "epochs : 60\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7552 - acc: 0.2671 - val_loss: 1.8336 - val_acc: 0.2067\n",
      "epochs : 61\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7522 - acc: 0.2529 - val_loss: 1.8324 - val_acc: 0.2367\n",
      "epochs : 62\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7500 - acc: 0.2857 - val_loss: 1.8311 - val_acc: 0.2000\n",
      "epochs : 63\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7475 - acc: 0.2586 - val_loss: 1.8287 - val_acc: 0.2400\n",
      "epochs : 64\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7454 - acc: 0.2886 - val_loss: 1.8310 - val_acc: 0.2033\n",
      "epochs : 65\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7441 - acc: 0.2686 - val_loss: 1.8282 - val_acc: 0.2000\n",
      "epochs : 66\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7408 - acc: 0.2800 - val_loss: 1.8244 - val_acc: 0.2433\n",
      "epochs : 67\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7388 - acc: 0.2871 - val_loss: 1.8313 - val_acc: 0.2000\n",
      "epochs : 68\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7378 - acc: 0.2671 - val_loss: 1.8272 - val_acc: 0.2167\n",
      "epochs : 69\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7359 - acc: 0.2757 - val_loss: 1.8250 - val_acc: 0.2067\n",
      "epochs : 70\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7343 - acc: 0.2686 - val_loss: 1.8232 - val_acc: 0.1967\n",
      "epochs : 71\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7307 - acc: 0.2743 - val_loss: 1.8268 - val_acc: 0.2467\n",
      "epochs : 72\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7306 - acc: 0.2957 - val_loss: 1.8216 - val_acc: 0.2100\n",
      "epochs : 73\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7280 - acc: 0.2900 - val_loss: 1.8301 - val_acc: 0.2067\n",
      "epochs : 74\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7254 - acc: 0.2857 - val_loss: 1.8145 - val_acc: 0.2733\n",
      "epochs : 75\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7253 - acc: 0.2914 - val_loss: 1.8178 - val_acc: 0.2200\n",
      "epochs : 76\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7239 - acc: 0.2957 - val_loss: 1.8209 - val_acc: 0.2233\n",
      "epochs : 77\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7212 - acc: 0.2914 - val_loss: 1.8180 - val_acc: 0.2067\n",
      "epochs : 78\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7199 - acc: 0.2986 - val_loss: 1.8259 - val_acc: 0.2000\n",
      "epochs : 79\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7182 - acc: 0.2814 - val_loss: 1.8226 - val_acc: 0.2067\n",
      "epochs : 80\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7161 - acc: 0.2857 - val_loss: 1.8163 - val_acc: 0.2433\n",
      "epochs : 81\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7147 - acc: 0.2971 - val_loss: 1.8203 - val_acc: 0.2267\n",
      "epochs : 82\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7120 - acc: 0.2986 - val_loss: 1.8193 - val_acc: 0.2667\n",
      "epochs : 83\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7125 - acc: 0.2914 - val_loss: 1.8220 - val_acc: 0.2333\n",
      "epochs : 84\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7099 - acc: 0.2929 - val_loss: 1.8160 - val_acc: 0.2200\n",
      "epochs : 85\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7088 - acc: 0.2786 - val_loss: 1.8112 - val_acc: 0.2400\n",
      "epochs : 86\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7067 - acc: 0.3086 - val_loss: 1.8148 - val_acc: 0.2633\n",
      "epochs : 87\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7059 - acc: 0.2914 - val_loss: 1.8124 - val_acc: 0.2200\n",
      "epochs : 88\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7036 - acc: 0.2900 - val_loss: 1.8249 - val_acc: 0.2267\n",
      "epochs : 89\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7024 - acc: 0.2900 - val_loss: 1.8213 - val_acc: 0.2700\n",
      "epochs : 90\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7010 - acc: 0.3114 - val_loss: 1.8224 - val_acc: 0.2267\n",
      "epochs : 91\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6997 - acc: 0.2943 - val_loss: 1.8120 - val_acc: 0.2433\n",
      "epochs : 92\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6981 - acc: 0.2971 - val_loss: 1.8207 - val_acc: 0.2167\n",
      "epochs : 93\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6978 - acc: 0.2929 - val_loss: 1.8131 - val_acc: 0.2133\n",
      "epochs : 94\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6956 - acc: 0.2986 - val_loss: 1.8180 - val_acc: 0.2300\n",
      "epochs : 95\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6942 - acc: 0.3000 - val_loss: 1.8171 - val_acc: 0.2567\n",
      "epochs : 96\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6923 - acc: 0.3043 - val_loss: 1.8177 - val_acc: 0.2433\n",
      "epochs : 97\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6918 - acc: 0.2914 - val_loss: 1.8146 - val_acc: 0.2367\n",
      "epochs : 98\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6905 - acc: 0.3071 - val_loss: 1.8150 - val_acc: 0.2267\n",
      "epochs : 99\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6881 - acc: 0.3057 - val_loss: 1.8221 - val_acc: 0.2233\n",
      "epochs : 100\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6874 - acc: 0.3014 - val_loss: 1.8132 - val_acc: 0.2433\n",
      "epochs : 101\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6861 - acc: 0.3143 - val_loss: 1.8168 - val_acc: 0.2300\n",
      "epochs : 102\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6844 - acc: 0.3186 - val_loss: 1.8221 - val_acc: 0.2133\n",
      "epochs : 103\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6844 - acc: 0.2986 - val_loss: 1.8160 - val_acc: 0.2300\n",
      "epochs : 104\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6833 - acc: 0.2986 - val_loss: 1.8113 - val_acc: 0.2267\n",
      "epochs : 105\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6821 - acc: 0.3100 - val_loss: 1.8154 - val_acc: 0.2267\n",
      "epochs : 106\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6808 - acc: 0.3071 - val_loss: 1.8130 - val_acc: 0.2267\n",
      "epochs : 107\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6782 - acc: 0.3057 - val_loss: 1.8131 - val_acc: 0.2167\n",
      "epochs : 108\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6779 - acc: 0.3014 - val_loss: 1.8148 - val_acc: 0.2267\n",
      "epochs : 109\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6765 - acc: 0.3086 - val_loss: 1.8176 - val_acc: 0.2433\n",
      "epochs : 110\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6752 - acc: 0.3114 - val_loss: 1.8148 - val_acc: 0.2267\n",
      "epochs : 111\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6728 - acc: 0.3257 - val_loss: 1.8294 - val_acc: 0.2167\n",
      "epochs : 112\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6736 - acc: 0.3129 - val_loss: 1.8144 - val_acc: 0.2200\n",
      "epochs : 113\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6727 - acc: 0.3100 - val_loss: 1.8148 - val_acc: 0.2200\n",
      "epochs : 114\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6709 - acc: 0.3171 - val_loss: 1.8155 - val_acc: 0.2200\n",
      "epochs : 115\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6700 - acc: 0.3114 - val_loss: 1.8224 - val_acc: 0.2200\n",
      "epochs : 116\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6700 - acc: 0.3029 - val_loss: 1.8197 - val_acc: 0.2200\n",
      "epochs : 117\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6681 - acc: 0.3043 - val_loss: 1.8154 - val_acc: 0.2267\n",
      "epochs : 118\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6672 - acc: 0.3029 - val_loss: 1.8243 - val_acc: 0.2433\n",
      "epochs : 119\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6667 - acc: 0.3100 - val_loss: 1.8175 - val_acc: 0.2233\n",
      "epochs : 120\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6645 - acc: 0.3071 - val_loss: 1.8219 - val_acc: 0.2267\n",
      "epochs : 121\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6637 - acc: 0.3200 - val_loss: 1.8229 - val_acc: 0.2233\n",
      "epochs : 122\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6625 - acc: 0.3214 - val_loss: 1.8235 - val_acc: 0.2467\n",
      "epochs : 123\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6620 - acc: 0.3071 - val_loss: 1.8211 - val_acc: 0.2600\n",
      "epochs : 124\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6605 - acc: 0.3243 - val_loss: 1.8191 - val_acc: 0.2200\n",
      "epochs : 125\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6604 - acc: 0.3257 - val_loss: 1.8155 - val_acc: 0.2167\n",
      "epochs : 126\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6552 - acc: 0.3329 - val_loss: 1.8314 - val_acc: 0.2300\n",
      "epochs : 127\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6599 - acc: 0.3143 - val_loss: 1.8179 - val_acc: 0.2233\n",
      "epochs : 128\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6564 - acc: 0.3143 - val_loss: 1.8273 - val_acc: 0.2500\n",
      "epochs : 129\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6546 - acc: 0.3214 - val_loss: 1.8169 - val_acc: 0.2667\n",
      "epochs : 130\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6550 - acc: 0.3200 - val_loss: 1.8223 - val_acc: 0.2600\n",
      "epochs : 131\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6542 - acc: 0.3143 - val_loss: 1.8175 - val_acc: 0.2233\n",
      "epochs : 132\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6520 - acc: 0.3271 - val_loss: 1.8161 - val_acc: 0.2233\n",
      "epochs : 133\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6509 - acc: 0.3214 - val_loss: 1.8155 - val_acc: 0.2500\n",
      "epochs : 134\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6511 - acc: 0.3229 - val_loss: 1.8188 - val_acc: 0.2667\n",
      "epochs : 135\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6504 - acc: 0.3271 - val_loss: 1.8147 - val_acc: 0.2233\n",
      "epochs : 136\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6491 - acc: 0.3286 - val_loss: 1.8123 - val_acc: 0.2333\n",
      "epochs : 137\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6486 - acc: 0.3300 - val_loss: 1.8257 - val_acc: 0.2700\n",
      "epochs : 138\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6480 - acc: 0.3286 - val_loss: 1.8210 - val_acc: 0.2200\n",
      "epochs : 139\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6476 - acc: 0.3143 - val_loss: 1.8218 - val_acc: 0.2200\n",
      "epochs : 140\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6454 - acc: 0.3300 - val_loss: 1.8258 - val_acc: 0.2333\n",
      "epochs : 141\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6448 - acc: 0.3157 - val_loss: 1.8177 - val_acc: 0.2233\n",
      "epochs : 142\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6434 - acc: 0.3343 - val_loss: 1.8214 - val_acc: 0.2567\n",
      "epochs : 143\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6424 - acc: 0.3286 - val_loss: 1.8162 - val_acc: 0.2200\n",
      "epochs : 144\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6414 - acc: 0.3157 - val_loss: 1.8182 - val_acc: 0.2333\n",
      "epochs : 145\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6407 - acc: 0.3343 - val_loss: 1.8158 - val_acc: 0.2667\n",
      "epochs : 146\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6402 - acc: 0.3371 - val_loss: 1.8264 - val_acc: 0.2300\n",
      "epochs : 147\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6392 - acc: 0.3271 - val_loss: 1.8269 - val_acc: 0.2167\n",
      "epochs : 148\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6379 - acc: 0.3229 - val_loss: 1.8306 - val_acc: 0.2400\n",
      "epochs : 149\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6381 - acc: 0.3343 - val_loss: 1.8139 - val_acc: 0.2167\n",
      "epochs : 150\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6355 - acc: 0.3257 - val_loss: 1.8408 - val_acc: 0.2367\n",
      "epochs : 151\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6349 - acc: 0.3400 - val_loss: 1.8263 - val_acc: 0.2600\n",
      "epochs : 152\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6346 - acc: 0.3214 - val_loss: 1.8180 - val_acc: 0.2300\n",
      "epochs : 153\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6345 - acc: 0.3300 - val_loss: 1.8262 - val_acc: 0.2167\n",
      "epochs : 154\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6318 - acc: 0.3429 - val_loss: 1.8186 - val_acc: 0.2167\n",
      "epochs : 155\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6325 - acc: 0.3300 - val_loss: 1.8154 - val_acc: 0.2200\n",
      "epochs : 156\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6319 - acc: 0.3329 - val_loss: 1.8298 - val_acc: 0.2600\n",
      "epochs : 157\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6308 - acc: 0.3343 - val_loss: 1.8320 - val_acc: 0.2600\n",
      "epochs : 158\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6291 - acc: 0.3286 - val_loss: 1.8304 - val_acc: 0.2167\n",
      "epochs : 159\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6297 - acc: 0.3171 - val_loss: 1.8263 - val_acc: 0.2333\n",
      "epochs : 160\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6270 - acc: 0.3343 - val_loss: 1.8162 - val_acc: 0.2467\n",
      "epochs : 161\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6283 - acc: 0.3271 - val_loss: 1.8372 - val_acc: 0.2300\n",
      "epochs : 162\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6268 - acc: 0.3314 - val_loss: 1.8222 - val_acc: 0.2300\n",
      "epochs : 163\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6243 - acc: 0.3486 - val_loss: 1.8351 - val_acc: 0.2200\n",
      "epochs : 164\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6258 - acc: 0.3257 - val_loss: 1.8305 - val_acc: 0.2233\n",
      "epochs : 165\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6224 - acc: 0.3343 - val_loss: 1.8306 - val_acc: 0.2200\n",
      "epochs : 166\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6237 - acc: 0.3329 - val_loss: 1.8291 - val_acc: 0.2667\n",
      "epochs : 167\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6230 - acc: 0.3257 - val_loss: 1.8434 - val_acc: 0.2233\n",
      "epochs : 168\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6207 - acc: 0.3371 - val_loss: 1.8332 - val_acc: 0.2267\n",
      "epochs : 169\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6212 - acc: 0.3314 - val_loss: 1.8349 - val_acc: 0.2300\n",
      "epochs : 170\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6211 - acc: 0.3114 - val_loss: 1.8332 - val_acc: 0.2233\n",
      "epochs : 171\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6189 - acc: 0.3314 - val_loss: 1.8289 - val_acc: 0.2233\n",
      "epochs : 172\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6202 - acc: 0.3243 - val_loss: 1.8475 - val_acc: 0.2267\n",
      "epochs : 173\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6183 - acc: 0.3300 - val_loss: 1.8395 - val_acc: 0.2200\n",
      "epochs : 174\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6170 - acc: 0.3343 - val_loss: 1.8450 - val_acc: 0.2233\n",
      "epochs : 175\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6154 - acc: 0.3171 - val_loss: 1.8271 - val_acc: 0.2600\n",
      "epochs : 176\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6169 - acc: 0.3229 - val_loss: 1.8374 - val_acc: 0.2400\n",
      "epochs : 177\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6162 - acc: 0.3314 - val_loss: 1.8347 - val_acc: 0.2167\n",
      "epochs : 178\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6147 - acc: 0.3329 - val_loss: 1.8330 - val_acc: 0.2133\n",
      "epochs : 179\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6136 - acc: 0.3314 - val_loss: 1.8375 - val_acc: 0.2167\n",
      "epochs : 180\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6122 - acc: 0.3514 - val_loss: 1.8438 - val_acc: 0.2333\n",
      "epochs : 181\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6131 - acc: 0.3343 - val_loss: 1.8360 - val_acc: 0.2233\n",
      "epochs : 182\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6127 - acc: 0.3314 - val_loss: 1.8466 - val_acc: 0.2167\n",
      "epochs : 183\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6100 - acc: 0.3514 - val_loss: 1.8515 - val_acc: 0.2633\n",
      "epochs : 184\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6110 - acc: 0.3286 - val_loss: 1.8416 - val_acc: 0.2467\n",
      "epochs : 185\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6103 - acc: 0.3314 - val_loss: 1.8387 - val_acc: 0.2200\n",
      "epochs : 186\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6095 - acc: 0.3271 - val_loss: 1.8392 - val_acc: 0.2167\n",
      "epochs : 187\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6073 - acc: 0.3314 - val_loss: 1.8458 - val_acc: 0.2167\n",
      "epochs : 188\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6058 - acc: 0.3386 - val_loss: 1.8500 - val_acc: 0.2700\n",
      "epochs : 189\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6073 - acc: 0.3329 - val_loss: 1.8452 - val_acc: 0.2200\n",
      "epochs : 190\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6053 - acc: 0.3329 - val_loss: 1.8417 - val_acc: 0.2167\n",
      "epochs : 191\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6056 - acc: 0.3329 - val_loss: 1.8516 - val_acc: 0.2233\n",
      "epochs : 192\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6058 - acc: 0.3414 - val_loss: 1.8423 - val_acc: 0.2200\n",
      "epochs : 193\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6035 - acc: 0.3443 - val_loss: 1.8597 - val_acc: 0.2233\n",
      "epochs : 194\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6036 - acc: 0.3300 - val_loss: 1.8459 - val_acc: 0.2267\n",
      "epochs : 195\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5999 - acc: 0.3471 - val_loss: 1.8756 - val_acc: 0.2333\n",
      "epochs : 196\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6025 - acc: 0.3357 - val_loss: 1.8491 - val_acc: 0.2167\n",
      "epochs : 197\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6012 - acc: 0.3329 - val_loss: 1.8466 - val_acc: 0.2133\n",
      "epochs : 198\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6002 - acc: 0.3343 - val_loss: 1.8476 - val_acc: 0.2167\n",
      "epochs : 199\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5989 - acc: 0.3443 - val_loss: 1.8590 - val_acc: 0.2333\n",
      "epochs : 200\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5998 - acc: 0.3471 - val_loss: 1.8499 - val_acc: 0.2200\n",
      "epochs : 201\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5960 - acc: 0.3343 - val_loss: 1.8613 - val_acc: 0.2633\n",
      "epochs : 202\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5970 - acc: 0.3471 - val_loss: 1.8563 - val_acc: 0.2200\n",
      "epochs : 203\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5980 - acc: 0.3486 - val_loss: 1.8435 - val_acc: 0.2233\n",
      "epochs : 204\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5956 - acc: 0.3471 - val_loss: 1.8555 - val_acc: 0.2267\n",
      "epochs : 205\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5947 - acc: 0.3429 - val_loss: 1.8590 - val_acc: 0.2667\n",
      "epochs : 206\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5956 - acc: 0.3414 - val_loss: 1.8505 - val_acc: 0.2333\n",
      "epochs : 207\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5954 - acc: 0.3443 - val_loss: 1.8533 - val_acc: 0.2167\n",
      "epochs : 208\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5941 - acc: 0.3386 - val_loss: 1.8599 - val_acc: 0.2267\n",
      "epochs : 209\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5914 - acc: 0.3457 - val_loss: 1.8541 - val_acc: 0.2067\n",
      "epochs : 210\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5928 - acc: 0.3386 - val_loss: 1.8592 - val_acc: 0.2233\n",
      "epochs : 211\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5905 - acc: 0.3586 - val_loss: 1.8607 - val_acc: 0.2133\n",
      "epochs : 212\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5917 - acc: 0.3471 - val_loss: 1.8486 - val_acc: 0.2267\n",
      "epochs : 213\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5912 - acc: 0.3314 - val_loss: 1.8581 - val_acc: 0.2500\n",
      "epochs : 214\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5899 - acc: 0.3443 - val_loss: 1.8501 - val_acc: 0.2567\n",
      "epochs : 215\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5893 - acc: 0.3443 - val_loss: 1.8549 - val_acc: 0.2233\n",
      "epochs : 216\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5882 - acc: 0.3400 - val_loss: 1.8598 - val_acc: 0.2267\n",
      "epochs : 217\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5882 - acc: 0.3429 - val_loss: 1.8589 - val_acc: 0.2200\n",
      "epochs : 218\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5867 - acc: 0.3443 - val_loss: 1.8583 - val_acc: 0.2100\n",
      "epochs : 219\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5867 - acc: 0.3500 - val_loss: 1.8705 - val_acc: 0.2233\n",
      "epochs : 220\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5858 - acc: 0.3514 - val_loss: 1.8603 - val_acc: 0.2267\n",
      "epochs : 221\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5849 - acc: 0.3486 - val_loss: 1.8598 - val_acc: 0.2500\n",
      "epochs : 222\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5848 - acc: 0.3386 - val_loss: 1.8593 - val_acc: 0.2133\n",
      "epochs : 223\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5854 - acc: 0.3371 - val_loss: 1.8627 - val_acc: 0.2167\n",
      "epochs : 224\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5846 - acc: 0.3414 - val_loss: 1.8624 - val_acc: 0.2200\n",
      "epochs : 225\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5835 - acc: 0.3571 - val_loss: 1.8629 - val_acc: 0.2133\n",
      "epochs : 226\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5831 - acc: 0.3400 - val_loss: 1.8615 - val_acc: 0.2233\n",
      "epochs : 227\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5815 - acc: 0.3457 - val_loss: 1.8605 - val_acc: 0.2267\n",
      "epochs : 228\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5819 - acc: 0.3471 - val_loss: 1.8663 - val_acc: 0.2267\n",
      "epochs : 229\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5812 - acc: 0.3514 - val_loss: 1.8802 - val_acc: 0.2267\n",
      "epochs : 230\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5812 - acc: 0.3429 - val_loss: 1.8672 - val_acc: 0.2500\n",
      "epochs : 231\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5800 - acc: 0.3543 - val_loss: 1.8711 - val_acc: 0.2233\n",
      "epochs : 232\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5798 - acc: 0.3500 - val_loss: 1.8716 - val_acc: 0.2200\n",
      "epochs : 233\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5784 - acc: 0.3457 - val_loss: 1.8787 - val_acc: 0.2200\n",
      "epochs : 234\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5789 - acc: 0.3371 - val_loss: 1.8736 - val_acc: 0.2267\n",
      "epochs : 235\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5777 - acc: 0.3529 - val_loss: 1.8798 - val_acc: 0.2267\n",
      "epochs : 236\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5770 - acc: 0.3500 - val_loss: 1.8730 - val_acc: 0.2233\n",
      "epochs : 237\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5773 - acc: 0.3657 - val_loss: 1.8878 - val_acc: 0.2233\n",
      "epochs : 238\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5758 - acc: 0.3586 - val_loss: 1.8664 - val_acc: 0.2200\n",
      "epochs : 239\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5766 - acc: 0.3529 - val_loss: 1.8719 - val_acc: 0.2133\n",
      "epochs : 240\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5751 - acc: 0.3486 - val_loss: 1.8772 - val_acc: 0.2300\n",
      "epochs : 241\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5747 - acc: 0.3471 - val_loss: 1.8894 - val_acc: 0.2267\n",
      "epochs : 242\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5747 - acc: 0.3443 - val_loss: 1.8741 - val_acc: 0.2533\n",
      "epochs : 243\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5739 - acc: 0.3514 - val_loss: 1.8612 - val_acc: 0.2333\n",
      "epochs : 244\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5729 - acc: 0.3500 - val_loss: 1.8707 - val_acc: 0.2133\n",
      "epochs : 245\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5722 - acc: 0.3500 - val_loss: 1.8743 - val_acc: 0.2533\n",
      "epochs : 246\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5715 - acc: 0.3471 - val_loss: 1.8720 - val_acc: 0.2167\n",
      "epochs : 247\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5703 - acc: 0.3600 - val_loss: 1.8882 - val_acc: 0.2167\n",
      "epochs : 248\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5708 - acc: 0.3400 - val_loss: 1.8749 - val_acc: 0.2267\n",
      "epochs : 249\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5697 - acc: 0.3500 - val_loss: 1.8829 - val_acc: 0.2267\n",
      "epochs : 250\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5709 - acc: 0.3586 - val_loss: 1.8894 - val_acc: 0.2267\n",
      "epochs : 251\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5701 - acc: 0.3571 - val_loss: 1.8751 - val_acc: 0.2167\n",
      "epochs : 252\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5684 - acc: 0.3457 - val_loss: 1.8773 - val_acc: 0.2133\n",
      "epochs : 253\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5682 - acc: 0.3514 - val_loss: 1.8762 - val_acc: 0.2200\n",
      "epochs : 254\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5680 - acc: 0.3557 - val_loss: 1.8783 - val_acc: 0.2533\n",
      "epochs : 255\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5678 - acc: 0.3500 - val_loss: 1.8988 - val_acc: 0.2300\n",
      "epochs : 256\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5665 - acc: 0.3529 - val_loss: 1.8842 - val_acc: 0.2500\n",
      "epochs : 257\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5678 - acc: 0.3571 - val_loss: 1.8816 - val_acc: 0.2233\n",
      "epochs : 258\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5662 - acc: 0.3643 - val_loss: 1.9030 - val_acc: 0.2233\n",
      "epochs : 259\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5656 - acc: 0.3386 - val_loss: 1.8758 - val_acc: 0.2200\n",
      "epochs : 260\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5656 - acc: 0.3529 - val_loss: 1.8786 - val_acc: 0.2267\n",
      "epochs : 261\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5647 - acc: 0.3500 - val_loss: 1.8836 - val_acc: 0.2200\n",
      "epochs : 262\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5640 - acc: 0.3586 - val_loss: 1.8796 - val_acc: 0.2233\n",
      "epochs : 263\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5631 - acc: 0.3500 - val_loss: 1.8795 - val_acc: 0.2233\n",
      "epochs : 264\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5633 - acc: 0.3543 - val_loss: 1.8833 - val_acc: 0.2267\n",
      "epochs : 265\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5629 - acc: 0.3557 - val_loss: 1.8773 - val_acc: 0.2133\n",
      "epochs : 266\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5619 - acc: 0.3586 - val_loss: 1.8991 - val_acc: 0.2300\n",
      "epochs : 267\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5622 - acc: 0.3543 - val_loss: 1.8876 - val_acc: 0.2200\n",
      "epochs : 268\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5619 - acc: 0.3457 - val_loss: 1.8950 - val_acc: 0.2467\n",
      "epochs : 269\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5616 - acc: 0.3643 - val_loss: 1.8955 - val_acc: 0.2267\n",
      "epochs : 270\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5602 - acc: 0.3500 - val_loss: 1.8961 - val_acc: 0.2233\n",
      "epochs : 271\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5595 - acc: 0.3614 - val_loss: 1.8900 - val_acc: 0.2167\n",
      "epochs : 272\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5588 - acc: 0.3671 - val_loss: 1.8939 - val_acc: 0.2300\n",
      "epochs : 273\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5586 - acc: 0.3500 - val_loss: 1.8869 - val_acc: 0.2533\n",
      "epochs : 274\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5584 - acc: 0.3629 - val_loss: 1.8907 - val_acc: 0.2167\n",
      "epochs : 275\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5568 - acc: 0.3457 - val_loss: 1.8991 - val_acc: 0.2567\n",
      "epochs : 276\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5586 - acc: 0.3557 - val_loss: 1.8934 - val_acc: 0.2233\n",
      "epochs : 277\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5567 - acc: 0.3586 - val_loss: 1.8860 - val_acc: 0.2200\n",
      "epochs : 278\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5563 - acc: 0.3643 - val_loss: 1.8944 - val_acc: 0.2167\n",
      "epochs : 279\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5565 - acc: 0.3586 - val_loss: 1.9122 - val_acc: 0.2200\n",
      "epochs : 280\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5555 - acc: 0.3600 - val_loss: 1.9009 - val_acc: 0.2300\n",
      "epochs : 281\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5551 - acc: 0.3586 - val_loss: 1.8986 - val_acc: 0.2300\n",
      "epochs : 282\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5558 - acc: 0.3557 - val_loss: 1.8899 - val_acc: 0.2133\n",
      "epochs : 283\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5544 - acc: 0.3671 - val_loss: 1.8987 - val_acc: 0.2133\n",
      "epochs : 284\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5541 - acc: 0.3629 - val_loss: 1.9096 - val_acc: 0.2233\n",
      "epochs : 285\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5544 - acc: 0.3643 - val_loss: 1.9103 - val_acc: 0.2200\n",
      "epochs : 286\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5524 - acc: 0.3600 - val_loss: 1.8916 - val_acc: 0.2333\n",
      "epochs : 287\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5519 - acc: 0.3700 - val_loss: 1.8911 - val_acc: 0.2167\n",
      "epochs : 288\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5519 - acc: 0.3686 - val_loss: 1.9047 - val_acc: 0.2367\n",
      "epochs : 289\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5512 - acc: 0.3557 - val_loss: 1.9073 - val_acc: 0.2233\n",
      "epochs : 290\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5497 - acc: 0.3657 - val_loss: 1.9094 - val_acc: 0.2267\n",
      "epochs : 291\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5505 - acc: 0.3629 - val_loss: 1.8940 - val_acc: 0.2133\n",
      "epochs : 292\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5500 - acc: 0.3571 - val_loss: 1.8877 - val_acc: 0.2133\n",
      "epochs : 293\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5492 - acc: 0.3557 - val_loss: 1.9036 - val_acc: 0.2267\n",
      "epochs : 294\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5490 - acc: 0.3657 - val_loss: 1.9174 - val_acc: 0.2233\n",
      "epochs : 295\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5489 - acc: 0.3629 - val_loss: 1.9089 - val_acc: 0.2200\n",
      "epochs : 296\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5474 - acc: 0.3614 - val_loss: 1.8971 - val_acc: 0.2133\n",
      "epochs : 297\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5502 - acc: 0.3543 - val_loss: 1.9079 - val_acc: 0.2167\n",
      "epochs : 298\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5482 - acc: 0.3671 - val_loss: 1.9047 - val_acc: 0.2200\n",
      "epochs : 299\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5462 - acc: 0.3686 - val_loss: 1.9120 - val_acc: 0.2200\n",
      "epochs : 300\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5464 - acc: 0.3586 - val_loss: 1.8909 - val_acc: 0.2300\n",
      "epochs : 301\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5477 - acc: 0.3686 - val_loss: 1.8990 - val_acc: 0.2200\n",
      "epochs : 302\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5470 - acc: 0.3686 - val_loss: 1.9094 - val_acc: 0.2267\n",
      "epochs : 303\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5453 - acc: 0.3586 - val_loss: 1.9088 - val_acc: 0.2200\n",
      "epochs : 304\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5451 - acc: 0.3600 - val_loss: 1.9066 - val_acc: 0.2167\n",
      "epochs : 305\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5456 - acc: 0.3571 - val_loss: 1.8988 - val_acc: 0.2233\n",
      "epochs : 306\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5456 - acc: 0.3643 - val_loss: 1.9150 - val_acc: 0.2267\n",
      "epochs : 307\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5435 - acc: 0.3643 - val_loss: 1.9147 - val_acc: 0.2133\n",
      "epochs : 308\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5437 - acc: 0.3643 - val_loss: 1.9158 - val_acc: 0.2300\n",
      "epochs : 309\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5435 - acc: 0.3671 - val_loss: 1.9178 - val_acc: 0.2300\n",
      "epochs : 310\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3657 - val_loss: 1.9362 - val_acc: 0.2233\n",
      "epochs : 311\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5433 - acc: 0.3686 - val_loss: 1.9201 - val_acc: 0.2333\n",
      "epochs : 312\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5412 - acc: 0.3657 - val_loss: 1.9199 - val_acc: 0.2267\n",
      "epochs : 313\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5428 - acc: 0.3771 - val_loss: 1.9183 - val_acc: 0.2200\n",
      "epochs : 314\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5411 - acc: 0.3629 - val_loss: 1.8999 - val_acc: 0.2300\n",
      "epochs : 315\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5414 - acc: 0.3586 - val_loss: 1.9076 - val_acc: 0.2433\n",
      "epochs : 316\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5414 - acc: 0.3700 - val_loss: 1.9100 - val_acc: 0.2267\n",
      "epochs : 317\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5394 - acc: 0.3700 - val_loss: 1.9223 - val_acc: 0.2267\n",
      "epochs : 318\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5395 - acc: 0.3586 - val_loss: 1.9215 - val_acc: 0.2300\n",
      "epochs : 319\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5393 - acc: 0.3657 - val_loss: 1.9335 - val_acc: 0.2300\n",
      "epochs : 320\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5387 - acc: 0.3671 - val_loss: 1.9215 - val_acc: 0.2233\n",
      "epochs : 321\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5380 - acc: 0.3657 - val_loss: 1.9164 - val_acc: 0.2333\n",
      "epochs : 322\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3714 - val_loss: 1.9072 - val_acc: 0.2167\n",
      "epochs : 323\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5380 - acc: 0.3643 - val_loss: 1.9100 - val_acc: 0.2200\n",
      "epochs : 324\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5384 - acc: 0.3629 - val_loss: 1.9281 - val_acc: 0.2233\n",
      "epochs : 325\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5370 - acc: 0.3686 - val_loss: 1.9109 - val_acc: 0.2200\n",
      "epochs : 326\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5369 - acc: 0.3657 - val_loss: 1.9124 - val_acc: 0.2167\n",
      "epochs : 327\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5366 - acc: 0.3700 - val_loss: 1.9316 - val_acc: 0.2433\n",
      "epochs : 328\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5380 - acc: 0.3771 - val_loss: 1.9244 - val_acc: 0.2167\n",
      "epochs : 329\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5352 - acc: 0.3800 - val_loss: 1.9231 - val_acc: 0.2200\n",
      "epochs : 330\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.3743 - val_loss: 1.9309 - val_acc: 0.2300\n",
      "epochs : 331\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5353 - acc: 0.3743 - val_loss: 1.9301 - val_acc: 0.2233\n",
      "epochs : 332\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5337 - acc: 0.3657 - val_loss: 1.9361 - val_acc: 0.2300\n",
      "epochs : 333\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5337 - acc: 0.3714 - val_loss: 1.9366 - val_acc: 0.2233\n",
      "epochs : 334\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5338 - acc: 0.3714 - val_loss: 1.9289 - val_acc: 0.2333\n",
      "epochs : 335\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5339 - acc: 0.3586 - val_loss: 1.9326 - val_acc: 0.2233\n",
      "epochs : 336\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5328 - acc: 0.3757 - val_loss: 1.9258 - val_acc: 0.2200\n",
      "epochs : 337\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5319 - acc: 0.3671 - val_loss: 1.9255 - val_acc: 0.2300\n",
      "epochs : 338\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5311 - acc: 0.3643 - val_loss: 1.9215 - val_acc: 0.2367\n",
      "epochs : 339\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5320 - acc: 0.3729 - val_loss: 1.9343 - val_acc: 0.2233\n",
      "epochs : 340\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5294 - acc: 0.3771 - val_loss: 1.9168 - val_acc: 0.2167\n",
      "epochs : 341\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5298 - acc: 0.3657 - val_loss: 1.9596 - val_acc: 0.2300\n",
      "epochs : 342\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5277 - acc: 0.3757 - val_loss: 1.9345 - val_acc: 0.2167\n",
      "epochs : 343\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5320 - acc: 0.3643 - val_loss: 1.9222 - val_acc: 0.2200\n",
      "epochs : 344\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5296 - acc: 0.3714 - val_loss: 1.9509 - val_acc: 0.2400\n",
      "epochs : 345\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5296 - acc: 0.3629 - val_loss: 1.9451 - val_acc: 0.2300\n",
      "epochs : 346\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5285 - acc: 0.3743 - val_loss: 1.9388 - val_acc: 0.2233\n",
      "epochs : 347\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5286 - acc: 0.3786 - val_loss: 1.9385 - val_acc: 0.2333\n",
      "epochs : 348\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5285 - acc: 0.3700 - val_loss: 1.9239 - val_acc: 0.2200\n",
      "epochs : 349\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5268 - acc: 0.3700 - val_loss: 1.9359 - val_acc: 0.2333\n",
      "epochs : 350\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5265 - acc: 0.3714 - val_loss: 1.9556 - val_acc: 0.2233\n",
      "epochs : 351\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5278 - acc: 0.3800 - val_loss: 1.9376 - val_acc: 0.2133\n",
      "epochs : 352\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5267 - acc: 0.3729 - val_loss: 1.9500 - val_acc: 0.2200\n",
      "epochs : 353\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5263 - acc: 0.3586 - val_loss: 1.9460 - val_acc: 0.2133\n",
      "epochs : 354\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5259 - acc: 0.3729 - val_loss: 1.9421 - val_acc: 0.2133\n",
      "epochs : 355\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5249 - acc: 0.3786 - val_loss: 1.9388 - val_acc: 0.2233\n",
      "epochs : 356\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5246 - acc: 0.3643 - val_loss: 1.9385 - val_acc: 0.2233\n",
      "epochs : 357\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5245 - acc: 0.3786 - val_loss: 1.9396 - val_acc: 0.2367\n",
      "epochs : 358\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5251 - acc: 0.3671 - val_loss: 1.9347 - val_acc: 0.2233\n",
      "epochs : 359\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5234 - acc: 0.3743 - val_loss: 1.9402 - val_acc: 0.2300\n",
      "epochs : 360\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5237 - acc: 0.3729 - val_loss: 1.9445 - val_acc: 0.2200\n",
      "epochs : 361\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5225 - acc: 0.3686 - val_loss: 1.9444 - val_acc: 0.2367\n",
      "epochs : 362\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5214 - acc: 0.3714 - val_loss: 1.9444 - val_acc: 0.2600\n",
      "epochs : 363\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5239 - acc: 0.3700 - val_loss: 1.9490 - val_acc: 0.2267\n",
      "epochs : 364\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5219 - acc: 0.3671 - val_loss: 1.9435 - val_acc: 0.2167\n",
      "epochs : 365\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5213 - acc: 0.3786 - val_loss: 1.9475 - val_acc: 0.2200\n",
      "epochs : 366\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5218 - acc: 0.3743 - val_loss: 1.9500 - val_acc: 0.2267\n",
      "epochs : 367\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5213 - acc: 0.3714 - val_loss: 1.9462 - val_acc: 0.2200\n",
      "epochs : 368\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5197 - acc: 0.3714 - val_loss: 1.9440 - val_acc: 0.2200\n",
      "epochs : 369\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5209 - acc: 0.3771 - val_loss: 1.9509 - val_acc: 0.2233\n",
      "epochs : 370\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5200 - acc: 0.3714 - val_loss: 1.9552 - val_acc: 0.2233\n",
      "epochs : 371\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5208 - acc: 0.3771 - val_loss: 1.9576 - val_acc: 0.2167\n",
      "epochs : 372\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5195 - acc: 0.3729 - val_loss: 1.9462 - val_acc: 0.2200\n",
      "epochs : 373\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5199 - acc: 0.3643 - val_loss: 1.9539 - val_acc: 0.2400\n",
      "epochs : 374\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5190 - acc: 0.3743 - val_loss: 1.9565 - val_acc: 0.2167\n",
      "epochs : 375\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5184 - acc: 0.3800 - val_loss: 1.9642 - val_acc: 0.2167\n",
      "epochs : 376\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5185 - acc: 0.3729 - val_loss: 1.9407 - val_acc: 0.2200\n",
      "epochs : 377\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5190 - acc: 0.3743 - val_loss: 1.9479 - val_acc: 0.2200\n",
      "epochs : 378\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5177 - acc: 0.3800 - val_loss: 1.9512 - val_acc: 0.2267\n",
      "epochs : 379\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5182 - acc: 0.3757 - val_loss: 1.9543 - val_acc: 0.2300\n",
      "epochs : 380\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5165 - acc: 0.3771 - val_loss: 1.9718 - val_acc: 0.2367\n",
      "epochs : 381\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5178 - acc: 0.3857 - val_loss: 1.9552 - val_acc: 0.2233\n",
      "epochs : 382\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5160 - acc: 0.3743 - val_loss: 1.9506 - val_acc: 0.2200\n",
      "epochs : 383\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5165 - acc: 0.3814 - val_loss: 1.9475 - val_acc: 0.2200\n",
      "epochs : 384\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5153 - acc: 0.3800 - val_loss: 1.9619 - val_acc: 0.2200\n",
      "epochs : 385\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5148 - acc: 0.3786 - val_loss: 1.9628 - val_acc: 0.2267\n",
      "epochs : 386\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5145 - acc: 0.3729 - val_loss: 1.9603 - val_acc: 0.2333\n",
      "epochs : 387\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5148 - acc: 0.3743 - val_loss: 1.9533 - val_acc: 0.2200\n",
      "epochs : 388\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5135 - acc: 0.3829 - val_loss: 1.9703 - val_acc: 0.2367\n",
      "epochs : 389\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5137 - acc: 0.3614 - val_loss: 1.9490 - val_acc: 0.2267\n",
      "epochs : 390\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5133 - acc: 0.3771 - val_loss: 1.9642 - val_acc: 0.2200\n",
      "epochs : 391\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5134 - acc: 0.3714 - val_loss: 1.9514 - val_acc: 0.2300\n",
      "epochs : 392\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5123 - acc: 0.3671 - val_loss: 1.9586 - val_acc: 0.2200\n",
      "epochs : 393\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5124 - acc: 0.3743 - val_loss: 1.9659 - val_acc: 0.2200\n",
      "epochs : 394\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5117 - acc: 0.3771 - val_loss: 1.9669 - val_acc: 0.2233\n",
      "epochs : 395\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5114 - acc: 0.3743 - val_loss: 1.9577 - val_acc: 0.2233\n",
      "epochs : 396\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5110 - acc: 0.3771 - val_loss: 1.9572 - val_acc: 0.2333\n",
      "epochs : 397\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5126 - acc: 0.3814 - val_loss: 1.9657 - val_acc: 0.2233\n",
      "epochs : 398\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5096 - acc: 0.3614 - val_loss: 1.9613 - val_acc: 0.2367\n",
      "epochs : 399\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5104 - acc: 0.3871 - val_loss: 1.9455 - val_acc: 0.2433\n",
      "epochs : 400\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5104 - acc: 0.3757 - val_loss: 1.9695 - val_acc: 0.2233\n",
      "epochs : 401\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5109 - acc: 0.3857 - val_loss: 1.9781 - val_acc: 0.2200\n",
      "epochs : 402\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5104 - acc: 0.3829 - val_loss: 1.9771 - val_acc: 0.2267\n",
      "epochs : 403\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5105 - acc: 0.3729 - val_loss: 1.9797 - val_acc: 0.2233\n",
      "epochs : 404\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5089 - acc: 0.3829 - val_loss: 1.9656 - val_acc: 0.2200\n",
      "epochs : 405\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5093 - acc: 0.3757 - val_loss: 1.9680 - val_acc: 0.2233\n",
      "epochs : 406\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5085 - acc: 0.3786 - val_loss: 1.9674 - val_acc: 0.2200\n",
      "epochs : 407\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5086 - acc: 0.3729 - val_loss: 1.9632 - val_acc: 0.2333\n",
      "epochs : 408\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5076 - acc: 0.3800 - val_loss: 1.9777 - val_acc: 0.2333\n",
      "epochs : 409\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5076 - acc: 0.3786 - val_loss: 1.9652 - val_acc: 0.2333\n",
      "epochs : 410\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5063 - acc: 0.3843 - val_loss: 1.9645 - val_acc: 0.2400\n",
      "epochs : 411\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5077 - acc: 0.3800 - val_loss: 1.9811 - val_acc: 0.2200\n",
      "epochs : 412\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5066 - acc: 0.3757 - val_loss: 1.9759 - val_acc: 0.2200\n",
      "epochs : 413\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5052 - acc: 0.3757 - val_loss: 1.9744 - val_acc: 0.2400\n",
      "epochs : 414\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5078 - acc: 0.3800 - val_loss: 1.9645 - val_acc: 0.2233\n",
      "epochs : 415\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5065 - acc: 0.3786 - val_loss: 1.9676 - val_acc: 0.2300\n",
      "epochs : 416\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5033 - acc: 0.3829 - val_loss: 1.9856 - val_acc: 0.2367\n",
      "epochs : 417\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5054 - acc: 0.3829 - val_loss: 1.9781 - val_acc: 0.2233\n",
      "epochs : 418\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5057 - acc: 0.3729 - val_loss: 1.9717 - val_acc: 0.2233\n",
      "epochs : 419\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5002 - acc: 0.402 - ETA: 0s - loss: 1.5043 - acc: 0.3843 - val_loss: 1.9986 - val_acc: 0.2433\n",
      "epochs : 420\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5059 - acc: 0.3771 - val_loss: 1.9889 - val_acc: 0.2300\n",
      "epochs : 421\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5045 - acc: 0.3786 - val_loss: 1.9700 - val_acc: 0.2233\n",
      "epochs : 422\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5026 - acc: 0.3743 - val_loss: 1.9652 - val_acc: 0.2500\n",
      "epochs : 423\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5032 - acc: 0.3757 - val_loss: 1.9841 - val_acc: 0.2233\n",
      "epochs : 424\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5021 - acc: 0.3743 - val_loss: 1.9790 - val_acc: 0.2333\n",
      "epochs : 425\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5035 - acc: 0.3886 - val_loss: 2.0016 - val_acc: 0.2367\n",
      "epochs : 426\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5021 - acc: 0.3743 - val_loss: 1.9851 - val_acc: 0.2400\n",
      "epochs : 427\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5018 - acc: 0.3729 - val_loss: 1.9685 - val_acc: 0.2333\n",
      "epochs : 428\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5017 - acc: 0.3743 - val_loss: 1.9752 - val_acc: 0.2333\n",
      "epochs : 429\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5011 - acc: 0.3886 - val_loss: 1.9793 - val_acc: 0.2267\n",
      "epochs : 430\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5013 - acc: 0.3886 - val_loss: 1.9813 - val_acc: 0.2200\n",
      "epochs : 431\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5018 - acc: 0.3814 - val_loss: 1.9887 - val_acc: 0.2200\n",
      "epochs : 432\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4993 - acc: 0.3814 - val_loss: 1.9766 - val_acc: 0.2400\n",
      "epochs : 433\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5011 - acc: 0.3829 - val_loss: 1.9828 - val_acc: 0.2233\n",
      "epochs : 434\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5000 - acc: 0.3729 - val_loss: 1.9847 - val_acc: 0.2200\n",
      "epochs : 435\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5002 - acc: 0.3900 - val_loss: 1.9808 - val_acc: 0.2200\n",
      "epochs : 436\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4998 - acc: 0.3786 - val_loss: 1.9871 - val_acc: 0.2233\n",
      "epochs : 437\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4993 - acc: 0.3871 - val_loss: 1.9911 - val_acc: 0.2233\n",
      "epochs : 438\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4992 - acc: 0.3829 - val_loss: 1.9833 - val_acc: 0.2300\n",
      "epochs : 439\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4989 - acc: 0.3843 - val_loss: 1.9793 - val_acc: 0.2233\n",
      "epochs : 440\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4986 - acc: 0.3757 - val_loss: 1.9785 - val_acc: 0.2300\n",
      "epochs : 441\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4986 - acc: 0.3971 - val_loss: 1.9909 - val_acc: 0.2200\n",
      "epochs : 442\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4976 - acc: 0.3843 - val_loss: 1.9788 - val_acc: 0.2433\n",
      "epochs : 443\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4974 - acc: 0.3771 - val_loss: 1.9930 - val_acc: 0.2333\n",
      "epochs : 444\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4968 - acc: 0.3871 - val_loss: 1.9849 - val_acc: 0.2267\n",
      "epochs : 445\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4973 - acc: 0.3771 - val_loss: 1.9829 - val_acc: 0.2200\n",
      "epochs : 446\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4964 - acc: 0.3829 - val_loss: 1.9986 - val_acc: 0.2467\n",
      "epochs : 447\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4965 - acc: 0.3886 - val_loss: 1.9820 - val_acc: 0.2333\n",
      "epochs : 448\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4951 - acc: 0.3814 - val_loss: 1.9950 - val_acc: 0.2467\n",
      "epochs : 449\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4954 - acc: 0.3800 - val_loss: 2.0024 - val_acc: 0.2233\n",
      "epochs : 450\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4947 - acc: 0.3671 - val_loss: 1.9822 - val_acc: 0.2233\n",
      "epochs : 451\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4937 - acc: 0.3943 - val_loss: 1.9899 - val_acc: 0.2167\n",
      "epochs : 452\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4960 - acc: 0.3857 - val_loss: 2.0175 - val_acc: 0.2333\n",
      "epochs : 453\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.3786 - val_loss: 1.9872 - val_acc: 0.2367\n",
      "epochs : 454\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4955 - acc: 0.3886 - val_loss: 1.9933 - val_acc: 0.2367\n",
      "epochs : 455\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4947 - acc: 0.3786 - val_loss: 1.9838 - val_acc: 0.2333\n",
      "epochs : 456\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4943 - acc: 0.3829 - val_loss: 1.9790 - val_acc: 0.2200\n",
      "epochs : 457\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4950 - acc: 0.3900 - val_loss: 1.9917 - val_acc: 0.2200\n",
      "epochs : 458\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4936 - acc: 0.3886 - val_loss: 1.9901 - val_acc: 0.2267\n",
      "epochs : 459\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4932 - acc: 0.3814 - val_loss: 1.9968 - val_acc: 0.2233\n",
      "epochs : 460\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4928 - acc: 0.3786 - val_loss: 1.9934 - val_acc: 0.2267\n",
      "epochs : 461\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4917 - acc: 0.3957 - val_loss: 1.9952 - val_acc: 0.2233\n",
      "epochs : 462\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4910 - acc: 0.3914 - val_loss: 1.9992 - val_acc: 0.2367\n",
      "epochs : 463\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4922 - acc: 0.3871 - val_loss: 2.0082 - val_acc: 0.2367\n",
      "epochs : 464\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4927 - acc: 0.3886 - val_loss: 2.0092 - val_acc: 0.2300\n",
      "epochs : 465\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4916 - acc: 0.3886 - val_loss: 1.9844 - val_acc: 0.2400\n",
      "epochs : 466\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4933 - acc: 0.3829 - val_loss: 1.9982 - val_acc: 0.2267\n",
      "epochs : 467\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4916 - acc: 0.3757 - val_loss: 2.0016 - val_acc: 0.2233\n",
      "epochs : 468\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4905 - acc: 0.3914 - val_loss: 1.9886 - val_acc: 0.2200\n",
      "epochs : 469\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4917 - acc: 0.3843 - val_loss: 2.0021 - val_acc: 0.2233\n",
      "epochs : 470\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4916 - acc: 0.3814 - val_loss: 2.0065 - val_acc: 0.2233\n",
      "epochs : 471\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4896 - acc: 0.3943 - val_loss: 2.0124 - val_acc: 0.2367\n",
      "epochs : 472\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4905 - acc: 0.3871 - val_loss: 1.9949 - val_acc: 0.2233\n",
      "epochs : 473\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4881 - acc: 0.3786 - val_loss: 2.0059 - val_acc: 0.2233\n",
      "epochs : 474\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4903 - acc: 0.3814 - val_loss: 2.0037 - val_acc: 0.2233\n",
      "epochs : 475\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4870 - acc: 0.3800 - val_loss: 1.9986 - val_acc: 0.2267\n",
      "epochs : 476\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4896 - acc: 0.3843 - val_loss: 2.0152 - val_acc: 0.2333\n",
      "epochs : 477\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4890 - acc: 0.3843 - val_loss: 1.9966 - val_acc: 0.2200\n",
      "epochs : 478\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4878 - acc: 0.3914 - val_loss: 2.0186 - val_acc: 0.2300\n",
      "epochs : 479\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4878 - acc: 0.3857 - val_loss: 1.9976 - val_acc: 0.2233\n",
      "epochs : 480\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4878 - acc: 0.3871 - val_loss: 1.9980 - val_acc: 0.2233\n",
      "epochs : 481\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4878 - acc: 0.3900 - val_loss: 2.0060 - val_acc: 0.2233\n",
      "epochs : 482\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4871 - acc: 0.3886 - val_loss: 2.0023 - val_acc: 0.2367\n",
      "epochs : 483\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4870 - acc: 0.3829 - val_loss: 2.0094 - val_acc: 0.2267\n",
      "epochs : 484\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4862 - acc: 0.3829 - val_loss: 2.0124 - val_acc: 0.2367\n",
      "epochs : 485\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4860 - acc: 0.3871 - val_loss: 2.0041 - val_acc: 0.2233\n",
      "epochs : 486\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4850 - acc: 0.3900 - val_loss: 2.0094 - val_acc: 0.2367\n",
      "epochs : 487\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4872 - acc: 0.3829 - val_loss: 2.0049 - val_acc: 0.2300\n",
      "epochs : 488\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4855 - acc: 0.3914 - val_loss: 2.0046 - val_acc: 0.2333\n",
      "epochs : 489\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4861 - acc: 0.3829 - val_loss: 2.0078 - val_acc: 0.2233\n",
      "epochs : 490\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4850 - acc: 0.3900 - val_loss: 2.0087 - val_acc: 0.2233\n",
      "epochs : 491\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4841 - acc: 0.3857 - val_loss: 2.0008 - val_acc: 0.2367\n",
      "epochs : 492\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4832 - acc: 0.3857 - val_loss: 2.0045 - val_acc: 0.2267\n",
      "epochs : 493\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4853 - acc: 0.3914 - val_loss: 2.0090 - val_acc: 0.2233\n",
      "epochs : 494\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4847 - acc: 0.3857 - val_loss: 2.0169 - val_acc: 0.2233\n",
      "epochs : 495\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4836 - acc: 0.3857 - val_loss: 2.0094 - val_acc: 0.2200\n",
      "epochs : 496\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4843 - acc: 0.4029 - val_loss: 1.9975 - val_acc: 0.2200\n",
      "epochs : 497\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4827 - acc: 0.3857 - val_loss: 1.9850 - val_acc: 0.2233\n",
      "epochs : 498\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4840 - acc: 0.3886 - val_loss: 2.0059 - val_acc: 0.2233\n",
      "epochs : 499\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3871 - val_loss: 2.0270 - val_acc: 0.2233\n",
      "epochs : 500\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3857 - val_loss: 2.0209 - val_acc: 0.2367\n",
      "epochs : 501\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4836 - acc: 0.3829 - val_loss: 1.9969 - val_acc: 0.2200\n",
      "epochs : 502\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4833 - acc: 0.3957 - val_loss: 2.0109 - val_acc: 0.2367\n",
      "epochs : 503\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3943 - val_loss: 2.0284 - val_acc: 0.2267\n",
      "epochs : 504\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4825 - acc: 0.3914 - val_loss: 2.0207 - val_acc: 0.2233\n",
      "epochs : 505\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3971 - val_loss: 2.0185 - val_acc: 0.2267\n",
      "epochs : 506\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4813 - acc: 0.3743 - val_loss: 2.0253 - val_acc: 0.2267\n",
      "epochs : 507\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4815 - acc: 0.3957 - val_loss: 2.0095 - val_acc: 0.2233\n",
      "epochs : 508\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4800 - acc: 0.3957 - val_loss: 2.0152 - val_acc: 0.2267\n",
      "epochs : 509\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4796 - acc: 0.3943 - val_loss: 2.0097 - val_acc: 0.2300\n",
      "epochs : 510\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4808 - acc: 0.4000 - val_loss: 2.0240 - val_acc: 0.2233\n",
      "epochs : 511\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4809 - acc: 0.3857 - val_loss: 2.0056 - val_acc: 0.2267\n",
      "epochs : 512\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4806 - acc: 0.3857 - val_loss: 2.0250 - val_acc: 0.2233\n",
      "epochs : 513\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4781 - acc: 0.3957 - val_loss: 2.0212 - val_acc: 0.2267\n",
      "epochs : 514\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4792 - acc: 0.3929 - val_loss: 2.0057 - val_acc: 0.2433\n",
      "epochs : 515\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4787 - acc: 0.3886 - val_loss: 2.0058 - val_acc: 0.2267\n",
      "epochs : 516\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4788 - acc: 0.3771 - val_loss: 2.0216 - val_acc: 0.2367\n",
      "epochs : 517\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4787 - acc: 0.3957 - val_loss: 2.0142 - val_acc: 0.2233\n",
      "epochs : 518\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4783 - acc: 0.3914 - val_loss: 2.0266 - val_acc: 0.2300\n",
      "epochs : 519\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4769 - acc: 0.3914 - val_loss: 2.0128 - val_acc: 0.2467\n",
      "epochs : 520\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4779 - acc: 0.3957 - val_loss: 2.0258 - val_acc: 0.2367\n",
      "epochs : 521\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4778 - acc: 0.3900 - val_loss: 2.0278 - val_acc: 0.2233\n",
      "epochs : 522\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4778 - acc: 0.3886 - val_loss: 2.0170 - val_acc: 0.2267\n",
      "epochs : 523\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4780 - acc: 0.4057 - val_loss: 2.0117 - val_acc: 0.2233\n",
      "epochs : 524\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4771 - acc: 0.3843 - val_loss: 2.0332 - val_acc: 0.2300\n",
      "epochs : 525\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4767 - acc: 0.3986 - val_loss: 2.0215 - val_acc: 0.2300\n",
      "epochs : 526\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4759 - acc: 0.4043 - val_loss: 2.0359 - val_acc: 0.2267\n",
      "epochs : 527\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4757 - acc: 0.3871 - val_loss: 2.0326 - val_acc: 0.2500\n",
      "epochs : 528\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4763 - acc: 0.3886 - val_loss: 2.0216 - val_acc: 0.2267\n",
      "epochs : 529\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4756 - acc: 0.3900 - val_loss: 2.0221 - val_acc: 0.2267\n",
      "epochs : 530\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4748 - acc: 0.3857 - val_loss: 2.0121 - val_acc: 0.2267\n",
      "epochs : 531\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4750 - acc: 0.3900 - val_loss: 2.0203 - val_acc: 0.2267\n",
      "epochs : 532\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4763 - acc: 0.3900 - val_loss: 2.0159 - val_acc: 0.2267\n",
      "epochs : 533\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4745 - acc: 0.3900 - val_loss: 2.0260 - val_acc: 0.2400\n",
      "epochs : 534\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4743 - acc: 0.3957 - val_loss: 2.0374 - val_acc: 0.2267\n",
      "epochs : 535\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4738 - acc: 0.3900 - val_loss: 2.0246 - val_acc: 0.2267\n",
      "epochs : 536\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4746 - acc: 0.3914 - val_loss: 2.0273 - val_acc: 0.2300\n",
      "epochs : 537\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4737 - acc: 0.3900 - val_loss: 2.0279 - val_acc: 0.2367\n",
      "epochs : 538\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4734 - acc: 0.3871 - val_loss: 2.0150 - val_acc: 0.2300\n",
      "epochs : 539\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4734 - acc: 0.4000 - val_loss: 2.0220 - val_acc: 0.2300\n",
      "epochs : 540\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4727 - acc: 0.4014 - val_loss: 2.0305 - val_acc: 0.2433\n",
      "epochs : 541\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4720 - acc: 0.3971 - val_loss: 2.0327 - val_acc: 0.2233\n",
      "epochs : 542\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4732 - acc: 0.4000 - val_loss: 2.0374 - val_acc: 0.2300\n",
      "epochs : 543\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4718 - acc: 0.3929 - val_loss: 2.0342 - val_acc: 0.2300\n",
      "epochs : 544\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4723 - acc: 0.3943 - val_loss: 2.0245 - val_acc: 0.2267\n",
      "epochs : 545\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4718 - acc: 0.3886 - val_loss: 2.0253 - val_acc: 0.2267\n",
      "epochs : 546\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4710 - acc: 0.3900 - val_loss: 2.0252 - val_acc: 0.2233\n",
      "epochs : 547\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4721 - acc: 0.3971 - val_loss: 2.0204 - val_acc: 0.2267\n",
      "epochs : 548\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4727 - acc: 0.3943 - val_loss: 2.0197 - val_acc: 0.2267\n",
      "epochs : 549\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4716 - acc: 0.3929 - val_loss: 2.0226 - val_acc: 0.2300\n",
      "epochs : 550\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4711 - acc: 0.3929 - val_loss: 2.0332 - val_acc: 0.2267\n",
      "epochs : 551\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4703 - acc: 0.4043 - val_loss: 2.0278 - val_acc: 0.2533\n",
      "epochs : 552\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4703 - acc: 0.4043 - val_loss: 2.0381 - val_acc: 0.2233\n",
      "epochs : 553\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4684 - acc: 0.3957 - val_loss: 2.0407 - val_acc: 0.2300\n",
      "epochs : 554\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4708 - acc: 0.3886 - val_loss: 2.0241 - val_acc: 0.2267\n",
      "epochs : 555\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4701 - acc: 0.4029 - val_loss: 2.0313 - val_acc: 0.2233\n",
      "epochs : 556\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4694 - acc: 0.4029 - val_loss: 2.0297 - val_acc: 0.2433\n",
      "epochs : 557\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4676 - acc: 0.3971 - val_loss: 2.0273 - val_acc: 0.2600\n",
      "epochs : 558\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4703 - acc: 0.3929 - val_loss: 2.0447 - val_acc: 0.2433\n",
      "epochs : 559\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4679 - acc: 0.4029 - val_loss: 2.0322 - val_acc: 0.2467\n",
      "epochs : 560\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4681 - acc: 0.3957 - val_loss: 2.0484 - val_acc: 0.2333\n",
      "epochs : 561\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4681 - acc: 0.3871 - val_loss: 2.0404 - val_acc: 0.2400\n",
      "epochs : 562\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4675 - acc: 0.3957 - val_loss: 2.0282 - val_acc: 0.2300\n",
      "epochs : 563\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4683 - acc: 0.4043 - val_loss: 2.0329 - val_acc: 0.2267\n",
      "epochs : 564\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4667 - acc: 0.3957 - val_loss: 2.0361 - val_acc: 0.2467\n",
      "epochs : 565\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4665 - acc: 0.3943 - val_loss: 2.0342 - val_acc: 0.2267\n",
      "epochs : 566\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4675 - acc: 0.4029 - val_loss: 2.0353 - val_acc: 0.2300\n",
      "epochs : 567\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4672 - acc: 0.4014 - val_loss: 2.0419 - val_acc: 0.2300\n",
      "epochs : 568\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4663 - acc: 0.4029 - val_loss: 2.0484 - val_acc: 0.2433\n",
      "epochs : 569\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4657 - acc: 0.3957 - val_loss: 2.0356 - val_acc: 0.2300\n",
      "epochs : 570\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4668 - acc: 0.3986 - val_loss: 2.0358 - val_acc: 0.2233\n",
      "epochs : 571\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4655 - acc: 0.4043 - val_loss: 2.0402 - val_acc: 0.2233\n",
      "epochs : 572\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4658 - acc: 0.4014 - val_loss: 2.0348 - val_acc: 0.2233\n",
      "epochs : 573\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4644 - acc: 0.4057 - val_loss: 2.0382 - val_acc: 0.2367\n",
      "epochs : 574\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4654 - acc: 0.4057 - val_loss: 2.0372 - val_acc: 0.2267\n",
      "epochs : 575\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4663 - acc: 0.3986 - val_loss: 2.0397 - val_acc: 0.2233\n",
      "epochs : 576\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4643 - acc: 0.3957 - val_loss: 2.0435 - val_acc: 0.2300\n",
      "epochs : 577\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4655 - acc: 0.4043 - val_loss: 2.0323 - val_acc: 0.2267\n",
      "epochs : 578\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4643 - acc: 0.4071 - val_loss: 2.0458 - val_acc: 0.2233\n",
      "epochs : 579\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4639 - acc: 0.3971 - val_loss: 2.0357 - val_acc: 0.2300\n",
      "epochs : 580\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4637 - acc: 0.4043 - val_loss: 2.0436 - val_acc: 0.2333\n",
      "epochs : 581\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4643 - acc: 0.3986 - val_loss: 2.0321 - val_acc: 0.2467\n",
      "epochs : 582\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4637 - acc: 0.4000 - val_loss: 2.0370 - val_acc: 0.2467\n",
      "epochs : 583\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4631 - acc: 0.3929 - val_loss: 2.0377 - val_acc: 0.2467\n",
      "epochs : 584\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4634 - acc: 0.3943 - val_loss: 2.0352 - val_acc: 0.2300\n",
      "epochs : 585\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4635 - acc: 0.3971 - val_loss: 2.0375 - val_acc: 0.2233\n",
      "epochs : 586\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3971 - val_loss: 2.0342 - val_acc: 0.2267\n",
      "epochs : 587\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4612 - acc: 0.3986 - val_loss: 2.0640 - val_acc: 0.2533\n",
      "epochs : 588\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.4029 - val_loss: 2.0397 - val_acc: 0.2467\n",
      "epochs : 589\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4624 - acc: 0.3900 - val_loss: 2.0502 - val_acc: 0.2267\n",
      "epochs : 590\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.3986 - val_loss: 2.0532 - val_acc: 0.2300\n",
      "epochs : 591\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4606 - acc: 0.4043 - val_loss: 2.0417 - val_acc: 0.2267\n",
      "epochs : 592\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4043 - val_loss: 2.0453 - val_acc: 0.2300\n",
      "epochs : 593\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4606 - acc: 0.4014 - val_loss: 2.0583 - val_acc: 0.2233\n",
      "epochs : 594\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4599 - acc: 0.3900 - val_loss: 2.0335 - val_acc: 0.2433\n",
      "epochs : 595\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4597 - acc: 0.3943 - val_loss: 2.0395 - val_acc: 0.2233\n",
      "epochs : 596\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4600 - acc: 0.4014 - val_loss: 2.0403 - val_acc: 0.2200\n",
      "epochs : 597\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4599 - acc: 0.3943 - val_loss: 2.0481 - val_acc: 0.2300\n",
      "epochs : 598\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4595 - acc: 0.3957 - val_loss: 2.0337 - val_acc: 0.2200\n",
      "epochs : 599\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4588 - acc: 0.4000 - val_loss: 2.0478 - val_acc: 0.2467\n",
      "epochs : 600\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4582 - acc: 0.4071 - val_loss: 2.0445 - val_acc: 0.2467\n",
      "epochs : 601\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4585 - acc: 0.4000 - val_loss: 2.0443 - val_acc: 0.2300\n",
      "epochs : 602\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4587 - acc: 0.4029 - val_loss: 2.0546 - val_acc: 0.2367\n",
      "epochs : 603\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4581 - acc: 0.3929 - val_loss: 2.0526 - val_acc: 0.2300\n",
      "epochs : 604\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4579 - acc: 0.4014 - val_loss: 2.0608 - val_acc: 0.2367\n",
      "epochs : 605\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4565 - acc: 0.4114 - val_loss: 2.0574 - val_acc: 0.2267\n",
      "epochs : 606\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4565 - acc: 0.3943 - val_loss: 2.0439 - val_acc: 0.2300\n",
      "epochs : 607\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4554 - acc: 0.3971 - val_loss: 2.0469 - val_acc: 0.2533\n",
      "epochs : 608\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4576 - acc: 0.4057 - val_loss: 2.0548 - val_acc: 0.2367\n",
      "epochs : 609\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4014 - val_loss: 2.0558 - val_acc: 0.2233\n",
      "epochs : 610\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4543 - acc: 0.4114 - val_loss: 2.0442 - val_acc: 0.2433\n",
      "epochs : 611\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4556 - acc: 0.4043 - val_loss: 2.0426 - val_acc: 0.2267\n",
      "epochs : 612\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4541 - acc: 0.4086 - val_loss: 2.0419 - val_acc: 0.2433\n",
      "epochs : 613\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4551 - acc: 0.4057 - val_loss: 2.0447 - val_acc: 0.2233\n",
      "epochs : 614\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4545 - acc: 0.4057 - val_loss: 2.0545 - val_acc: 0.2500\n",
      "epochs : 615\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4558 - acc: 0.3986 - val_loss: 2.0427 - val_acc: 0.2300\n",
      "epochs : 616\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4544 - acc: 0.4000 - val_loss: 2.0497 - val_acc: 0.2233\n",
      "epochs : 617\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4542 - acc: 0.4071 - val_loss: 2.0554 - val_acc: 0.2333\n",
      "epochs : 618\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4536 - acc: 0.4143 - val_loss: 2.0424 - val_acc: 0.2300\n",
      "epochs : 619\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4552 - acc: 0.4014 - val_loss: 2.0403 - val_acc: 0.2300\n",
      "epochs : 620\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4538 - acc: 0.4057 - val_loss: 2.0482 - val_acc: 0.2333\n",
      "epochs : 621\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4504 - acc: 0.4086 - val_loss: 2.0511 - val_acc: 0.2533\n",
      "epochs : 622\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4528 - acc: 0.4114 - val_loss: 2.0446 - val_acc: 0.2567\n",
      "epochs : 623\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4540 - acc: 0.4100 - val_loss: 2.0407 - val_acc: 0.2267\n",
      "epochs : 624\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4532 - acc: 0.4000 - val_loss: 2.0527 - val_acc: 0.2333\n",
      "epochs : 625\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4526 - acc: 0.4043 - val_loss: 2.0529 - val_acc: 0.2333\n",
      "epochs : 626\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4057 - val_loss: 2.0652 - val_acc: 0.2267\n",
      "epochs : 627\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4511 - acc: 0.4043 - val_loss: 2.0640 - val_acc: 0.2267\n",
      "epochs : 628\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4500 - acc: 0.4029 - val_loss: 2.0560 - val_acc: 0.2367\n",
      "epochs : 629\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4521 - acc: 0.3986 - val_loss: 2.0426 - val_acc: 0.2233\n",
      "epochs : 630\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4512 - acc: 0.4100 - val_loss: 2.0434 - val_acc: 0.2233\n",
      "epochs : 631\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4043 - val_loss: 2.0516 - val_acc: 0.2367\n",
      "epochs : 632\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4509 - acc: 0.3971 - val_loss: 2.0527 - val_acc: 0.2400\n",
      "epochs : 633\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4143 - val_loss: 2.0535 - val_acc: 0.2300\n",
      "epochs : 634\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4502 - acc: 0.4086 - val_loss: 2.0471 - val_acc: 0.2267\n",
      "epochs : 635\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4071 - val_loss: 2.0769 - val_acc: 0.2300\n",
      "epochs : 636\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4504 - acc: 0.4100 - val_loss: 2.0565 - val_acc: 0.2433\n",
      "epochs : 637\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4499 - acc: 0.4043 - val_loss: 2.0581 - val_acc: 0.2200\n",
      "epochs : 638\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4483 - acc: 0.4071 - val_loss: 2.0518 - val_acc: 0.2500\n",
      "epochs : 639\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4505 - acc: 0.4129 - val_loss: 2.0582 - val_acc: 0.2233\n",
      "epochs : 640\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4496 - acc: 0.4114 - val_loss: 2.0610 - val_acc: 0.2567\n",
      "epochs : 641\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4029 - val_loss: 2.0569 - val_acc: 0.2233\n",
      "epochs : 642\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4486 - acc: 0.4057 - val_loss: 2.0620 - val_acc: 0.2400\n",
      "epochs : 643\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4493 - acc: 0.4086 - val_loss: 2.0691 - val_acc: 0.2300\n",
      "epochs : 644\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4487 - acc: 0.4057 - val_loss: 2.0528 - val_acc: 0.2267\n",
      "epochs : 645\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4478 - acc: 0.4186 - val_loss: 2.0777 - val_acc: 0.2567\n",
      "epochs : 646\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4489 - acc: 0.4171 - val_loss: 2.0493 - val_acc: 0.2233\n",
      "epochs : 647\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4480 - acc: 0.4071 - val_loss: 2.0478 - val_acc: 0.2267\n",
      "epochs : 648\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4478 - acc: 0.4086 - val_loss: 2.0694 - val_acc: 0.2433\n",
      "epochs : 649\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4474 - acc: 0.4129 - val_loss: 2.0523 - val_acc: 0.2333\n",
      "epochs : 650\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4043 - val_loss: 2.0525 - val_acc: 0.2333\n",
      "epochs : 651\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4480 - acc: 0.4043 - val_loss: 2.0537 - val_acc: 0.2433\n",
      "epochs : 652\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4475 - acc: 0.4086 - val_loss: 2.0669 - val_acc: 0.2400\n",
      "epochs : 653\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4462 - acc: 0.4100 - val_loss: 2.0615 - val_acc: 0.2367\n",
      "epochs : 654\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4463 - acc: 0.4143 - val_loss: 2.0586 - val_acc: 0.2400\n",
      "epochs : 655\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4463 - acc: 0.4086 - val_loss: 2.0543 - val_acc: 0.2233\n",
      "epochs : 656\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4463 - acc: 0.4143 - val_loss: 2.0494 - val_acc: 0.2233\n",
      "epochs : 657\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4460 - acc: 0.4114 - val_loss: 2.0572 - val_acc: 0.2500\n",
      "epochs : 658\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4451 - acc: 0.4143 - val_loss: 2.0651 - val_acc: 0.2533\n",
      "epochs : 659\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4431 - acc: 0.4143 - val_loss: 2.0751 - val_acc: 0.2400\n",
      "epochs : 660\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4447 - acc: 0.4114 - val_loss: 2.0582 - val_acc: 0.2467\n",
      "epochs : 661\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4459 - acc: 0.4129 - val_loss: 2.0563 - val_acc: 0.2433\n",
      "epochs : 662\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4449 - acc: 0.4114 - val_loss: 2.0577 - val_acc: 0.2300\n",
      "epochs : 663\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4446 - acc: 0.4043 - val_loss: 2.0745 - val_acc: 0.2400\n",
      "epochs : 664\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4171 - val_loss: 2.0636 - val_acc: 0.2233\n",
      "epochs : 665\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4441 - acc: 0.4114 - val_loss: 2.0710 - val_acc: 0.2400\n",
      "epochs : 666\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4439 - acc: 0.4100 - val_loss: 2.0673 - val_acc: 0.2300\n",
      "epochs : 667\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4434 - acc: 0.4143 - val_loss: 2.0620 - val_acc: 0.2333\n",
      "epochs : 668\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4437 - acc: 0.4100 - val_loss: 2.0837 - val_acc: 0.2300\n",
      "epochs : 669\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4436 - acc: 0.4171 - val_loss: 2.0763 - val_acc: 0.2333\n",
      "epochs : 670\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4425 - acc: 0.4057 - val_loss: 2.0495 - val_acc: 0.2467\n",
      "epochs : 671\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4423 - acc: 0.4186 - val_loss: 2.0745 - val_acc: 0.2467\n",
      "epochs : 672\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4432 - acc: 0.4200 - val_loss: 2.0775 - val_acc: 0.2500\n",
      "epochs : 673\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4425 - acc: 0.4057 - val_loss: 2.0820 - val_acc: 0.2300\n",
      "epochs : 674\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4423 - acc: 0.4157 - val_loss: 2.0677 - val_acc: 0.2233\n",
      "epochs : 675\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4423 - acc: 0.4143 - val_loss: 2.0694 - val_acc: 0.2233\n",
      "epochs : 676\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4419 - acc: 0.4229 - val_loss: 2.0725 - val_acc: 0.2300\n",
      "epochs : 677\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4412 - acc: 0.4157 - val_loss: 2.0712 - val_acc: 0.2333\n",
      "epochs : 678\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4416 - acc: 0.4114 - val_loss: 2.0849 - val_acc: 0.2400\n",
      "epochs : 679\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4397 - acc: 0.4143 - val_loss: 2.0903 - val_acc: 0.2300\n",
      "epochs : 680\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4415 - acc: 0.4100 - val_loss: 2.0715 - val_acc: 0.2300\n",
      "epochs : 681\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4414 - acc: 0.4157 - val_loss: 2.0690 - val_acc: 0.2267\n",
      "epochs : 682\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4402 - acc: 0.4171 - val_loss: 2.0723 - val_acc: 0.2367\n",
      "epochs : 683\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4402 - acc: 0.4157 - val_loss: 2.0700 - val_acc: 0.2300\n",
      "epochs : 684\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4401 - acc: 0.4114 - val_loss: 2.0684 - val_acc: 0.2367\n",
      "epochs : 685\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4404 - acc: 0.4186 - val_loss: 2.0690 - val_acc: 0.2333\n",
      "epochs : 686\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4400 - acc: 0.4186 - val_loss: 2.0665 - val_acc: 0.2233\n",
      "epochs : 687\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4402 - acc: 0.4100 - val_loss: 2.0748 - val_acc: 0.2300\n",
      "epochs : 688\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4229 - val_loss: 2.0668 - val_acc: 0.2267\n",
      "epochs : 689\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4388 - acc: 0.4214 - val_loss: 2.0673 - val_acc: 0.2500\n",
      "epochs : 690\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4171 - val_loss: 2.0637 - val_acc: 0.2467\n",
      "epochs : 691\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4389 - acc: 0.4157 - val_loss: 2.0720 - val_acc: 0.2333\n",
      "epochs : 692\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4378 - acc: 0.4157 - val_loss: 2.0907 - val_acc: 0.2367\n",
      "epochs : 693\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4381 - acc: 0.4129 - val_loss: 2.0862 - val_acc: 0.2400\n",
      "epochs : 694\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4229 - val_loss: 2.0995 - val_acc: 0.2533\n",
      "epochs : 695\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4381 - acc: 0.4143 - val_loss: 2.0788 - val_acc: 0.2400\n",
      "epochs : 696\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4384 - acc: 0.4171 - val_loss: 2.0802 - val_acc: 0.2333\n",
      "epochs : 697\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4374 - acc: 0.4200 - val_loss: 2.0796 - val_acc: 0.2533\n",
      "epochs : 698\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4373 - acc: 0.4143 - val_loss: 2.0879 - val_acc: 0.2300\n",
      "epochs : 699\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4375 - acc: 0.4171 - val_loss: 2.0775 - val_acc: 0.2400\n",
      "epochs : 700\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4368 - acc: 0.4186 - val_loss: 2.0796 - val_acc: 0.2267\n",
      "epochs : 701\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4367 - acc: 0.4100 - val_loss: 2.0833 - val_acc: 0.2367\n",
      "epochs : 702\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4360 - acc: 0.4186 - val_loss: 2.0748 - val_acc: 0.2267\n",
      "epochs : 703\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4364 - acc: 0.4143 - val_loss: 2.0869 - val_acc: 0.2433\n",
      "epochs : 704\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4129 - val_loss: 2.0910 - val_acc: 0.2333\n",
      "epochs : 705\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4364 - acc: 0.4171 - val_loss: 2.0811 - val_acc: 0.2267\n",
      "epochs : 706\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4341 - acc: 0.4214 - val_loss: 2.0768 - val_acc: 0.2533\n",
      "epochs : 707\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4157 - val_loss: 2.0856 - val_acc: 0.2400\n",
      "epochs : 708\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4364 - acc: 0.4157 - val_loss: 2.0834 - val_acc: 0.2333\n",
      "epochs : 709\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4357 - acc: 0.4229 - val_loss: 2.0846 - val_acc: 0.2367\n",
      "epochs : 710\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4337 - acc: 0.4143 - val_loss: 2.0910 - val_acc: 0.2367\n",
      "epochs : 711\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4350 - acc: 0.4114 - val_loss: 2.0862 - val_acc: 0.2367\n",
      "epochs : 712\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4343 - acc: 0.4200 - val_loss: 2.0874 - val_acc: 0.2267\n",
      "epochs : 713\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4348 - acc: 0.4171 - val_loss: 2.0901 - val_acc: 0.2467\n",
      "epochs : 714\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4342 - acc: 0.4157 - val_loss: 2.0710 - val_acc: 0.2333\n",
      "epochs : 715\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4347 - acc: 0.4229 - val_loss: 2.0883 - val_acc: 0.2400\n",
      "epochs : 716\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4334 - acc: 0.4186 - val_loss: 2.0736 - val_acc: 0.2300\n",
      "epochs : 717\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4334 - acc: 0.4243 - val_loss: 2.0922 - val_acc: 0.2400\n",
      "epochs : 718\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4337 - acc: 0.4171 - val_loss: 2.0930 - val_acc: 0.2400\n",
      "epochs : 719\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4323 - acc: 0.4229 - val_loss: 2.0976 - val_acc: 0.2400\n",
      "epochs : 720\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4318 - acc: 0.4171 - val_loss: 2.0834 - val_acc: 0.2533\n",
      "epochs : 721\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4337 - acc: 0.4157 - val_loss: 2.0977 - val_acc: 0.2400\n",
      "epochs : 722\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4326 - acc: 0.4143 - val_loss: 2.0831 - val_acc: 0.2367\n",
      "epochs : 723\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4186 - val_loss: 2.0924 - val_acc: 0.2633\n",
      "epochs : 724\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4318 - acc: 0.4186 - val_loss: 2.0928 - val_acc: 0.2567\n",
      "epochs : 725\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4337 - acc: 0.4129 - val_loss: 2.0916 - val_acc: 0.2267\n",
      "epochs : 726\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4323 - acc: 0.4229 - val_loss: 2.0755 - val_acc: 0.2267\n",
      "epochs : 727\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4319 - acc: 0.4257 - val_loss: 2.0929 - val_acc: 0.2400\n",
      "epochs : 728\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4171 - val_loss: 2.0889 - val_acc: 0.2400\n",
      "epochs : 729\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4318 - acc: 0.4257 - val_loss: 2.0944 - val_acc: 0.2367\n",
      "epochs : 730\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4316 - acc: 0.4229 - val_loss: 2.0816 - val_acc: 0.2367\n",
      "epochs : 731\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4314 - acc: 0.4200 - val_loss: 2.0906 - val_acc: 0.2300\n",
      "epochs : 732\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4305 - acc: 0.4229 - val_loss: 2.0821 - val_acc: 0.2267\n",
      "epochs : 733\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4298 - acc: 0.4186 - val_loss: 2.0922 - val_acc: 0.2300\n",
      "epochs : 734\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4309 - acc: 0.4186 - val_loss: 2.0886 - val_acc: 0.2500\n",
      "epochs : 735\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4315 - acc: 0.4229 - val_loss: 2.0872 - val_acc: 0.2367\n",
      "epochs : 736\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4307 - acc: 0.4171 - val_loss: 2.0890 - val_acc: 0.2367\n",
      "epochs : 737\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4281 - acc: 0.4171 - val_loss: 2.1035 - val_acc: 0.2567\n",
      "epochs : 738\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4311 - acc: 0.4143 - val_loss: 2.0970 - val_acc: 0.2433\n",
      "epochs : 739\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4291 - acc: 0.4171 - val_loss: 2.0881 - val_acc: 0.2300\n",
      "epochs : 740\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4293 - acc: 0.4243 - val_loss: 2.1075 - val_acc: 0.2333\n",
      "epochs : 741\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4294 - acc: 0.4214 - val_loss: 2.1047 - val_acc: 0.2467\n",
      "epochs : 742\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4290 - acc: 0.4171 - val_loss: 2.1011 - val_acc: 0.2500\n",
      "epochs : 743\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4283 - acc: 0.4200 - val_loss: 2.0901 - val_acc: 0.2600\n",
      "epochs : 744\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4289 - acc: 0.4171 - val_loss: 2.1004 - val_acc: 0.2300\n",
      "epochs : 745\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4289 - acc: 0.4229 - val_loss: 2.0911 - val_acc: 0.2400\n",
      "epochs : 746\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4270 - acc: 0.4271 - val_loss: 2.0896 - val_acc: 0.2367\n",
      "epochs : 747\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4284 - acc: 0.4200 - val_loss: 2.0937 - val_acc: 0.2433\n",
      "epochs : 748\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4277 - acc: 0.4214 - val_loss: 2.1015 - val_acc: 0.2467\n",
      "epochs : 749\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4273 - acc: 0.4186 - val_loss: 2.1024 - val_acc: 0.2367\n",
      "epochs : 750\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4285 - acc: 0.4200 - val_loss: 2.0880 - val_acc: 0.2367\n",
      "epochs : 751\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4286 - acc: 0.4129 - val_loss: 2.0938 - val_acc: 0.2400\n",
      "epochs : 752\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4279 - acc: 0.4200 - val_loss: 2.1121 - val_acc: 0.2467\n",
      "epochs : 753\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4267 - acc: 0.4229 - val_loss: 2.0944 - val_acc: 0.2567\n",
      "epochs : 754\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4273 - acc: 0.4214 - val_loss: 2.0841 - val_acc: 0.2333\n",
      "epochs : 755\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4256 - acc: 0.4229 - val_loss: 2.0972 - val_acc: 0.2367\n",
      "epochs : 756\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4281 - acc: 0.4229 - val_loss: 2.0981 - val_acc: 0.2267\n",
      "epochs : 757\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4262 - acc: 0.4300 - val_loss: 2.0854 - val_acc: 0.2400\n",
      "epochs : 758\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4266 - acc: 0.4200 - val_loss: 2.1025 - val_acc: 0.2433\n",
      "epochs : 759\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4261 - acc: 0.4214 - val_loss: 2.1103 - val_acc: 0.2467\n",
      "epochs : 760\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4266 - acc: 0.4243 - val_loss: 2.0934 - val_acc: 0.2400\n",
      "epochs : 761\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4256 - acc: 0.4257 - val_loss: 2.1050 - val_acc: 0.2633\n",
      "epochs : 762\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4229 - val_loss: 2.0981 - val_acc: 0.2633\n",
      "epochs : 763\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4251 - acc: 0.4171 - val_loss: 2.0974 - val_acc: 0.2367\n",
      "epochs : 764\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4252 - acc: 0.4229 - val_loss: 2.0898 - val_acc: 0.2433\n",
      "epochs : 765\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4246 - acc: 0.4200 - val_loss: 2.1208 - val_acc: 0.2367\n",
      "epochs : 766\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4251 - acc: 0.4314 - val_loss: 2.1085 - val_acc: 0.2433\n",
      "epochs : 767\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4244 - acc: 0.4271 - val_loss: 2.1076 - val_acc: 0.2367\n",
      "epochs : 768\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4249 - acc: 0.4214 - val_loss: 2.0946 - val_acc: 0.2367\n",
      "epochs : 769\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4250 - acc: 0.4229 - val_loss: 2.1134 - val_acc: 0.2433\n",
      "epochs : 770\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4253 - acc: 0.4214 - val_loss: 2.1044 - val_acc: 0.2367\n",
      "epochs : 771\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4241 - acc: 0.4271 - val_loss: 2.1041 - val_acc: 0.2333\n",
      "epochs : 772\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4200 - val_loss: 2.1115 - val_acc: 0.2433\n",
      "epochs : 773\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4244 - acc: 0.4200 - val_loss: 2.1078 - val_acc: 0.2300\n",
      "epochs : 774\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4245 - acc: 0.4314 - val_loss: 2.0951 - val_acc: 0.2467\n",
      "epochs : 775\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4239 - acc: 0.4257 - val_loss: 2.1092 - val_acc: 0.2500\n",
      "epochs : 776\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4240 - acc: 0.4300 - val_loss: 2.0901 - val_acc: 0.2300\n",
      "epochs : 777\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4231 - acc: 0.4229 - val_loss: 2.1051 - val_acc: 0.2333\n",
      "epochs : 778\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4229 - acc: 0.4200 - val_loss: 2.1100 - val_acc: 0.2433\n",
      "epochs : 779\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4233 - acc: 0.4257 - val_loss: 2.1049 - val_acc: 0.2467\n",
      "epochs : 780\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4224 - acc: 0.4200 - val_loss: 2.1059 - val_acc: 0.2467\n",
      "epochs : 781\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4221 - acc: 0.4214 - val_loss: 2.1105 - val_acc: 0.2367\n",
      "epochs : 782\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4225 - acc: 0.4300 - val_loss: 2.0960 - val_acc: 0.2333\n",
      "epochs : 783\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4224 - acc: 0.4214 - val_loss: 2.1085 - val_acc: 0.2333\n",
      "epochs : 784\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4222 - acc: 0.4243 - val_loss: 2.1134 - val_acc: 0.2367\n",
      "epochs : 785\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4216 - acc: 0.4243 - val_loss: 2.1067 - val_acc: 0.2333\n",
      "epochs : 786\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4217 - acc: 0.4271 - val_loss: 2.1123 - val_acc: 0.2400\n",
      "epochs : 787\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4220 - acc: 0.4186 - val_loss: 2.1052 - val_acc: 0.2367\n",
      "epochs : 788\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4212 - acc: 0.4257 - val_loss: 2.1165 - val_acc: 0.2400\n",
      "epochs : 789\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4215 - acc: 0.4257 - val_loss: 2.1251 - val_acc: 0.2667\n",
      "epochs : 790\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4206 - acc: 0.4300 - val_loss: 2.0962 - val_acc: 0.2400\n",
      "epochs : 791\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4210 - acc: 0.4300 - val_loss: 2.1179 - val_acc: 0.2400\n",
      "epochs : 792\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4206 - acc: 0.4300 - val_loss: 2.1001 - val_acc: 0.2333\n",
      "epochs : 793\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4211 - acc: 0.4214 - val_loss: 2.1121 - val_acc: 0.2400\n",
      "epochs : 794\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4202 - acc: 0.4286 - val_loss: 2.1226 - val_acc: 0.2467\n",
      "epochs : 795\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4201 - acc: 0.4257 - val_loss: 2.1094 - val_acc: 0.2400\n",
      "epochs : 796\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4204 - acc: 0.4157 - val_loss: 2.1072 - val_acc: 0.2333\n",
      "epochs : 797\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4201 - acc: 0.4243 - val_loss: 2.1097 - val_acc: 0.2300\n",
      "epochs : 798\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4194 - acc: 0.4286 - val_loss: 2.1139 - val_acc: 0.2433\n",
      "epochs : 799\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4203 - acc: 0.4143 - val_loss: 2.1230 - val_acc: 0.2500\n",
      "epochs : 800\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4300 - val_loss: 2.1043 - val_acc: 0.2367\n",
      "epochs : 801\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4183 - acc: 0.4329 - val_loss: 2.1249 - val_acc: 0.2500\n",
      "epochs : 802\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4189 - acc: 0.4286 - val_loss: 2.1181 - val_acc: 0.2500\n",
      "epochs : 803\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4184 - acc: 0.4243 - val_loss: 2.1334 - val_acc: 0.2400\n",
      "epochs : 804\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4196 - acc: 0.4257 - val_loss: 2.1360 - val_acc: 0.2333\n",
      "epochs : 805\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4191 - acc: 0.4271 - val_loss: 2.1148 - val_acc: 0.2433\n",
      "epochs : 806\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4175 - acc: 0.4300 - val_loss: 2.1008 - val_acc: 0.2467\n",
      "epochs : 807\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4191 - acc: 0.4257 - val_loss: 2.1131 - val_acc: 0.2467\n",
      "epochs : 808\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4186 - acc: 0.4300 - val_loss: 2.1114 - val_acc: 0.2500\n",
      "epochs : 809\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4187 - acc: 0.4214 - val_loss: 2.1189 - val_acc: 0.2367\n",
      "epochs : 810\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4184 - acc: 0.4200 - val_loss: 2.1326 - val_acc: 0.2367\n",
      "epochs : 811\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4182 - acc: 0.4171 - val_loss: 2.1178 - val_acc: 0.2400\n",
      "epochs : 812\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4182 - acc: 0.4271 - val_loss: 2.1183 - val_acc: 0.2367\n",
      "epochs : 813\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4155 - acc: 0.4343 - val_loss: 2.1175 - val_acc: 0.2600\n",
      "epochs : 814\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4170 - acc: 0.4229 - val_loss: 2.1395 - val_acc: 0.2333\n",
      "epochs : 815\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4177 - acc: 0.4243 - val_loss: 2.1203 - val_acc: 0.2400\n",
      "epochs : 816\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4171 - acc: 0.4257 - val_loss: 2.1206 - val_acc: 0.2567\n",
      "epochs : 817\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4174 - acc: 0.4271 - val_loss: 2.1120 - val_acc: 0.2433\n",
      "epochs : 818\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4174 - acc: 0.4257 - val_loss: 2.1130 - val_acc: 0.2333\n",
      "epochs : 819\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4172 - acc: 0.4257 - val_loss: 2.1123 - val_acc: 0.2400\n",
      "epochs : 820\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4168 - acc: 0.4229 - val_loss: 2.1177 - val_acc: 0.2400\n",
      "epochs : 821\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4162 - acc: 0.4329 - val_loss: 2.1328 - val_acc: 0.2400\n",
      "epochs : 822\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4167 - acc: 0.4214 - val_loss: 2.1106 - val_acc: 0.2333\n",
      "epochs : 823\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4150 - acc: 0.4286 - val_loss: 2.1252 - val_acc: 0.2467\n",
      "epochs : 824\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4165 - acc: 0.4257 - val_loss: 2.1226 - val_acc: 0.2367\n",
      "epochs : 825\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4161 - acc: 0.4243 - val_loss: 2.1158 - val_acc: 0.2367\n",
      "epochs : 826\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4154 - acc: 0.4257 - val_loss: 2.1181 - val_acc: 0.2400\n",
      "epochs : 827\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4149 - acc: 0.4314 - val_loss: 2.1252 - val_acc: 0.2367\n",
      "epochs : 828\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4156 - acc: 0.4314 - val_loss: 2.1226 - val_acc: 0.2367\n",
      "epochs : 829\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4229 - val_loss: 2.1189 - val_acc: 0.2333\n",
      "epochs : 830\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4137 - acc: 0.4300 - val_loss: 2.1226 - val_acc: 0.2367\n",
      "epochs : 831\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4142 - acc: 0.4300 - val_loss: 2.1151 - val_acc: 0.2467\n",
      "epochs : 832\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4257 - val_loss: 2.1180 - val_acc: 0.2367\n",
      "epochs : 833\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4271 - val_loss: 2.1429 - val_acc: 0.2400\n",
      "epochs : 834\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4143 - acc: 0.4300 - val_loss: 2.1290 - val_acc: 0.2400\n",
      "epochs : 835\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4129 - acc: 0.4257 - val_loss: 2.1253 - val_acc: 0.2400\n",
      "epochs : 836\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4357 - val_loss: 2.1373 - val_acc: 0.2533\n",
      "epochs : 837\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4143 - acc: 0.4229 - val_loss: 2.1235 - val_acc: 0.2367\n",
      "epochs : 838\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4144 - acc: 0.4357 - val_loss: 2.1340 - val_acc: 0.2467\n",
      "epochs : 839\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4120 - acc: 0.4357 - val_loss: 2.1419 - val_acc: 0.2567\n",
      "epochs : 840\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4271 - val_loss: 2.1249 - val_acc: 0.2467\n",
      "epochs : 841\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4134 - acc: 0.4343 - val_loss: 2.1275 - val_acc: 0.2367\n",
      "epochs : 842\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4119 - acc: 0.4257 - val_loss: 2.1440 - val_acc: 0.2433\n",
      "epochs : 843\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4271 - val_loss: 2.1352 - val_acc: 0.2367\n",
      "epochs : 844\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4286 - val_loss: 2.1588 - val_acc: 0.2400\n",
      "epochs : 845\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4138 - acc: 0.4214 - val_loss: 2.1354 - val_acc: 0.2467\n",
      "epochs : 846\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4129 - acc: 0.4329 - val_loss: 2.1321 - val_acc: 0.2433\n",
      "epochs : 847\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4132 - acc: 0.4300 - val_loss: 2.1334 - val_acc: 0.2400\n",
      "epochs : 848\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4112 - acc: 0.4229 - val_loss: 2.1165 - val_acc: 0.2500\n",
      "epochs : 849\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4257 - val_loss: 2.1290 - val_acc: 0.2500\n",
      "epochs : 850\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4095 - acc: 0.4343 - val_loss: 2.1259 - val_acc: 0.2600\n",
      "epochs : 851\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4130 - acc: 0.4286 - val_loss: 2.1307 - val_acc: 0.2500\n",
      "epochs : 852\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4122 - acc: 0.4243 - val_loss: 2.1265 - val_acc: 0.2533\n",
      "epochs : 853\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4116 - acc: 0.4286 - val_loss: 2.1382 - val_acc: 0.2433\n",
      "epochs : 854\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4117 - acc: 0.4357 - val_loss: 2.1326 - val_acc: 0.2433\n",
      "epochs : 855\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4107 - acc: 0.4343 - val_loss: 2.1318 - val_acc: 0.2400\n",
      "epochs : 856\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4114 - acc: 0.4357 - val_loss: 2.1157 - val_acc: 0.2333\n",
      "epochs : 857\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4112 - acc: 0.4271 - val_loss: 2.1545 - val_acc: 0.2333\n",
      "epochs : 858\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4120 - acc: 0.4257 - val_loss: 2.1286 - val_acc: 0.2367\n",
      "epochs : 859\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4106 - acc: 0.4243 - val_loss: 2.1416 - val_acc: 0.2400\n",
      "epochs : 860\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4114 - acc: 0.4343 - val_loss: 2.1280 - val_acc: 0.2367\n",
      "epochs : 861\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4109 - acc: 0.4271 - val_loss: 2.1303 - val_acc: 0.2433\n",
      "epochs : 862\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4097 - acc: 0.4300 - val_loss: 2.1332 - val_acc: 0.2500\n",
      "epochs : 863\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4087 - acc: 0.4286 - val_loss: 2.1453 - val_acc: 0.2333\n",
      "epochs : 864\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4300 - val_loss: 2.1376 - val_acc: 0.2500\n",
      "epochs : 865\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4098 - acc: 0.4329 - val_loss: 2.1333 - val_acc: 0.2533\n",
      "epochs : 866\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4314 - val_loss: 2.1343 - val_acc: 0.2600\n",
      "epochs : 867\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4097 - acc: 0.4214 - val_loss: 2.1376 - val_acc: 0.2500\n",
      "epochs : 868\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4094 - acc: 0.4271 - val_loss: 2.1339 - val_acc: 0.2533\n",
      "epochs : 869\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4091 - acc: 0.4243 - val_loss: 2.1266 - val_acc: 0.2367\n",
      "epochs : 870\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4092 - acc: 0.4343 - val_loss: 2.1484 - val_acc: 0.2467\n",
      "epochs : 871\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4079 - acc: 0.4314 - val_loss: 2.1274 - val_acc: 0.2367\n",
      "epochs : 872\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4329 - val_loss: 2.1573 - val_acc: 0.2467\n",
      "epochs : 873\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4087 - acc: 0.4286 - val_loss: 2.1465 - val_acc: 0.2500\n",
      "epochs : 874\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4080 - acc: 0.4343 - val_loss: 2.1380 - val_acc: 0.2467\n",
      "epochs : 875\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4092 - acc: 0.4243 - val_loss: 2.1168 - val_acc: 0.2400\n",
      "epochs : 876\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4271 - val_loss: 2.1351 - val_acc: 0.2400\n",
      "epochs : 877\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4243 - val_loss: 2.1354 - val_acc: 0.2433\n",
      "epochs : 878\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4343 - val_loss: 2.1341 - val_acc: 0.2367\n",
      "epochs : 879\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4065 - acc: 0.4371 - val_loss: 2.1540 - val_acc: 0.2400\n",
      "epochs : 880\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4077 - acc: 0.4271 - val_loss: 2.1416 - val_acc: 0.2433\n",
      "epochs : 881\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4075 - acc: 0.4329 - val_loss: 2.1396 - val_acc: 0.2600\n",
      "epochs : 882\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4067 - acc: 0.4343 - val_loss: 2.1469 - val_acc: 0.2367\n",
      "epochs : 883\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4079 - acc: 0.4357 - val_loss: 2.1678 - val_acc: 0.2367\n",
      "epochs : 884\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4077 - acc: 0.4300 - val_loss: 2.1439 - val_acc: 0.2500\n",
      "epochs : 885\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4069 - acc: 0.4300 - val_loss: 2.1352 - val_acc: 0.2367\n",
      "epochs : 886\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4067 - acc: 0.4329 - val_loss: 2.1343 - val_acc: 0.2367\n",
      "epochs : 887\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4055 - acc: 0.4329 - val_loss: 2.1498 - val_acc: 0.2400\n",
      "epochs : 888\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4058 - acc: 0.4371 - val_loss: 2.1363 - val_acc: 0.2567\n",
      "epochs : 889\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4063 - acc: 0.4343 - val_loss: 2.1372 - val_acc: 0.2367\n",
      "epochs : 890\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4064 - acc: 0.4314 - val_loss: 2.1460 - val_acc: 0.2467\n",
      "epochs : 891\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4054 - acc: 0.4343 - val_loss: 2.1505 - val_acc: 0.2467\n",
      "epochs : 892\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4057 - acc: 0.4329 - val_loss: 2.1484 - val_acc: 0.2467\n",
      "epochs : 893\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4061 - acc: 0.4286 - val_loss: 2.1422 - val_acc: 0.2433\n",
      "epochs : 894\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4062 - acc: 0.4343 - val_loss: 2.1530 - val_acc: 0.2433\n",
      "epochs : 895\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4054 - acc: 0.4314 - val_loss: 2.1302 - val_acc: 0.2367\n",
      "epochs : 896\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4357 - val_loss: 2.1400 - val_acc: 0.2400\n",
      "epochs : 897\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4314 - val_loss: 2.1516 - val_acc: 0.2467\n",
      "epochs : 898\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4051 - acc: 0.4329 - val_loss: 2.1479 - val_acc: 0.2600\n",
      "epochs : 899\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4033 - acc: 0.4314 - val_loss: 2.1459 - val_acc: 0.2433\n",
      "epochs : 900\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4314 - val_loss: 2.1616 - val_acc: 0.2467\n",
      "epochs : 901\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4386 - val_loss: 2.1395 - val_acc: 0.2433\n",
      "epochs : 902\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4329 - val_loss: 2.1377 - val_acc: 0.2500\n",
      "epochs : 903\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4343 - val_loss: 2.1541 - val_acc: 0.2467\n",
      "epochs : 904\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4040 - acc: 0.4271 - val_loss: 2.1559 - val_acc: 0.2500\n",
      "epochs : 905\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4257 - val_loss: 2.1375 - val_acc: 0.2400\n",
      "epochs : 906\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4035 - acc: 0.4314 - val_loss: 2.1463 - val_acc: 0.2500\n",
      "epochs : 907\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4030 - acc: 0.4371 - val_loss: 2.1539 - val_acc: 0.2367\n",
      "epochs : 908\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4051 - acc: 0.4371 - val_loss: 2.1407 - val_acc: 0.2367\n",
      "epochs : 909\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4034 - acc: 0.4386 - val_loss: 2.1514 - val_acc: 0.2500\n",
      "epochs : 910\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4017 - acc: 0.4386 - val_loss: 2.1724 - val_acc: 0.2633\n",
      "epochs : 911\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4034 - acc: 0.4357 - val_loss: 2.1590 - val_acc: 0.2467\n",
      "epochs : 912\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4038 - acc: 0.4271 - val_loss: 2.1434 - val_acc: 0.2333\n",
      "epochs : 913\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4031 - acc: 0.4286 - val_loss: 2.1610 - val_acc: 0.2467\n",
      "epochs : 914\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4022 - acc: 0.4386 - val_loss: 2.1711 - val_acc: 0.2600\n",
      "epochs : 915\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4016 - acc: 0.4300 - val_loss: 2.1580 - val_acc: 0.2433\n",
      "epochs : 916\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4025 - acc: 0.4314 - val_loss: 2.1737 - val_acc: 0.2500\n",
      "epochs : 917\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4023 - acc: 0.4371 - val_loss: 2.1845 - val_acc: 0.2600\n",
      "epochs : 918\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4035 - acc: 0.4271 - val_loss: 2.1581 - val_acc: 0.2467\n",
      "epochs : 919\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4016 - acc: 0.4329 - val_loss: 2.1690 - val_acc: 0.2333\n",
      "epochs : 920\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4357 - val_loss: 2.1570 - val_acc: 0.2500\n",
      "epochs : 921\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4016 - acc: 0.4357 - val_loss: 2.1589 - val_acc: 0.2600\n",
      "epochs : 922\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4016 - acc: 0.4314 - val_loss: 2.1670 - val_acc: 0.2400\n",
      "epochs : 923\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4025 - acc: 0.4386 - val_loss: 2.1541 - val_acc: 0.2367\n",
      "epochs : 924\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4018 - acc: 0.4300 - val_loss: 2.1581 - val_acc: 0.2467\n",
      "epochs : 925\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4329 - val_loss: 2.1667 - val_acc: 0.2333\n",
      "epochs : 926\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4024 - acc: 0.4400 - val_loss: 2.1544 - val_acc: 0.2500\n",
      "epochs : 927\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4314 - val_loss: 2.1576 - val_acc: 0.2500\n",
      "epochs : 928\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4019 - acc: 0.4343 - val_loss: 2.1759 - val_acc: 0.2367\n",
      "epochs : 929\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4002 - acc: 0.4457 - val_loss: 2.1618 - val_acc: 0.2433\n",
      "epochs : 930\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4001 - acc: 0.4429 - val_loss: 2.1566 - val_acc: 0.2433\n",
      "epochs : 931\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4007 - acc: 0.4314 - val_loss: 2.1638 - val_acc: 0.2500\n",
      "epochs : 932\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4009 - acc: 0.4314 - val_loss: 2.1458 - val_acc: 0.2467\n",
      "epochs : 933\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3973 - acc: 0.4286 - val_loss: 2.1545 - val_acc: 0.2400\n",
      "epochs : 934\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4314 - val_loss: 2.1571 - val_acc: 0.2467\n",
      "epochs : 935\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3995 - acc: 0.4400 - val_loss: 2.1611 - val_acc: 0.2400\n",
      "epochs : 936\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3998 - acc: 0.4443 - val_loss: 2.1463 - val_acc: 0.2467\n",
      "epochs : 937\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3997 - acc: 0.4343 - val_loss: 2.1601 - val_acc: 0.2433\n",
      "epochs : 938\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4000 - acc: 0.4300 - val_loss: 2.1571 - val_acc: 0.2467\n",
      "epochs : 939\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4002 - acc: 0.4386 - val_loss: 2.1608 - val_acc: 0.2433\n",
      "epochs : 940\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3992 - acc: 0.4329 - val_loss: 2.1631 - val_acc: 0.2533\n",
      "epochs : 941\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3990 - acc: 0.4371 - val_loss: 2.1639 - val_acc: 0.2433\n",
      "epochs : 942\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3990 - acc: 0.4400 - val_loss: 2.1637 - val_acc: 0.2467\n",
      "epochs : 943\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3986 - acc: 0.4329 - val_loss: 2.1750 - val_acc: 0.2433\n",
      "epochs : 944\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3987 - acc: 0.4371 - val_loss: 2.1729 - val_acc: 0.2433\n",
      "epochs : 945\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3993 - acc: 0.4371 - val_loss: 2.1776 - val_acc: 0.2300\n",
      "epochs : 946\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3980 - acc: 0.4443 - val_loss: 2.1576 - val_acc: 0.2533\n",
      "epochs : 947\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4386 - val_loss: 2.1646 - val_acc: 0.2333\n",
      "epochs : 948\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3983 - acc: 0.4357 - val_loss: 2.1858 - val_acc: 0.2500\n",
      "epochs : 949\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3987 - acc: 0.4400 - val_loss: 2.1719 - val_acc: 0.2367\n",
      "epochs : 950\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4357 - val_loss: 2.1681 - val_acc: 0.2467\n",
      "epochs : 951\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3987 - acc: 0.4357 - val_loss: 2.1678 - val_acc: 0.2500\n",
      "epochs : 952\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4329 - val_loss: 2.1959 - val_acc: 0.2500\n",
      "epochs : 953\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4400 - val_loss: 2.1801 - val_acc: 0.2433\n",
      "epochs : 954\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3971 - acc: 0.4314 - val_loss: 2.1785 - val_acc: 0.2367\n",
      "epochs : 955\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3978 - acc: 0.4400 - val_loss: 2.1782 - val_acc: 0.2433\n",
      "epochs : 956\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3970 - acc: 0.4400 - val_loss: 2.1712 - val_acc: 0.2467\n",
      "epochs : 957\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3969 - acc: 0.4371 - val_loss: 2.1890 - val_acc: 0.2400\n",
      "epochs : 958\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3968 - acc: 0.4371 - val_loss: 2.1730 - val_acc: 0.2467\n",
      "epochs : 959\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3966 - acc: 0.4457 - val_loss: 2.1568 - val_acc: 0.2500\n",
      "epochs : 960\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3964 - acc: 0.4329 - val_loss: 2.1730 - val_acc: 0.2367\n",
      "epochs : 961\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4414 - val_loss: 2.1735 - val_acc: 0.2333\n",
      "epochs : 962\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3963 - acc: 0.4400 - val_loss: 2.1778 - val_acc: 0.2433\n",
      "epochs : 963\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3973 - acc: 0.4371 - val_loss: 2.1733 - val_acc: 0.2333\n",
      "epochs : 964\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3956 - acc: 0.4429 - val_loss: 2.1875 - val_acc: 0.2400\n",
      "epochs : 965\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3957 - acc: 0.4343 - val_loss: 2.1778 - val_acc: 0.2333\n",
      "epochs : 966\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3961 - acc: 0.4429 - val_loss: 2.1802 - val_acc: 0.2500\n",
      "epochs : 967\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3966 - acc: 0.4400 - val_loss: 2.1716 - val_acc: 0.2400\n",
      "epochs : 968\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4357 - val_loss: 2.1806 - val_acc: 0.2500\n",
      "epochs : 969\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3956 - acc: 0.4400 - val_loss: 2.1750 - val_acc: 0.2467\n",
      "epochs : 970\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3944 - acc: 0.4271 - val_loss: 2.1672 - val_acc: 0.2533\n",
      "epochs : 971\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3949 - acc: 0.4471 - val_loss: 2.1827 - val_acc: 0.2333\n",
      "epochs : 972\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3951 - acc: 0.4400 - val_loss: 2.1666 - val_acc: 0.2467\n",
      "epochs : 973\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3936 - acc: 0.4414 - val_loss: 2.1786 - val_acc: 0.2567\n",
      "epochs : 974\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3958 - acc: 0.4400 - val_loss: 2.1879 - val_acc: 0.2600\n",
      "epochs : 975\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3954 - acc: 0.4357 - val_loss: 2.1598 - val_acc: 0.2567\n",
      "epochs : 976\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3950 - acc: 0.4443 - val_loss: 2.1724 - val_acc: 0.2500\n",
      "epochs : 977\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3948 - acc: 0.4343 - val_loss: 2.1835 - val_acc: 0.2400\n",
      "epochs : 978\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3945 - acc: 0.4400 - val_loss: 2.1725 - val_acc: 0.2367\n",
      "epochs : 979\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4357 - val_loss: 2.1656 - val_acc: 0.2500\n",
      "epochs : 980\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3947 - acc: 0.4400 - val_loss: 2.1731 - val_acc: 0.2567\n",
      "epochs : 981\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4386 - val_loss: 2.1765 - val_acc: 0.2467\n",
      "epochs : 982\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3940 - acc: 0.4386 - val_loss: 2.1846 - val_acc: 0.2467\n",
      "epochs : 983\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3937 - acc: 0.4443 - val_loss: 2.1825 - val_acc: 0.2367\n",
      "epochs : 984\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4400 - val_loss: 2.1754 - val_acc: 0.2567\n",
      "epochs : 985\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3919 - acc: 0.4386 - val_loss: 2.1828 - val_acc: 0.2400\n",
      "epochs : 986\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3939 - acc: 0.4386 - val_loss: 2.1862 - val_acc: 0.2367\n",
      "epochs : 987\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4429 - val_loss: 2.1822 - val_acc: 0.2500\n",
      "epochs : 988\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3929 - acc: 0.4343 - val_loss: 2.1825 - val_acc: 0.2467\n",
      "epochs : 989\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3931 - acc: 0.4429 - val_loss: 2.2005 - val_acc: 0.2600\n",
      "epochs : 990\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3927 - acc: 0.4371 - val_loss: 2.1947 - val_acc: 0.2567\n",
      "epochs : 991\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4386 - val_loss: 2.1751 - val_acc: 0.2433\n",
      "epochs : 992\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3927 - acc: 0.4500 - val_loss: 2.1798 - val_acc: 0.2567\n",
      "epochs : 993\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3935 - acc: 0.4343 - val_loss: 2.1804 - val_acc: 0.2400\n",
      "epochs : 994\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4400 - val_loss: 2.2021 - val_acc: 0.2500\n",
      "epochs : 995\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3932 - acc: 0.4429 - val_loss: 2.1718 - val_acc: 0.2433\n",
      "epochs : 996\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4371 - val_loss: 2.1870 - val_acc: 0.2400\n",
      "epochs : 997\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4443 - val_loss: 2.1861 - val_acc: 0.2467\n",
      "epochs : 998\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3916 - acc: 0.4329 - val_loss: 2.1822 - val_acc: 0.2567\n",
      "epochs : 999\n",
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/1\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3908 - acc: 0.4457 - val_loss: 2.1778 - val_acc: 0.2367\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272cf2d6438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# 사용자 정의 히스토리 클래스 정의\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []        \n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "    \n",
    "# 모델 학습시키기\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()\n",
    "\n",
    "for epoch_idx in range(1000):\n",
    "    print ('epochs : ' + str(epoch_idx) )\n",
    "    model.fit(X_train, Y_train, epochs=1, batch_size=10, validation_data=(X_val, Y_val), callbacks=[custom_hist])\n",
    "\n",
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(custom_hist.train_loss, 'y', label='train loss')\n",
    "loss_ax.plot(custom_hist.val_loss, 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(custom_hist.train_acc, 'b', label='train acc')\n",
    "acc_ax.plot(custom_hist.val_acc, 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2072 - acc: 0.1657 - val_loss: 2.1908 - val_acc: 0.1800\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1730 - acc: 0.1729 - val_loss: 2.1631 - val_acc: 0.1867\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1441 - acc: 0.1786 - val_loss: 2.1372 - val_acc: 0.1867\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1177 - acc: 0.1900 - val_loss: 2.1141 - val_acc: 0.1867\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0940 - acc: 0.2029 - val_loss: 2.0931 - val_acc: 0.2033\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.2086 - val_loss: 2.0727 - val_acc: 0.2067\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0521 - acc: 0.2129 - val_loss: 2.0563 - val_acc: 0.2067\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0341 - acc: 0.2157 - val_loss: 2.0409 - val_acc: 0.2033\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0188 - acc: 0.2129 - val_loss: 2.0271 - val_acc: 0.2067\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0042 - acc: 0.2200 - val_loss: 2.0125 - val_acc: 0.2100\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9912 - acc: 0.2186 - val_loss: 2.0036 - val_acc: 0.2100\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9788 - acc: 0.2271 - val_loss: 1.9953 - val_acc: 0.2100\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9684 - acc: 0.2314 - val_loss: 1.9833 - val_acc: 0.2033\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9582 - acc: 0.2214 - val_loss: 1.9753 - val_acc: 0.2067\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9484 - acc: 0.2357 - val_loss: 1.9685 - val_acc: 0.2000\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9393 - acc: 0.2343 - val_loss: 1.9612 - val_acc: 0.2033\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9309 - acc: 0.2314 - val_loss: 1.9537 - val_acc: 0.2100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.2271 - val_loss: 1.9452 - val_acc: 0.2100\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9156 - acc: 0.2386 - val_loss: 1.9392 - val_acc: 0.2100\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9085 - acc: 0.2329 - val_loss: 1.9359 - val_acc: 0.2067\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9011 - acc: 0.2386 - val_loss: 1.9288 - val_acc: 0.2033\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8953 - acc: 0.2357 - val_loss: 1.9233 - val_acc: 0.2100\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8895 - acc: 0.2314 - val_loss: 1.9200 - val_acc: 0.2067\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8831 - acc: 0.2371 - val_loss: 1.9166 - val_acc: 0.2167\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8771 - acc: 0.2329 - val_loss: 1.9111 - val_acc: 0.2167\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8715 - acc: 0.2400 - val_loss: 1.9100 - val_acc: 0.2167\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8663 - acc: 0.2386 - val_loss: 1.9094 - val_acc: 0.2000\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2414 - val_loss: 1.9055 - val_acc: 0.1900\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8567 - acc: 0.2271 - val_loss: 1.8978 - val_acc: 0.2167\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8514 - acc: 0.2471 - val_loss: 1.8972 - val_acc: 0.1900\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8464 - acc: 0.2371 - val_loss: 1.8927 - val_acc: 0.1867\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8425 - acc: 0.2243 - val_loss: 1.8872 - val_acc: 0.2100\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8384 - acc: 0.2300 - val_loss: 1.8811 - val_acc: 0.1967\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8336 - acc: 0.2471 - val_loss: 1.8836 - val_acc: 0.1933\n",
      "Epoch 36/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8300 - acc: 0.2386 - val_loss: 1.8756 - val_acc: 0.1900\n",
      "Epoch 37/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8258 - acc: 0.2514 - val_loss: 1.8744 - val_acc: 0.1800\n",
      "Epoch 38/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8221 - acc: 0.2371 - val_loss: 1.8699 - val_acc: 0.1933\n",
      "Epoch 39/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8185 - acc: 0.2443 - val_loss: 1.8705 - val_acc: 0.1767\n",
      "Epoch 40/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8145 - acc: 0.2357 - val_loss: 1.8680 - val_acc: 0.1967\n",
      "Epoch 41/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8105 - acc: 0.2400 - val_loss: 1.8671 - val_acc: 0.1833\n",
      "Epoch 42/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8076 - acc: 0.2471 - val_loss: 1.8636 - val_acc: 0.1800\n",
      "Epoch 43/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8044 - acc: 0.2443 - val_loss: 1.8616 - val_acc: 0.1733\n",
      "Epoch 44/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8002 - acc: 0.2371 - val_loss: 1.8577 - val_acc: 0.1967\n",
      "Epoch 45/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7970 - acc: 0.2486 - val_loss: 1.8564 - val_acc: 0.1767\n",
      "Epoch 46/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7939 - acc: 0.2243 - val_loss: 1.8527 - val_acc: 0.1933\n",
      "Epoch 47/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7913 - acc: 0.2586 - val_loss: 1.8552 - val_acc: 0.1800\n",
      "Epoch 48/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7886 - acc: 0.2486 - val_loss: 1.8545 - val_acc: 0.1867\n",
      "Epoch 49/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7858 - acc: 0.2471 - val_loss: 1.8503 - val_acc: 0.1833\n",
      "Epoch 50/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7819 - acc: 0.2457 - val_loss: 1.8445 - val_acc: 0.2333\n",
      "Epoch 51/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7794 - acc: 0.2614 - val_loss: 1.8469 - val_acc: 0.1900\n",
      "Epoch 52/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7761 - acc: 0.2500 - val_loss: 1.8412 - val_acc: 0.1967\n",
      "Epoch 53/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7743 - acc: 0.2543 - val_loss: 1.8489 - val_acc: 0.2067\n",
      "Epoch 54/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7714 - acc: 0.2671 - val_loss: 1.8482 - val_acc: 0.1867\n",
      "Epoch 55/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7685 - acc: 0.2514 - val_loss: 1.8356 - val_acc: 0.2033\n",
      "Epoch 56/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7671 - acc: 0.2543 - val_loss: 1.8437 - val_acc: 0.2200\n",
      "Epoch 57/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7639 - acc: 0.2771 - val_loss: 1.8387 - val_acc: 0.2133\n",
      "Epoch 58/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7613 - acc: 0.2586 - val_loss: 1.8348 - val_acc: 0.2233\n",
      "Epoch 59/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7587 - acc: 0.2657 - val_loss: 1.8330 - val_acc: 0.2200\n",
      "Epoch 60/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7550 - acc: 0.2614 - val_loss: 1.8260 - val_acc: 0.2467\n",
      "Epoch 61/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7563 - acc: 0.2843 - val_loss: 1.8338 - val_acc: 0.2367\n",
      "Epoch 62/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7528 - acc: 0.2714 - val_loss: 1.8318 - val_acc: 0.2233\n",
      "Epoch 63/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7501 - acc: 0.2814 - val_loss: 1.8304 - val_acc: 0.2000\n",
      "Epoch 64/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7480 - acc: 0.2800 - val_loss: 1.8279 - val_acc: 0.2167\n",
      "Epoch 65/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7454 - acc: 0.2843 - val_loss: 1.8306 - val_acc: 0.2000\n",
      "Epoch 66/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7436 - acc: 0.2800 - val_loss: 1.8305 - val_acc: 0.2067\n",
      "Epoch 67/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7417 - acc: 0.2671 - val_loss: 1.8300 - val_acc: 0.2033\n",
      "Epoch 68/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7401 - acc: 0.2757 - val_loss: 1.8243 - val_acc: 0.2000\n",
      "Epoch 69/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7373 - acc: 0.2843 - val_loss: 1.8291 - val_acc: 0.2167\n",
      "Epoch 70/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7353 - acc: 0.2843 - val_loss: 1.8273 - val_acc: 0.2433\n",
      "Epoch 71/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7342 - acc: 0.2814 - val_loss: 1.8220 - val_acc: 0.2167\n",
      "Epoch 72/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7324 - acc: 0.2843 - val_loss: 1.8233 - val_acc: 0.2133\n",
      "Epoch 73/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7294 - acc: 0.2829 - val_loss: 1.8264 - val_acc: 0.2467\n",
      "Epoch 74/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7280 - acc: 0.2843 - val_loss: 1.8263 - val_acc: 0.1967\n",
      "Epoch 75/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7265 - acc: 0.2771 - val_loss: 1.8201 - val_acc: 0.2267\n",
      "Epoch 76/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7252 - acc: 0.2857 - val_loss: 1.8203 - val_acc: 0.2200\n",
      "Epoch 77/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7225 - acc: 0.3100 - val_loss: 1.8236 - val_acc: 0.2033\n",
      "Epoch 78/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7208 - acc: 0.2857 - val_loss: 1.8177 - val_acc: 0.1933\n",
      "Epoch 79/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7195 - acc: 0.2814 - val_loss: 1.8201 - val_acc: 0.2433\n",
      "Epoch 80/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7188 - acc: 0.2914 - val_loss: 1.8203 - val_acc: 0.2067\n",
      "Epoch 81/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7162 - acc: 0.2800 - val_loss: 1.8260 - val_acc: 0.2067\n",
      "Epoch 82/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7138 - acc: 0.2829 - val_loss: 1.8158 - val_acc: 0.2667\n",
      "Epoch 83/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7130 - acc: 0.2857 - val_loss: 1.8207 - val_acc: 0.2400\n",
      "Epoch 84/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7124 - acc: 0.2971 - val_loss: 1.8229 - val_acc: 0.2133\n",
      "Epoch 85/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7093 - acc: 0.3000 - val_loss: 1.8163 - val_acc: 0.2267\n",
      "Epoch 86/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7079 - acc: 0.2786 - val_loss: 1.8161 - val_acc: 0.2633\n",
      "Epoch 87/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7048 - acc: 0.3071 - val_loss: 1.8202 - val_acc: 0.2733\n",
      "Epoch 88/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7058 - acc: 0.3043 - val_loss: 1.8130 - val_acc: 0.2433\n",
      "Epoch 89/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7040 - acc: 0.3014 - val_loss: 1.8179 - val_acc: 0.2167\n",
      "Epoch 90/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7024 - acc: 0.2829 - val_loss: 1.8167 - val_acc: 0.2233\n",
      "Epoch 91/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7015 - acc: 0.3057 - val_loss: 1.8202 - val_acc: 0.2167\n",
      "Epoch 92/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6985 - acc: 0.3129 - val_loss: 1.8233 - val_acc: 0.2067\n",
      "Epoch 93/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6978 - acc: 0.3029 - val_loss: 1.8177 - val_acc: 0.2733\n",
      "Epoch 94/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6963 - acc: 0.3186 - val_loss: 1.8198 - val_acc: 0.2100\n",
      "Epoch 95/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6937 - acc: 0.3029 - val_loss: 1.8247 - val_acc: 0.2833\n",
      "Epoch 96/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6941 - acc: 0.3057 - val_loss: 1.8117 - val_acc: 0.2200\n",
      "Epoch 97/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6923 - acc: 0.3057 - val_loss: 1.8263 - val_acc: 0.2200\n",
      "Epoch 98/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6915 - acc: 0.3057 - val_loss: 1.8135 - val_acc: 0.2167\n",
      "Epoch 99/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6894 - acc: 0.3129 - val_loss: 1.8269 - val_acc: 0.2200\n",
      "Epoch 100/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6882 - acc: 0.3029 - val_loss: 1.8226 - val_acc: 0.2300\n",
      "Epoch 101/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6869 - acc: 0.3157 - val_loss: 1.8238 - val_acc: 0.2233\n",
      "Epoch 102/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6869 - acc: 0.3057 - val_loss: 1.8228 - val_acc: 0.2300\n",
      "Epoch 103/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6834 - acc: 0.3043 - val_loss: 1.8115 - val_acc: 0.2500\n",
      "Epoch 104/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6834 - acc: 0.3171 - val_loss: 1.8143 - val_acc: 0.2167\n",
      "Epoch 105/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6813 - acc: 0.3100 - val_loss: 1.8087 - val_acc: 0.2100\n",
      "Epoch 106/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6804 - acc: 0.3200 - val_loss: 1.8205 - val_acc: 0.2200\n",
      "Epoch 107/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6795 - acc: 0.3143 - val_loss: 1.8244 - val_acc: 0.2367\n",
      "Epoch 108/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6775 - acc: 0.3143 - val_loss: 1.8198 - val_acc: 0.2767\n",
      "Epoch 109/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6771 - acc: 0.3129 - val_loss: 1.8200 - val_acc: 0.2333\n",
      "Epoch 110/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6765 - acc: 0.3214 - val_loss: 1.8168 - val_acc: 0.2200\n",
      "Epoch 111/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6765 - acc: 0.3057 - val_loss: 1.8211 - val_acc: 0.2267\n",
      "Epoch 112/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6733 - acc: 0.3186 - val_loss: 1.8228 - val_acc: 0.2300\n",
      "Epoch 113/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6725 - acc: 0.3143 - val_loss: 1.8207 - val_acc: 0.2367\n",
      "Epoch 114/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6714 - acc: 0.3086 - val_loss: 1.8227 - val_acc: 0.2200\n",
      "Epoch 115/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6699 - acc: 0.3171 - val_loss: 1.8140 - val_acc: 0.2367\n",
      "Epoch 116/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6689 - acc: 0.3243 - val_loss: 1.8306 - val_acc: 0.2300\n",
      "Epoch 117/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6684 - acc: 0.3143 - val_loss: 1.8230 - val_acc: 0.2300\n",
      "Epoch 118/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6670 - acc: 0.3171 - val_loss: 1.8288 - val_acc: 0.2333\n",
      "Epoch 119/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6657 - acc: 0.3286 - val_loss: 1.8261 - val_acc: 0.2267\n",
      "Epoch 120/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6647 - acc: 0.3171 - val_loss: 1.8247 - val_acc: 0.2200\n",
      "Epoch 121/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6634 - acc: 0.3171 - val_loss: 1.8152 - val_acc: 0.2267\n",
      "Epoch 122/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6623 - acc: 0.3200 - val_loss: 1.8210 - val_acc: 0.2267\n",
      "Epoch 123/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6614 - acc: 0.3229 - val_loss: 1.8174 - val_acc: 0.2233\n",
      "Epoch 124/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6601 - acc: 0.3186 - val_loss: 1.8245 - val_acc: 0.2367\n",
      "Epoch 125/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6591 - acc: 0.3171 - val_loss: 1.8290 - val_acc: 0.2367\n",
      "Epoch 126/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6573 - acc: 0.3186 - val_loss: 1.8295 - val_acc: 0.2267\n",
      "Epoch 127/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6565 - acc: 0.3200 - val_loss: 1.8174 - val_acc: 0.2400\n",
      "Epoch 128/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6546 - acc: 0.3357 - val_loss: 1.8324 - val_acc: 0.2200\n",
      "Epoch 129/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6550 - acc: 0.3129 - val_loss: 1.8266 - val_acc: 0.2233\n",
      "Epoch 130/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6528 - acc: 0.3257 - val_loss: 1.8176 - val_acc: 0.2567\n",
      "Epoch 131/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6524 - acc: 0.3229 - val_loss: 1.8281 - val_acc: 0.2533\n",
      "Epoch 132/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6522 - acc: 0.3286 - val_loss: 1.8165 - val_acc: 0.2233\n",
      "Epoch 133/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6505 - acc: 0.3257 - val_loss: 1.8354 - val_acc: 0.2500\n",
      "Epoch 134/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6490 - acc: 0.3200 - val_loss: 1.8281 - val_acc: 0.2733\n",
      "Epoch 135/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6487 - acc: 0.3314 - val_loss: 1.8250 - val_acc: 0.2267\n",
      "Epoch 136/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6472 - acc: 0.3229 - val_loss: 1.8213 - val_acc: 0.2333\n",
      "Epoch 137/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6462 - acc: 0.3200 - val_loss: 1.8225 - val_acc: 0.2300\n",
      "Epoch 138/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6466 - acc: 0.3214 - val_loss: 1.8263 - val_acc: 0.2367\n",
      "Epoch 139/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6438 - acc: 0.3300 - val_loss: 1.8512 - val_acc: 0.2467\n",
      "Epoch 140/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6439 - acc: 0.3286 - val_loss: 1.8181 - val_acc: 0.2300\n",
      "Epoch 141/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6422 - acc: 0.3286 - val_loss: 1.8329 - val_acc: 0.2367\n",
      "Epoch 142/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6411 - acc: 0.3314 - val_loss: 1.8214 - val_acc: 0.2667\n",
      "Epoch 143/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6410 - acc: 0.3286 - val_loss: 1.8376 - val_acc: 0.2533\n",
      "Epoch 144/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6403 - acc: 0.3443 - val_loss: 1.8324 - val_acc: 0.2333\n",
      "Epoch 145/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6393 - acc: 0.3300 - val_loss: 1.8230 - val_acc: 0.2200\n",
      "Epoch 146/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6369 - acc: 0.3386 - val_loss: 1.8371 - val_acc: 0.2267\n",
      "Epoch 147/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6389 - acc: 0.3157 - val_loss: 1.8362 - val_acc: 0.2333\n",
      "Epoch 148/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6370 - acc: 0.3343 - val_loss: 1.8303 - val_acc: 0.2533\n",
      "Epoch 149/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6357 - acc: 0.3371 - val_loss: 1.8217 - val_acc: 0.2133\n",
      "Epoch 150/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6349 - acc: 0.3286 - val_loss: 1.8305 - val_acc: 0.2267\n",
      "Epoch 151/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6346 - acc: 0.3300 - val_loss: 1.8259 - val_acc: 0.2267\n",
      "Epoch 152/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6338 - acc: 0.3314 - val_loss: 1.8307 - val_acc: 0.2333\n",
      "Epoch 153/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6322 - acc: 0.3443 - val_loss: 1.8293 - val_acc: 0.2333\n",
      "Epoch 154/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6302 - acc: 0.3514 - val_loss: 1.8366 - val_acc: 0.2233\n",
      "Epoch 155/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6303 - acc: 0.3271 - val_loss: 1.8231 - val_acc: 0.2333\n",
      "Epoch 156/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6309 - acc: 0.3329 - val_loss: 1.8345 - val_acc: 0.2267\n",
      "Epoch 157/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6291 - acc: 0.3400 - val_loss: 1.8269 - val_acc: 0.2200\n",
      "Epoch 158/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6293 - acc: 0.3386 - val_loss: 1.8348 - val_acc: 0.2267\n",
      "Epoch 159/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6272 - acc: 0.3300 - val_loss: 1.8354 - val_acc: 0.2467\n",
      "Epoch 160/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6266 - acc: 0.3343 - val_loss: 1.8328 - val_acc: 0.2300\n",
      "Epoch 161/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6258 - acc: 0.3343 - val_loss: 1.8423 - val_acc: 0.2300\n",
      "Epoch 162/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6252 - acc: 0.3329 - val_loss: 1.8387 - val_acc: 0.2267\n",
      "Epoch 163/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6240 - acc: 0.3357 - val_loss: 1.8378 - val_acc: 0.2267\n",
      "Epoch 164/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6227 - acc: 0.3386 - val_loss: 1.8279 - val_acc: 0.2133\n",
      "Epoch 165/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6223 - acc: 0.3243 - val_loss: 1.8411 - val_acc: 0.2400\n",
      "Epoch 166/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6218 - acc: 0.3329 - val_loss: 1.8359 - val_acc: 0.2233\n",
      "Epoch 167/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6209 - acc: 0.3357 - val_loss: 1.8403 - val_acc: 0.2567\n",
      "Epoch 168/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6196 - acc: 0.3457 - val_loss: 1.8461 - val_acc: 0.2267\n",
      "Epoch 169/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6187 - acc: 0.3443 - val_loss: 1.8541 - val_acc: 0.2233\n",
      "Epoch 170/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6182 - acc: 0.3371 - val_loss: 1.8351 - val_acc: 0.2267\n",
      "Epoch 171/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6160 - acc: 0.3257 - val_loss: 1.8515 - val_acc: 0.2633\n",
      "Epoch 172/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6151 - acc: 0.3443 - val_loss: 1.8403 - val_acc: 0.2200\n",
      "Epoch 173/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6171 - acc: 0.3400 - val_loss: 1.8402 - val_acc: 0.2433\n",
      "Epoch 174/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6157 - acc: 0.3343 - val_loss: 1.8442 - val_acc: 0.2167\n",
      "Epoch 175/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6156 - acc: 0.3400 - val_loss: 1.8432 - val_acc: 0.2200\n",
      "Epoch 176/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6134 - acc: 0.3300 - val_loss: 1.8421 - val_acc: 0.2667\n",
      "Epoch 177/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6124 - acc: 0.3414 - val_loss: 1.8398 - val_acc: 0.2200\n",
      "Epoch 178/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6130 - acc: 0.3386 - val_loss: 1.8437 - val_acc: 0.2300\n",
      "Epoch 179/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6124 - acc: 0.3443 - val_loss: 1.8382 - val_acc: 0.2233\n",
      "Epoch 180/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6102 - acc: 0.3414 - val_loss: 1.8428 - val_acc: 0.2667\n",
      "Epoch 181/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.3471 - val_loss: 1.8357 - val_acc: 0.2567\n",
      "Epoch 182/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6098 - acc: 0.3457 - val_loss: 1.8390 - val_acc: 0.2167\n",
      "Epoch 183/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.3329 - val_loss: 1.8442 - val_acc: 0.2300\n",
      "Epoch 184/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6083 - acc: 0.3500 - val_loss: 1.8470 - val_acc: 0.2267\n",
      "Epoch 185/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6066 - acc: 0.3429 - val_loss: 1.8661 - val_acc: 0.2233\n",
      "Epoch 186/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6077 - acc: 0.3414 - val_loss: 1.8461 - val_acc: 0.2267\n",
      "Epoch 187/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6057 - acc: 0.3543 - val_loss: 1.8475 - val_acc: 0.2200\n",
      "Epoch 188/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6066 - acc: 0.3414 - val_loss: 1.8435 - val_acc: 0.2200\n",
      "Epoch 189/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6037 - acc: 0.3543 - val_loss: 1.8558 - val_acc: 0.2200\n",
      "Epoch 190/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6057 - acc: 0.3314 - val_loss: 1.8426 - val_acc: 0.2333\n",
      "Epoch 191/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6033 - acc: 0.3571 - val_loss: 1.8621 - val_acc: 0.2233\n",
      "Epoch 192/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6037 - acc: 0.3486 - val_loss: 1.8528 - val_acc: 0.2200\n",
      "Epoch 193/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6021 - acc: 0.3443 - val_loss: 1.8570 - val_acc: 0.2300\n",
      "Epoch 194/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6017 - acc: 0.3400 - val_loss: 1.8503 - val_acc: 0.2300\n",
      "Epoch 195/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6012 - acc: 0.3471 - val_loss: 1.8487 - val_acc: 0.2467\n",
      "Epoch 196/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6008 - acc: 0.3500 - val_loss: 1.8509 - val_acc: 0.2200\n",
      "Epoch 197/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5994 - acc: 0.3500 - val_loss: 1.8513 - val_acc: 0.2300\n",
      "Epoch 198/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5998 - acc: 0.3400 - val_loss: 1.8613 - val_acc: 0.2233\n",
      "Epoch 199/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5995 - acc: 0.3400 - val_loss: 1.8578 - val_acc: 0.2267\n",
      "Epoch 200/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5967 - acc: 0.3457 - val_loss: 1.8624 - val_acc: 0.2633\n",
      "Epoch 201/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5978 - acc: 0.3443 - val_loss: 1.8496 - val_acc: 0.2167\n",
      "Epoch 202/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5972 - acc: 0.3486 - val_loss: 1.8499 - val_acc: 0.2100\n",
      "Epoch 203/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5959 - acc: 0.3371 - val_loss: 1.8599 - val_acc: 0.2333\n",
      "Epoch 204/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5947 - acc: 0.3629 - val_loss: 1.8490 - val_acc: 0.2167\n",
      "Epoch 205/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5946 - acc: 0.3443 - val_loss: 1.8532 - val_acc: 0.2233\n",
      "Epoch 206/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5939 - acc: 0.3543 - val_loss: 1.8498 - val_acc: 0.2133\n",
      "Epoch 207/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5940 - acc: 0.3543 - val_loss: 1.8574 - val_acc: 0.2267\n",
      "Epoch 208/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5914 - acc: 0.3471 - val_loss: 1.8582 - val_acc: 0.2233\n",
      "Epoch 209/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5913 - acc: 0.3514 - val_loss: 1.8553 - val_acc: 0.2367\n",
      "Epoch 210/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5908 - acc: 0.3471 - val_loss: 1.8556 - val_acc: 0.2300\n",
      "Epoch 211/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5897 - acc: 0.3429 - val_loss: 1.8479 - val_acc: 0.2600\n",
      "Epoch 212/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5907 - acc: 0.3514 - val_loss: 1.8596 - val_acc: 0.2233\n",
      "Epoch 213/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5888 - acc: 0.3514 - val_loss: 1.8556 - val_acc: 0.2300\n",
      "Epoch 214/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5899 - acc: 0.3443 - val_loss: 1.8616 - val_acc: 0.2300\n",
      "Epoch 215/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5880 - acc: 0.3471 - val_loss: 1.8609 - val_acc: 0.2667\n",
      "Epoch 216/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5890 - acc: 0.3514 - val_loss: 1.8657 - val_acc: 0.2300\n",
      "Epoch 217/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5874 - acc: 0.3457 - val_loss: 1.8679 - val_acc: 0.2267\n",
      "Epoch 218/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5871 - acc: 0.3457 - val_loss: 1.8616 - val_acc: 0.2300\n",
      "Epoch 219/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5859 - acc: 0.3486 - val_loss: 1.8683 - val_acc: 0.2267\n",
      "Epoch 220/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5850 - acc: 0.3529 - val_loss: 1.8619 - val_acc: 0.2267\n",
      "Epoch 221/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5851 - acc: 0.3471 - val_loss: 1.8644 - val_acc: 0.2267\n",
      "Epoch 222/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5838 - acc: 0.3514 - val_loss: 1.8806 - val_acc: 0.2433\n",
      "Epoch 223/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3443 - val_loss: 1.8651 - val_acc: 0.2600\n",
      "Epoch 224/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5839 - acc: 0.3543 - val_loss: 1.8794 - val_acc: 0.2400\n",
      "Epoch 225/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5828 - acc: 0.3443 - val_loss: 1.8600 - val_acc: 0.2200\n",
      "Epoch 226/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5830 - acc: 0.3500 - val_loss: 1.8729 - val_acc: 0.2267\n",
      "Epoch 227/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5820 - acc: 0.3500 - val_loss: 1.8668 - val_acc: 0.2167\n",
      "Epoch 228/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5819 - acc: 0.3443 - val_loss: 1.8713 - val_acc: 0.2267\n",
      "Epoch 229/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5796 - acc: 0.3571 - val_loss: 1.8747 - val_acc: 0.2233\n",
      "Epoch 230/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5813 - acc: 0.3414 - val_loss: 1.8647 - val_acc: 0.2167\n",
      "Epoch 231/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5793 - acc: 0.3543 - val_loss: 1.8655 - val_acc: 0.2233\n",
      "Epoch 232/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5804 - acc: 0.3514 - val_loss: 1.8779 - val_acc: 0.2200\n",
      "Epoch 233/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5791 - acc: 0.3486 - val_loss: 1.8838 - val_acc: 0.2300\n",
      "Epoch 234/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5786 - acc: 0.3457 - val_loss: 1.8718 - val_acc: 0.2200\n",
      "Epoch 235/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5771 - acc: 0.3586 - val_loss: 1.8738 - val_acc: 0.2100\n",
      "Epoch 236/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5773 - acc: 0.3457 - val_loss: 1.8691 - val_acc: 0.2167\n",
      "Epoch 237/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3471 - val_loss: 1.8761 - val_acc: 0.2300\n",
      "Epoch 238/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5769 - acc: 0.3457 - val_loss: 1.8660 - val_acc: 0.2133\n",
      "Epoch 239/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5765 - acc: 0.3543 - val_loss: 1.8731 - val_acc: 0.2300\n",
      "Epoch 240/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5745 - acc: 0.3543 - val_loss: 1.8775 - val_acc: 0.2467\n",
      "Epoch 241/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5743 - acc: 0.3571 - val_loss: 1.8799 - val_acc: 0.2433\n",
      "Epoch 242/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5739 - acc: 0.3543 - val_loss: 1.8785 - val_acc: 0.2067\n",
      "Epoch 243/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5738 - acc: 0.3600 - val_loss: 1.8845 - val_acc: 0.2167\n",
      "Epoch 244/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5707 - acc: 0.3600 - val_loss: 1.8760 - val_acc: 0.2200\n",
      "Epoch 245/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5710 - acc: 0.3586 - val_loss: 1.8889 - val_acc: 0.2300\n",
      "Epoch 246/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5707 - acc: 0.3571 - val_loss: 1.8754 - val_acc: 0.2633\n",
      "Epoch 247/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5718 - acc: 0.3557 - val_loss: 1.8653 - val_acc: 0.2200\n",
      "Epoch 248/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5710 - acc: 0.3557 - val_loss: 1.8728 - val_acc: 0.2300\n",
      "Epoch 249/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5708 - acc: 0.3471 - val_loss: 1.8868 - val_acc: 0.2267\n",
      "Epoch 250/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5675 - acc: 0.3629 - val_loss: 1.9023 - val_acc: 0.2167\n",
      "Epoch 251/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5699 - acc: 0.3471 - val_loss: 1.8843 - val_acc: 0.2333\n",
      "Epoch 252/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3657 - val_loss: 1.8804 - val_acc: 0.2300\n",
      "Epoch 253/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5684 - acc: 0.3543 - val_loss: 1.8865 - val_acc: 0.2167\n",
      "Epoch 254/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5674 - acc: 0.3557 - val_loss: 1.8857 - val_acc: 0.2267\n",
      "Epoch 255/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5677 - acc: 0.3471 - val_loss: 1.8744 - val_acc: 0.2300\n",
      "Epoch 256/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5653 - acc: 0.3529 - val_loss: 1.8795 - val_acc: 0.2533\n",
      "Epoch 257/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5662 - acc: 0.3486 - val_loss: 1.8947 - val_acc: 0.2367\n",
      "Epoch 258/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5644 - acc: 0.3600 - val_loss: 1.9015 - val_acc: 0.2567\n",
      "Epoch 259/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5643 - acc: 0.3629 - val_loss: 1.8974 - val_acc: 0.2233\n",
      "Epoch 260/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5653 - acc: 0.3614 - val_loss: 1.9021 - val_acc: 0.2233\n",
      "Epoch 261/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5632 - acc: 0.3586 - val_loss: 1.8782 - val_acc: 0.2167\n",
      "Epoch 262/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3600 - val_loss: 1.8893 - val_acc: 0.2100\n",
      "Epoch 263/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5627 - acc: 0.3643 - val_loss: 1.8925 - val_acc: 0.2200\n",
      "Epoch 264/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3557 - val_loss: 1.8868 - val_acc: 0.2067\n",
      "Epoch 265/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5634 - acc: 0.3657 - val_loss: 1.8963 - val_acc: 0.2267\n",
      "Epoch 266/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5609 - acc: 0.3629 - val_loss: 1.9026 - val_acc: 0.2500\n",
      "Epoch 267/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5610 - acc: 0.3657 - val_loss: 1.8956 - val_acc: 0.2267\n",
      "Epoch 268/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5608 - acc: 0.3629 - val_loss: 1.8804 - val_acc: 0.2167\n",
      "Epoch 269/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5596 - acc: 0.3657 - val_loss: 1.8882 - val_acc: 0.2167\n",
      "Epoch 270/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5592 - acc: 0.3686 - val_loss: 1.8748 - val_acc: 0.2333\n",
      "Epoch 271/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5605 - acc: 0.3643 - val_loss: 1.8973 - val_acc: 0.2300\n",
      "Epoch 272/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5591 - acc: 0.3571 - val_loss: 1.9101 - val_acc: 0.2267\n",
      "Epoch 273/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5587 - acc: 0.3571 - val_loss: 1.9020 - val_acc: 0.2133\n",
      "Epoch 274/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5642 - acc: 0.336 - ETA: 0s - loss: 1.5584 - acc: 0.3471 - val_loss: 1.8899 - val_acc: 0.2233\n",
      "Epoch 275/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5585 - acc: 0.3586 - val_loss: 1.9001 - val_acc: 0.2233\n",
      "Epoch 276/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5575 - acc: 0.3543 - val_loss: 1.9018 - val_acc: 0.2567\n",
      "Epoch 277/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5576 - acc: 0.3743 - val_loss: 1.8966 - val_acc: 0.2167\n",
      "Epoch 278/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5574 - acc: 0.3600 - val_loss: 1.9085 - val_acc: 0.2300\n",
      "Epoch 279/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5554 - acc: 0.3586 - val_loss: 1.8916 - val_acc: 0.2200\n",
      "Epoch 280/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5555 - acc: 0.3643 - val_loss: 1.8933 - val_acc: 0.2167\n",
      "Epoch 281/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5537 - acc: 0.3557 - val_loss: 1.8958 - val_acc: 0.2333\n",
      "Epoch 282/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5550 - acc: 0.3686 - val_loss: 1.8885 - val_acc: 0.2167\n",
      "Epoch 283/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5537 - acc: 0.3571 - val_loss: 1.8992 - val_acc: 0.2233\n",
      "Epoch 284/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5538 - acc: 0.3586 - val_loss: 1.8983 - val_acc: 0.2600\n",
      "Epoch 285/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5531 - acc: 0.3586 - val_loss: 1.8938 - val_acc: 0.2233\n",
      "Epoch 286/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5532 - acc: 0.3557 - val_loss: 1.9053 - val_acc: 0.2233\n",
      "Epoch 287/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5523 - acc: 0.3700 - val_loss: 1.9063 - val_acc: 0.2100\n",
      "Epoch 288/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5520 - acc: 0.3614 - val_loss: 1.8988 - val_acc: 0.2100\n",
      "Epoch 289/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5515 - acc: 0.3600 - val_loss: 1.9081 - val_acc: 0.2100\n",
      "Epoch 290/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5510 - acc: 0.3657 - val_loss: 1.9099 - val_acc: 0.2200\n",
      "Epoch 291/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5509 - acc: 0.3586 - val_loss: 1.9089 - val_acc: 0.2167\n",
      "Epoch 292/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5503 - acc: 0.3514 - val_loss: 1.9039 - val_acc: 0.2400\n",
      "Epoch 293/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5502 - acc: 0.3571 - val_loss: 1.9063 - val_acc: 0.2333\n",
      "Epoch 294/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5494 - acc: 0.3571 - val_loss: 1.9064 - val_acc: 0.2533\n",
      "Epoch 295/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5495 - acc: 0.3729 - val_loss: 1.9166 - val_acc: 0.2133\n",
      "Epoch 296/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5490 - acc: 0.3571 - val_loss: 1.9152 - val_acc: 0.2067\n",
      "Epoch 297/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5475 - acc: 0.3586 - val_loss: 1.9183 - val_acc: 0.2200\n",
      "Epoch 298/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5473 - acc: 0.3486 - val_loss: 1.9015 - val_acc: 0.2300\n",
      "Epoch 299/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5480 - acc: 0.3671 - val_loss: 1.9222 - val_acc: 0.2300\n",
      "Epoch 300/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5469 - acc: 0.3657 - val_loss: 1.9092 - val_acc: 0.2300\n",
      "Epoch 301/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5465 - acc: 0.3629 - val_loss: 1.8993 - val_acc: 0.2200\n",
      "Epoch 302/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5476 - acc: 0.3686 - val_loss: 1.9137 - val_acc: 0.2200\n",
      "Epoch 303/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5461 - acc: 0.3543 - val_loss: 1.9058 - val_acc: 0.2367\n",
      "Epoch 304/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5448 - acc: 0.3714 - val_loss: 1.9129 - val_acc: 0.2200\n",
      "Epoch 305/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5428 - acc: 0.3671 - val_loss: 1.9211 - val_acc: 0.2233\n",
      "Epoch 306/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5455 - acc: 0.3643 - val_loss: 1.9147 - val_acc: 0.2333\n",
      "Epoch 307/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3729 - val_loss: 1.9137 - val_acc: 0.2300\n",
      "Epoch 308/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5449 - acc: 0.3600 - val_loss: 1.9182 - val_acc: 0.2333\n",
      "Epoch 309/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5431 - acc: 0.3629 - val_loss: 1.9223 - val_acc: 0.2333\n",
      "Epoch 310/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5444 - acc: 0.3657 - val_loss: 1.9143 - val_acc: 0.2233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5423 - acc: 0.3571 - val_loss: 1.9101 - val_acc: 0.2300\n",
      "Epoch 312/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5420 - acc: 0.3729 - val_loss: 1.9335 - val_acc: 0.2333\n",
      "Epoch 313/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3714 - val_loss: 1.9127 - val_acc: 0.2567\n",
      "Epoch 314/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5429 - acc: 0.3671 - val_loss: 1.9181 - val_acc: 0.2300\n",
      "Epoch 315/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3643 - val_loss: 1.9162 - val_acc: 0.2233\n",
      "Epoch 316/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5401 - acc: 0.3643 - val_loss: 1.9192 - val_acc: 0.2267\n",
      "Epoch 317/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5409 - acc: 0.3600 - val_loss: 1.9272 - val_acc: 0.2333\n",
      "Epoch 318/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5404 - acc: 0.3643 - val_loss: 1.9161 - val_acc: 0.2300\n",
      "Epoch 319/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5386 - acc: 0.3600 - val_loss: 1.9150 - val_acc: 0.2267\n",
      "Epoch 320/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5388 - acc: 0.3600 - val_loss: 1.9338 - val_acc: 0.2233\n",
      "Epoch 321/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5388 - acc: 0.3700 - val_loss: 1.9331 - val_acc: 0.2300\n",
      "Epoch 322/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5370 - acc: 0.3686 - val_loss: 1.9218 - val_acc: 0.2500\n",
      "Epoch 323/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5378 - acc: 0.3700 - val_loss: 1.9235 - val_acc: 0.2300\n",
      "Epoch 324/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5368 - acc: 0.3700 - val_loss: 1.9299 - val_acc: 0.2233\n",
      "Epoch 325/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5383 - acc: 0.3600 - val_loss: 1.9244 - val_acc: 0.2133\n",
      "Epoch 326/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5361 - acc: 0.3614 - val_loss: 1.9342 - val_acc: 0.2300\n",
      "Epoch 327/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5354 - acc: 0.3714 - val_loss: 1.9471 - val_acc: 0.2367\n",
      "Epoch 328/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5364 - acc: 0.3614 - val_loss: 1.9368 - val_acc: 0.2233\n",
      "Epoch 329/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5350 - acc: 0.3743 - val_loss: 1.9220 - val_acc: 0.2267\n",
      "Epoch 330/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5347 - acc: 0.3657 - val_loss: 1.9304 - val_acc: 0.2333\n",
      "Epoch 331/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.3657 - val_loss: 1.9174 - val_acc: 0.2133\n",
      "Epoch 332/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5341 - acc: 0.3643 - val_loss: 1.9381 - val_acc: 0.2533\n",
      "Epoch 333/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5353 - acc: 0.3743 - val_loss: 1.9301 - val_acc: 0.2300\n",
      "Epoch 334/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5346 - acc: 0.3614 - val_loss: 1.9314 - val_acc: 0.2300\n",
      "Epoch 335/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5323 - acc: 0.3800 - val_loss: 1.9315 - val_acc: 0.2567\n",
      "Epoch 336/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.3800 - val_loss: 1.9284 - val_acc: 0.2300\n",
      "Epoch 337/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5327 - acc: 0.3771 - val_loss: 1.9378 - val_acc: 0.2233\n",
      "Epoch 338/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5317 - acc: 0.3800 - val_loss: 1.9299 - val_acc: 0.2133\n",
      "Epoch 339/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5308 - acc: 0.3714 - val_loss: 1.9232 - val_acc: 0.2133\n",
      "Epoch 340/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5324 - acc: 0.3743 - val_loss: 1.9477 - val_acc: 0.2233\n",
      "Epoch 341/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5314 - acc: 0.3786 - val_loss: 1.9289 - val_acc: 0.2333\n",
      "Epoch 342/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5302 - acc: 0.3657 - val_loss: 1.9502 - val_acc: 0.2200\n",
      "Epoch 343/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5301 - acc: 0.3700 - val_loss: 1.9388 - val_acc: 0.2233\n",
      "Epoch 344/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5297 - acc: 0.3671 - val_loss: 1.9484 - val_acc: 0.2200\n",
      "Epoch 345/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5281 - acc: 0.3671 - val_loss: 1.9503 - val_acc: 0.2467\n",
      "Epoch 346/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5288 - acc: 0.3729 - val_loss: 1.9376 - val_acc: 0.2267\n",
      "Epoch 347/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5265 - acc: 0.3814 - val_loss: 1.9232 - val_acc: 0.2233\n",
      "Epoch 348/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5269 - acc: 0.3643 - val_loss: 1.9376 - val_acc: 0.2167\n",
      "Epoch 349/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5268 - acc: 0.3700 - val_loss: 1.9360 - val_acc: 0.2200\n",
      "Epoch 350/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5265 - acc: 0.3714 - val_loss: 1.9393 - val_acc: 0.2300\n",
      "Epoch 351/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5247 - acc: 0.3800 - val_loss: 1.9319 - val_acc: 0.2367\n",
      "Epoch 352/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5263 - acc: 0.3757 - val_loss: 1.9366 - val_acc: 0.2300\n",
      "Epoch 353/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5239 - acc: 0.3657 - val_loss: 1.9490 - val_acc: 0.2267\n",
      "Epoch 354/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5259 - acc: 0.3786 - val_loss: 1.9396 - val_acc: 0.2167\n",
      "Epoch 355/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5240 - acc: 0.3614 - val_loss: 1.9582 - val_acc: 0.2300\n",
      "Epoch 356/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5239 - acc: 0.3729 - val_loss: 1.9498 - val_acc: 0.2400\n",
      "Epoch 357/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5237 - acc: 0.3743 - val_loss: 1.9389 - val_acc: 0.2300\n",
      "Epoch 358/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5229 - acc: 0.3743 - val_loss: 1.9579 - val_acc: 0.2167\n",
      "Epoch 359/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5232 - acc: 0.3786 - val_loss: 1.9503 - val_acc: 0.2200\n",
      "Epoch 360/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5213 - acc: 0.3686 - val_loss: 1.9548 - val_acc: 0.2167\n",
      "Epoch 361/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5207 - acc: 0.3714 - val_loss: 1.9498 - val_acc: 0.2300\n",
      "Epoch 362/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5219 - acc: 0.3657 - val_loss: 1.9538 - val_acc: 0.2200\n",
      "Epoch 363/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5193 - acc: 0.3757 - val_loss: 1.9556 - val_acc: 0.2300\n",
      "Epoch 364/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5204 - acc: 0.3643 - val_loss: 1.9367 - val_acc: 0.2300\n",
      "Epoch 365/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5203 - acc: 0.3829 - val_loss: 1.9551 - val_acc: 0.2167\n",
      "Epoch 366/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5200 - acc: 0.3757 - val_loss: 1.9417 - val_acc: 0.2133\n",
      "Epoch 367/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5201 - acc: 0.3786 - val_loss: 1.9530 - val_acc: 0.2233\n",
      "Epoch 368/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5186 - acc: 0.3814 - val_loss: 1.9565 - val_acc: 0.2333\n",
      "Epoch 369/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5185 - acc: 0.3714 - val_loss: 1.9361 - val_acc: 0.2367\n",
      "Epoch 370/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5176 - acc: 0.3729 - val_loss: 1.9570 - val_acc: 0.2333\n",
      "Epoch 371/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5184 - acc: 0.3643 - val_loss: 1.9571 - val_acc: 0.2200\n",
      "Epoch 372/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5170 - acc: 0.3743 - val_loss: 1.9632 - val_acc: 0.2267\n",
      "Epoch 373/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.5180 - acc: 0.3757 - val_loss: 1.9592 - val_acc: 0.2200\n",
      "Epoch 374/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5173 - acc: 0.3686 - val_loss: 1.9537 - val_acc: 0.2333\n",
      "Epoch 375/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5156 - acc: 0.3843 - val_loss: 1.9444 - val_acc: 0.2267\n",
      "Epoch 376/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5158 - acc: 0.3786 - val_loss: 1.9530 - val_acc: 0.2200\n",
      "Epoch 377/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5149 - acc: 0.3714 - val_loss: 1.9757 - val_acc: 0.2233\n",
      "Epoch 378/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5157 - acc: 0.3743 - val_loss: 1.9806 - val_acc: 0.2333\n",
      "Epoch 379/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5162 - acc: 0.3714 - val_loss: 1.9533 - val_acc: 0.2400\n",
      "Epoch 380/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5140 - acc: 0.3686 - val_loss: 1.9629 - val_acc: 0.2567\n",
      "Epoch 381/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5154 - acc: 0.3800 - val_loss: 1.9754 - val_acc: 0.2133\n",
      "Epoch 382/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5139 - acc: 0.3771 - val_loss: 1.9585 - val_acc: 0.2200\n",
      "Epoch 383/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5129 - acc: 0.3800 - val_loss: 1.9699 - val_acc: 0.2433\n",
      "Epoch 384/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5138 - acc: 0.3829 - val_loss: 1.9514 - val_acc: 0.2233\n",
      "Epoch 385/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5139 - acc: 0.3829 - val_loss: 1.9596 - val_acc: 0.2233\n",
      "Epoch 386/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5130 - acc: 0.3743 - val_loss: 1.9531 - val_acc: 0.2400\n",
      "Epoch 387/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5128 - acc: 0.3829 - val_loss: 1.9622 - val_acc: 0.2333\n",
      "Epoch 388/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5116 - acc: 0.3757 - val_loss: 1.9702 - val_acc: 0.2500\n",
      "Epoch 389/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5134 - acc: 0.3800 - val_loss: 1.9524 - val_acc: 0.2267\n",
      "Epoch 390/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5099 - acc: 0.3686 - val_loss: 1.9548 - val_acc: 0.2400\n",
      "Epoch 391/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5104 - acc: 0.3843 - val_loss: 1.9592 - val_acc: 0.2367\n",
      "Epoch 392/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5111 - acc: 0.3743 - val_loss: 1.9601 - val_acc: 0.2267\n",
      "Epoch 393/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5112 - acc: 0.3800 - val_loss: 1.9741 - val_acc: 0.2233\n",
      "Epoch 394/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5105 - acc: 0.3786 - val_loss: 1.9588 - val_acc: 0.2300\n",
      "Epoch 395/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5104 - acc: 0.3871 - val_loss: 1.9600 - val_acc: 0.2233\n",
      "Epoch 396/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5098 - acc: 0.3771 - val_loss: 1.9640 - val_acc: 0.2267\n",
      "Epoch 397/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5086 - acc: 0.3871 - val_loss: 1.9772 - val_acc: 0.2300\n",
      "Epoch 398/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5099 - acc: 0.3657 - val_loss: 1.9613 - val_acc: 0.2300\n",
      "Epoch 399/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5088 - acc: 0.3771 - val_loss: 1.9809 - val_acc: 0.2333\n",
      "Epoch 400/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5089 - acc: 0.3743 - val_loss: 1.9578 - val_acc: 0.2233\n",
      "Epoch 401/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5089 - acc: 0.3686 - val_loss: 1.9648 - val_acc: 0.2333\n",
      "Epoch 402/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5066 - acc: 0.3800 - val_loss: 1.9684 - val_acc: 0.2267\n",
      "Epoch 403/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5053 - acc: 0.3843 - val_loss: 1.9912 - val_acc: 0.2200\n",
      "Epoch 404/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5062 - acc: 0.3786 - val_loss: 1.9657 - val_acc: 0.2367\n",
      "Epoch 405/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5055 - acc: 0.3829 - val_loss: 1.9852 - val_acc: 0.2300\n",
      "Epoch 406/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5065 - acc: 0.3857 - val_loss: 1.9863 - val_acc: 0.2200\n",
      "Epoch 407/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5065 - acc: 0.3843 - val_loss: 1.9790 - val_acc: 0.2367\n",
      "Epoch 408/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5064 - acc: 0.3843 - val_loss: 1.9732 - val_acc: 0.2300\n",
      "Epoch 409/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5023 - acc: 0.3757 - val_loss: 1.9857 - val_acc: 0.2400\n",
      "Epoch 410/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5048 - acc: 0.3814 - val_loss: 1.9641 - val_acc: 0.2233\n",
      "Epoch 411/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5047 - acc: 0.3771 - val_loss: 1.9841 - val_acc: 0.2267\n",
      "Epoch 412/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5044 - acc: 0.3943 - val_loss: 1.9759 - val_acc: 0.2267\n",
      "Epoch 413/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5026 - acc: 0.3771 - val_loss: 1.9729 - val_acc: 0.2367\n",
      "Epoch 414/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5038 - acc: 0.3829 - val_loss: 1.9852 - val_acc: 0.2333\n",
      "Epoch 415/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5011 - acc: 0.3729 - val_loss: 1.9851 - val_acc: 0.2400\n",
      "Epoch 416/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5024 - acc: 0.3886 - val_loss: 1.9952 - val_acc: 0.2167\n",
      "Epoch 417/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5030 - acc: 0.3786 - val_loss: 1.9829 - val_acc: 0.2267\n",
      "Epoch 418/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5014 - acc: 0.3914 - val_loss: 1.9851 - val_acc: 0.2367\n",
      "Epoch 419/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5016 - acc: 0.3800 - val_loss: 1.9724 - val_acc: 0.2267\n",
      "Epoch 420/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5016 - acc: 0.3829 - val_loss: 1.9958 - val_acc: 0.2333\n",
      "Epoch 421/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5014 - acc: 0.3886 - val_loss: 1.9737 - val_acc: 0.2400\n",
      "Epoch 422/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5004 - acc: 0.3786 - val_loss: 1.9725 - val_acc: 0.2367\n",
      "Epoch 423/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5002 - acc: 0.3871 - val_loss: 1.9869 - val_acc: 0.2400\n",
      "Epoch 424/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.5000 - acc: 0.3771 - val_loss: 1.9639 - val_acc: 0.2233\n",
      "Epoch 425/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4993 - acc: 0.3957 - val_loss: 1.9668 - val_acc: 0.2367\n",
      "Epoch 426/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4986 - acc: 0.3957 - val_loss: 1.9875 - val_acc: 0.2200\n",
      "Epoch 427/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4983 - acc: 0.3843 - val_loss: 1.9853 - val_acc: 0.2600\n",
      "Epoch 428/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4988 - acc: 0.3871 - val_loss: 2.0063 - val_acc: 0.2267\n",
      "Epoch 429/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4987 - acc: 0.3814 - val_loss: 1.9713 - val_acc: 0.2300\n",
      "Epoch 430/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4981 - acc: 0.3871 - val_loss: 1.9812 - val_acc: 0.2500\n",
      "Epoch 431/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4993 - acc: 0.3929 - val_loss: 1.9891 - val_acc: 0.2333\n",
      "Epoch 432/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4981 - acc: 0.3843 - val_loss: 1.9830 - val_acc: 0.2267\n",
      "Epoch 433/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4973 - acc: 0.3814 - val_loss: 1.9856 - val_acc: 0.2400\n",
      "Epoch 434/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4980 - acc: 0.3757 - val_loss: 1.9853 - val_acc: 0.2267\n",
      "Epoch 435/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4964 - acc: 0.3929 - val_loss: 1.9861 - val_acc: 0.2333\n",
      "Epoch 436/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4979 - acc: 0.3900 - val_loss: 1.9701 - val_acc: 0.2267\n",
      "Epoch 437/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4963 - acc: 0.3957 - val_loss: 1.9878 - val_acc: 0.2300\n",
      "Epoch 438/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4955 - acc: 0.3900 - val_loss: 1.9653 - val_acc: 0.2300\n",
      "Epoch 439/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.3771 - val_loss: 1.9771 - val_acc: 0.2267\n",
      "Epoch 440/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4965 - acc: 0.3800 - val_loss: 1.9776 - val_acc: 0.2300\n",
      "Epoch 441/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4957 - acc: 0.3871 - val_loss: 1.9809 - val_acc: 0.2267\n",
      "Epoch 442/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4953 - acc: 0.3857 - val_loss: 1.9882 - val_acc: 0.2233\n",
      "Epoch 443/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4945 - acc: 0.3843 - val_loss: 1.9830 - val_acc: 0.2400\n",
      "Epoch 444/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4954 - acc: 0.3857 - val_loss: 1.9940 - val_acc: 0.2233\n",
      "Epoch 445/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4949 - acc: 0.3814 - val_loss: 1.9942 - val_acc: 0.2333\n",
      "Epoch 446/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4940 - acc: 0.3800 - val_loss: 2.0022 - val_acc: 0.2233\n",
      "Epoch 447/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4937 - acc: 0.3843 - val_loss: 2.0141 - val_acc: 0.2267\n",
      "Epoch 448/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4933 - acc: 0.3829 - val_loss: 1.9865 - val_acc: 0.2267\n",
      "Epoch 449/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4931 - acc: 0.3957 - val_loss: 2.0034 - val_acc: 0.2333\n",
      "Epoch 450/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4940 - acc: 0.3871 - val_loss: 1.9934 - val_acc: 0.2267\n",
      "Epoch 451/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4931 - acc: 0.3829 - val_loss: 1.9953 - val_acc: 0.2233\n",
      "Epoch 452/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4924 - acc: 0.3871 - val_loss: 1.9909 - val_acc: 0.2233\n",
      "Epoch 453/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4919 - acc: 0.3857 - val_loss: 1.9946 - val_acc: 0.2367\n",
      "Epoch 454/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4920 - acc: 0.3900 - val_loss: 1.9991 - val_acc: 0.2267\n",
      "Epoch 455/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4918 - acc: 0.3800 - val_loss: 1.9892 - val_acc: 0.2333\n",
      "Epoch 456/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4909 - acc: 0.3843 - val_loss: 1.9933 - val_acc: 0.2467\n",
      "Epoch 457/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4909 - acc: 0.3900 - val_loss: 1.9977 - val_acc: 0.2267\n",
      "Epoch 458/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4915 - acc: 0.3829 - val_loss: 1.9908 - val_acc: 0.2267\n",
      "Epoch 459/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4895 - acc: 0.3957 - val_loss: 1.9937 - val_acc: 0.2433\n",
      "Epoch 460/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4913 - acc: 0.3871 - val_loss: 1.9932 - val_acc: 0.2300\n",
      "Epoch 461/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4896 - acc: 0.3929 - val_loss: 2.0019 - val_acc: 0.2300\n",
      "Epoch 462/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4897 - acc: 0.3871 - val_loss: 1.9903 - val_acc: 0.2267\n",
      "Epoch 463/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4887 - acc: 0.3843 - val_loss: 1.9986 - val_acc: 0.2400\n",
      "Epoch 464/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4885 - acc: 0.3900 - val_loss: 1.9858 - val_acc: 0.2267\n",
      "Epoch 465/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4889 - acc: 0.3871 - val_loss: 1.9907 - val_acc: 0.2267\n",
      "Epoch 466/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4883 - acc: 0.3900 - val_loss: 2.0003 - val_acc: 0.2300\n",
      "Epoch 467/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4886 - acc: 0.3800 - val_loss: 2.0023 - val_acc: 0.2300\n",
      "Epoch 468/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4880 - acc: 0.3814 - val_loss: 1.9929 - val_acc: 0.2267\n",
      "Epoch 469/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4871 - acc: 0.3929 - val_loss: 2.0192 - val_acc: 0.2367\n",
      "Epoch 470/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4884 - acc: 0.3843 - val_loss: 2.0093 - val_acc: 0.2233\n",
      "Epoch 471/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4874 - acc: 0.3929 - val_loss: 2.0059 - val_acc: 0.2233\n",
      "Epoch 472/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4861 - acc: 0.3943 - val_loss: 2.0034 - val_acc: 0.2400\n",
      "Epoch 473/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4866 - acc: 0.3829 - val_loss: 2.0050 - val_acc: 0.2367\n",
      "Epoch 474/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4854 - acc: 0.3914 - val_loss: 1.9965 - val_acc: 0.2300\n",
      "Epoch 475/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4858 - acc: 0.3886 - val_loss: 2.0025 - val_acc: 0.2200\n",
      "Epoch 476/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4849 - acc: 0.3886 - val_loss: 2.0027 - val_acc: 0.2233\n",
      "Epoch 477/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4855 - acc: 0.3957 - val_loss: 2.0070 - val_acc: 0.2233\n",
      "Epoch 478/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4853 - acc: 0.3957 - val_loss: 2.0044 - val_acc: 0.2233\n",
      "Epoch 479/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4849 - acc: 0.3871 - val_loss: 1.9957 - val_acc: 0.2233\n",
      "Epoch 480/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4841 - acc: 0.3957 - val_loss: 2.0158 - val_acc: 0.2367\n",
      "Epoch 481/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4840 - acc: 0.3900 - val_loss: 2.0117 - val_acc: 0.2300\n",
      "Epoch 482/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3971 - val_loss: 2.0159 - val_acc: 0.2333\n",
      "Epoch 483/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4837 - acc: 0.3971 - val_loss: 2.0118 - val_acc: 0.2400\n",
      "Epoch 484/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4830 - acc: 0.3986 - val_loss: 2.0127 - val_acc: 0.2400\n",
      "Epoch 485/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4828 - acc: 0.3957 - val_loss: 2.0087 - val_acc: 0.2267\n",
      "Epoch 486/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3871 - val_loss: 2.0048 - val_acc: 0.2433\n",
      "Epoch 487/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4830 - acc: 0.3929 - val_loss: 2.0077 - val_acc: 0.2467\n",
      "Epoch 488/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3914 - val_loss: 2.0089 - val_acc: 0.2467\n",
      "Epoch 489/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4821 - acc: 0.3857 - val_loss: 1.9987 - val_acc: 0.2300\n",
      "Epoch 490/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4828 - acc: 0.3900 - val_loss: 1.9986 - val_acc: 0.2333\n",
      "Epoch 491/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4824 - acc: 0.3871 - val_loss: 2.0159 - val_acc: 0.2267\n",
      "Epoch 492/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4818 - acc: 0.3914 - val_loss: 1.9974 - val_acc: 0.2333\n",
      "Epoch 493/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4818 - acc: 0.3929 - val_loss: 2.0051 - val_acc: 0.2300\n",
      "Epoch 494/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4821 - acc: 0.3957 - val_loss: 2.0015 - val_acc: 0.2200\n",
      "Epoch 495/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4814 - acc: 0.3943 - val_loss: 2.0131 - val_acc: 0.2300\n",
      "Epoch 496/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4808 - acc: 0.3914 - val_loss: 2.0100 - val_acc: 0.2433\n",
      "Epoch 497/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4792 - acc: 0.3871 - val_loss: 2.0129 - val_acc: 0.2467\n",
      "Epoch 498/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4787 - acc: 0.3814 - val_loss: 2.0401 - val_acc: 0.2333\n",
      "Epoch 499/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4795 - acc: 0.3971 - val_loss: 2.0197 - val_acc: 0.2367\n",
      "Epoch 500/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4802 - acc: 0.3900 - val_loss: 2.0202 - val_acc: 0.2367\n",
      "Epoch 501/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4802 - acc: 0.3943 - val_loss: 2.0235 - val_acc: 0.2300\n",
      "Epoch 502/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4782 - acc: 0.3900 - val_loss: 2.0182 - val_acc: 0.2267\n",
      "Epoch 503/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4787 - acc: 0.4014 - val_loss: 2.0152 - val_acc: 0.2267\n",
      "Epoch 504/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4779 - acc: 0.3943 - val_loss: 2.0224 - val_acc: 0.2300\n",
      "Epoch 505/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4780 - acc: 0.3957 - val_loss: 2.0166 - val_acc: 0.2367\n",
      "Epoch 506/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4776 - acc: 0.3943 - val_loss: 2.0109 - val_acc: 0.2267\n",
      "Epoch 507/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4792 - acc: 0.3943 - val_loss: 2.0168 - val_acc: 0.2233\n",
      "Epoch 508/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4772 - acc: 0.3871 - val_loss: 2.0225 - val_acc: 0.2267\n",
      "Epoch 509/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3886 - val_loss: 2.0352 - val_acc: 0.2333\n",
      "Epoch 510/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4769 - acc: 0.3871 - val_loss: 2.0202 - val_acc: 0.2267\n",
      "Epoch 511/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3957 - val_loss: 2.0127 - val_acc: 0.2233\n",
      "Epoch 512/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4765 - acc: 0.3914 - val_loss: 2.0043 - val_acc: 0.2433\n",
      "Epoch 513/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4753 - acc: 0.3986 - val_loss: 2.0170 - val_acc: 0.2433\n",
      "Epoch 514/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4752 - acc: 0.3943 - val_loss: 2.0251 - val_acc: 0.2267\n",
      "Epoch 515/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4757 - acc: 0.3914 - val_loss: 2.0230 - val_acc: 0.2300\n",
      "Epoch 516/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4768 - acc: 0.3829 - val_loss: 2.0185 - val_acc: 0.2233\n",
      "Epoch 517/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4731 - acc: 0.3943 - val_loss: 2.0331 - val_acc: 0.2533\n",
      "Epoch 518/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4766 - acc: 0.3943 - val_loss: 2.0240 - val_acc: 0.2233\n",
      "Epoch 519/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4750 - acc: 0.4014 - val_loss: 2.0201 - val_acc: 0.2300\n",
      "Epoch 520/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4743 - acc: 0.3900 - val_loss: 2.0280 - val_acc: 0.2333\n",
      "Epoch 521/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4738 - acc: 0.4086 - val_loss: 2.0073 - val_acc: 0.2367\n",
      "Epoch 522/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4752 - acc: 0.3986 - val_loss: 2.0312 - val_acc: 0.2267\n",
      "Epoch 523/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4748 - acc: 0.3957 - val_loss: 2.0270 - val_acc: 0.2267\n",
      "Epoch 524/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4738 - acc: 0.4000 - val_loss: 2.0196 - val_acc: 0.2300\n",
      "Epoch 525/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4722 - acc: 0.3957 - val_loss: 2.0241 - val_acc: 0.2433\n",
      "Epoch 526/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4737 - acc: 0.3943 - val_loss: 2.0255 - val_acc: 0.2300\n",
      "Epoch 527/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4729 - acc: 0.3914 - val_loss: 2.0228 - val_acc: 0.2267\n",
      "Epoch 528/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4720 - acc: 0.3929 - val_loss: 2.0282 - val_acc: 0.2300\n",
      "Epoch 529/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4708 - acc: 0.3857 - val_loss: 2.0275 - val_acc: 0.2300\n",
      "Epoch 530/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4713 - acc: 0.4014 - val_loss: 2.0401 - val_acc: 0.2333\n",
      "Epoch 531/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4716 - acc: 0.3871 - val_loss: 2.0217 - val_acc: 0.2433\n",
      "Epoch 532/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4719 - acc: 0.3971 - val_loss: 2.0278 - val_acc: 0.2433\n",
      "Epoch 533/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4710 - acc: 0.4014 - val_loss: 2.0227 - val_acc: 0.2267\n",
      "Epoch 534/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4718 - acc: 0.3914 - val_loss: 2.0226 - val_acc: 0.2333\n",
      "Epoch 535/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4703 - acc: 0.3957 - val_loss: 2.0436 - val_acc: 0.2367\n",
      "Epoch 536/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3957 - val_loss: 2.0466 - val_acc: 0.2267\n",
      "Epoch 537/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4707 - acc: 0.3971 - val_loss: 2.0249 - val_acc: 0.2300\n",
      "Epoch 538/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4698 - acc: 0.3886 - val_loss: 2.0354 - val_acc: 0.2333\n",
      "Epoch 539/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4722 - acc: 0.3914 - val_loss: 2.0217 - val_acc: 0.2233\n",
      "Epoch 540/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4706 - acc: 0.3943 - val_loss: 2.0359 - val_acc: 0.2267\n",
      "Epoch 541/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4697 - acc: 0.3971 - val_loss: 2.0328 - val_acc: 0.2233\n",
      "Epoch 542/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4696 - acc: 0.4029 - val_loss: 2.0417 - val_acc: 0.2400\n",
      "Epoch 543/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4701 - acc: 0.3957 - val_loss: 2.0313 - val_acc: 0.2333\n",
      "Epoch 544/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4686 - acc: 0.3914 - val_loss: 2.0202 - val_acc: 0.2400\n",
      "Epoch 545/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4695 - acc: 0.3986 - val_loss: 2.0243 - val_acc: 0.2333\n",
      "Epoch 546/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4666 - acc: 0.3971 - val_loss: 2.0412 - val_acc: 0.2367\n",
      "Epoch 547/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4688 - acc: 0.3986 - val_loss: 2.0468 - val_acc: 0.2333\n",
      "Epoch 548/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4674 - acc: 0.3914 - val_loss: 2.0223 - val_acc: 0.2267\n",
      "Epoch 549/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4676 - acc: 0.4014 - val_loss: 2.0210 - val_acc: 0.2267\n",
      "Epoch 550/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4670 - acc: 0.4000 - val_loss: 2.0331 - val_acc: 0.2300\n",
      "Epoch 551/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4677 - acc: 0.3929 - val_loss: 2.0349 - val_acc: 0.2233\n",
      "Epoch 552/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4661 - acc: 0.4029 - val_loss: 2.0297 - val_acc: 0.2300\n",
      "Epoch 553/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4687 - acc: 0.3929 - val_loss: 2.0347 - val_acc: 0.2300\n",
      "Epoch 554/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4658 - acc: 0.3986 - val_loss: 2.0380 - val_acc: 0.2333\n",
      "Epoch 555/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4669 - acc: 0.3929 - val_loss: 2.0400 - val_acc: 0.2333\n",
      "Epoch 556/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4653 - acc: 0.3943 - val_loss: 2.0347 - val_acc: 0.2433\n",
      "Epoch 557/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.4071 - val_loss: 2.0515 - val_acc: 0.2567\n",
      "Epoch 558/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4663 - acc: 0.3986 - val_loss: 2.0345 - val_acc: 0.2300\n",
      "Epoch 559/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4645 - acc: 0.4029 - val_loss: 2.0337 - val_acc: 0.2500\n",
      "Epoch 560/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.4043 - val_loss: 2.0382 - val_acc: 0.2567\n",
      "Epoch 561/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4656 - acc: 0.3971 - val_loss: 2.0286 - val_acc: 0.2300\n",
      "Epoch 562/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4652 - acc: 0.3986 - val_loss: 2.0310 - val_acc: 0.2233\n",
      "Epoch 563/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4644 - acc: 0.3957 - val_loss: 2.0283 - val_acc: 0.2267\n",
      "Epoch 564/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4644 - acc: 0.4100 - val_loss: 2.0442 - val_acc: 0.2267\n",
      "Epoch 565/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4636 - acc: 0.3986 - val_loss: 2.0322 - val_acc: 0.2333\n",
      "Epoch 566/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4655 - acc: 0.4014 - val_loss: 2.0353 - val_acc: 0.2300\n",
      "Epoch 567/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4630 - acc: 0.4114 - val_loss: 2.0475 - val_acc: 0.2233\n",
      "Epoch 568/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.4043 - val_loss: 2.0417 - val_acc: 0.2333\n",
      "Epoch 569/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4625 - acc: 0.4029 - val_loss: 2.0361 - val_acc: 0.2400\n",
      "Epoch 570/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4621 - acc: 0.4014 - val_loss: 2.0447 - val_acc: 0.2367\n",
      "Epoch 571/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4620 - acc: 0.3986 - val_loss: 2.0449 - val_acc: 0.2233\n",
      "Epoch 572/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4612 - acc: 0.3943 - val_loss: 2.0391 - val_acc: 0.2400\n",
      "Epoch 573/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.4071 - val_loss: 2.0503 - val_acc: 0.2467\n",
      "Epoch 574/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.3986 - val_loss: 2.0370 - val_acc: 0.2500\n",
      "Epoch 575/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4632 - acc: 0.4057 - val_loss: 2.0375 - val_acc: 0.2267\n",
      "Epoch 576/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4619 - acc: 0.3943 - val_loss: 2.0413 - val_acc: 0.2233\n",
      "Epoch 577/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4617 - acc: 0.3986 - val_loss: 2.0387 - val_acc: 0.2267\n",
      "Epoch 578/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4612 - acc: 0.4014 - val_loss: 2.0399 - val_acc: 0.2367\n",
      "Epoch 579/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4618 - acc: 0.4043 - val_loss: 2.0366 - val_acc: 0.2267\n",
      "Epoch 580/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4609 - acc: 0.4043 - val_loss: 2.0448 - val_acc: 0.2367\n",
      "Epoch 581/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4603 - acc: 0.3971 - val_loss: 2.0386 - val_acc: 0.2533\n",
      "Epoch 582/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4693 - acc: 0.403 - ETA: 0s - loss: 1.4604 - acc: 0.4029 - val_loss: 2.0494 - val_acc: 0.2300\n",
      "Epoch 583/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4597 - acc: 0.3957 - val_loss: 2.0475 - val_acc: 0.2267\n",
      "Epoch 584/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4607 - acc: 0.4000 - val_loss: 2.0436 - val_acc: 0.2300\n",
      "Epoch 585/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4590 - acc: 0.4071 - val_loss: 2.0515 - val_acc: 0.2367\n",
      "Epoch 586/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4584 - acc: 0.3943 - val_loss: 2.0503 - val_acc: 0.2467\n",
      "Epoch 587/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4607 - acc: 0.4057 - val_loss: 2.0503 - val_acc: 0.2333\n",
      "Epoch 588/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4595 - acc: 0.4000 - val_loss: 2.0521 - val_acc: 0.2333\n",
      "Epoch 589/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4586 - acc: 0.4000 - val_loss: 2.0529 - val_acc: 0.2267\n",
      "Epoch 590/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4589 - acc: 0.3971 - val_loss: 2.0446 - val_acc: 0.2267\n",
      "Epoch 591/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4588 - acc: 0.4043 - val_loss: 2.0546 - val_acc: 0.2300\n",
      "Epoch 592/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4580 - acc: 0.4043 - val_loss: 2.0506 - val_acc: 0.2367\n",
      "Epoch 593/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4580 - acc: 0.4014 - val_loss: 2.0427 - val_acc: 0.2267\n",
      "Epoch 594/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4577 - acc: 0.4157 - val_loss: 2.0429 - val_acc: 0.2333\n",
      "Epoch 595/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4574 - acc: 0.4014 - val_loss: 2.0488 - val_acc: 0.2333\n",
      "Epoch 596/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4572 - acc: 0.4057 - val_loss: 2.0385 - val_acc: 0.2300\n",
      "Epoch 597/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4071 - val_loss: 2.0434 - val_acc: 0.2500\n",
      "Epoch 598/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4572 - acc: 0.3957 - val_loss: 2.0433 - val_acc: 0.2267\n",
      "Epoch 599/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4579 - acc: 0.4000 - val_loss: 2.0419 - val_acc: 0.2367\n",
      "Epoch 600/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4562 - acc: 0.4086 - val_loss: 2.0568 - val_acc: 0.2300\n",
      "Epoch 601/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4559 - acc: 0.4157 - val_loss: 2.0499 - val_acc: 0.2333\n",
      "Epoch 602/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4561 - acc: 0.3971 - val_loss: 2.0484 - val_acc: 0.2367\n",
      "Epoch 603/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4565 - acc: 0.4086 - val_loss: 2.0427 - val_acc: 0.2400\n",
      "Epoch 604/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4563 - acc: 0.4000 - val_loss: 2.0416 - val_acc: 0.2400\n",
      "Epoch 605/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4550 - acc: 0.4000 - val_loss: 2.0484 - val_acc: 0.2367\n",
      "Epoch 606/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4554 - acc: 0.4057 - val_loss: 2.0413 - val_acc: 0.2333\n",
      "Epoch 607/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4555 - acc: 0.4100 - val_loss: 2.0490 - val_acc: 0.2300\n",
      "Epoch 608/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4551 - acc: 0.4100 - val_loss: 2.0411 - val_acc: 0.2400\n",
      "Epoch 609/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4535 - acc: 0.4057 - val_loss: 2.0571 - val_acc: 0.2567\n",
      "Epoch 610/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4545 - acc: 0.4086 - val_loss: 2.0570 - val_acc: 0.2300\n",
      "Epoch 611/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4545 - acc: 0.4043 - val_loss: 2.0583 - val_acc: 0.2300\n",
      "Epoch 612/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4544 - acc: 0.3986 - val_loss: 2.0528 - val_acc: 0.2400\n",
      "Epoch 613/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4541 - acc: 0.4086 - val_loss: 2.0616 - val_acc: 0.2267\n",
      "Epoch 614/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4532 - acc: 0.4100 - val_loss: 2.0595 - val_acc: 0.2433\n",
      "Epoch 615/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4539 - acc: 0.4043 - val_loss: 2.0488 - val_acc: 0.2467\n",
      "Epoch 616/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4534 - acc: 0.4143 - val_loss: 2.0651 - val_acc: 0.2267\n",
      "Epoch 617/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4514 - acc: 0.4071 - val_loss: 2.0598 - val_acc: 0.2567\n",
      "Epoch 618/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4529 - acc: 0.4029 - val_loss: 2.0543 - val_acc: 0.2233\n",
      "Epoch 619/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4517 - acc: 0.4129 - val_loss: 2.0661 - val_acc: 0.2367\n",
      "Epoch 620/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4523 - acc: 0.4086 - val_loss: 2.0516 - val_acc: 0.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 621/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4525 - acc: 0.4143 - val_loss: 2.0674 - val_acc: 0.2367\n",
      "Epoch 622/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4086 - val_loss: 2.0655 - val_acc: 0.2367\n",
      "Epoch 623/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4129 - val_loss: 2.0613 - val_acc: 0.2300\n",
      "Epoch 624/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4518 - acc: 0.4143 - val_loss: 2.0609 - val_acc: 0.2333\n",
      "Epoch 625/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4057 - val_loss: 2.0651 - val_acc: 0.2433\n",
      "Epoch 626/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4043 - val_loss: 2.0452 - val_acc: 0.2333\n",
      "Epoch 627/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4516 - acc: 0.4186 - val_loss: 2.0543 - val_acc: 0.2333\n",
      "Epoch 628/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4496 - acc: 0.4057 - val_loss: 2.0459 - val_acc: 0.2400\n",
      "Epoch 629/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4513 - acc: 0.3986 - val_loss: 2.0618 - val_acc: 0.2367\n",
      "Epoch 630/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4100 - val_loss: 2.0483 - val_acc: 0.2333\n",
      "Epoch 631/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4507 - acc: 0.4114 - val_loss: 2.0743 - val_acc: 0.2400\n",
      "Epoch 632/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4508 - acc: 0.4057 - val_loss: 2.0652 - val_acc: 0.2367\n",
      "Epoch 633/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4495 - acc: 0.4143 - val_loss: 2.0756 - val_acc: 0.2400\n",
      "Epoch 634/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4497 - acc: 0.4114 - val_loss: 2.0604 - val_acc: 0.2333\n",
      "Epoch 635/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4482 - acc: 0.3986 - val_loss: 2.0581 - val_acc: 0.2467\n",
      "Epoch 636/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4504 - acc: 0.4129 - val_loss: 2.0620 - val_acc: 0.2333\n",
      "Epoch 637/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4491 - acc: 0.4057 - val_loss: 2.0609 - val_acc: 0.2333\n",
      "Epoch 638/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4492 - acc: 0.4143 - val_loss: 2.0728 - val_acc: 0.2300\n",
      "Epoch 639/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4489 - acc: 0.4086 - val_loss: 2.0725 - val_acc: 0.2300\n",
      "Epoch 640/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4478 - acc: 0.4114 - val_loss: 2.0509 - val_acc: 0.2333\n",
      "Epoch 641/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4477 - acc: 0.4114 - val_loss: 2.0672 - val_acc: 0.2400\n",
      "Epoch 642/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4490 - acc: 0.4086 - val_loss: 2.0748 - val_acc: 0.2400\n",
      "Epoch 643/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4478 - acc: 0.4071 - val_loss: 2.0770 - val_acc: 0.2333\n",
      "Epoch 644/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4479 - acc: 0.4086 - val_loss: 2.0658 - val_acc: 0.2367\n",
      "Epoch 645/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4114 - val_loss: 2.0728 - val_acc: 0.2367\n",
      "Epoch 646/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4452 - acc: 0.4086 - val_loss: 2.0634 - val_acc: 0.2600\n",
      "Epoch 647/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4464 - acc: 0.4157 - val_loss: 2.0638 - val_acc: 0.2333\n",
      "Epoch 648/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4471 - acc: 0.4086 - val_loss: 2.0773 - val_acc: 0.2367\n",
      "Epoch 649/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4454 - acc: 0.4014 - val_loss: 2.0789 - val_acc: 0.2367\n",
      "Epoch 650/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4472 - acc: 0.4114 - val_loss: 2.0648 - val_acc: 0.2367\n",
      "Epoch 651/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4459 - acc: 0.4071 - val_loss: 2.0843 - val_acc: 0.2433\n",
      "Epoch 652/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4468 - acc: 0.4114 - val_loss: 2.0711 - val_acc: 0.2333\n",
      "Epoch 653/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4454 - acc: 0.4129 - val_loss: 2.0696 - val_acc: 0.2467\n",
      "Epoch 654/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4456 - acc: 0.4100 - val_loss: 2.0582 - val_acc: 0.2467\n",
      "Epoch 655/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4445 - acc: 0.4100 - val_loss: 2.0686 - val_acc: 0.2467\n",
      "Epoch 656/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4467 - acc: 0.4086 - val_loss: 2.0808 - val_acc: 0.2400\n",
      "Epoch 657/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4450 - acc: 0.4086 - val_loss: 2.0705 - val_acc: 0.2300\n",
      "Epoch 658/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4449 - acc: 0.4100 - val_loss: 2.0698 - val_acc: 0.2400\n",
      "Epoch 659/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4129 - val_loss: 2.0711 - val_acc: 0.2433\n",
      "Epoch 660/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4449 - acc: 0.4057 - val_loss: 2.0698 - val_acc: 0.2333\n",
      "Epoch 661/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4459 - acc: 0.4100 - val_loss: 2.0678 - val_acc: 0.2300\n",
      "Epoch 662/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4451 - acc: 0.4043 - val_loss: 2.0718 - val_acc: 0.2300\n",
      "Epoch 663/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4444 - acc: 0.4114 - val_loss: 2.0855 - val_acc: 0.2367\n",
      "Epoch 664/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4435 - acc: 0.4129 - val_loss: 2.0739 - val_acc: 0.2400\n",
      "Epoch 665/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4438 - acc: 0.4129 - val_loss: 2.0808 - val_acc: 0.2400\n",
      "Epoch 666/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4430 - acc: 0.4129 - val_loss: 2.0707 - val_acc: 0.2467\n",
      "Epoch 667/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4446 - acc: 0.4100 - val_loss: 2.0661 - val_acc: 0.2333\n",
      "Epoch 668/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4414 - acc: 0.4100 - val_loss: 2.0832 - val_acc: 0.2600\n",
      "Epoch 669/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4422 - acc: 0.4171 - val_loss: 2.0848 - val_acc: 0.2400\n",
      "Epoch 670/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4436 - acc: 0.4143 - val_loss: 2.0738 - val_acc: 0.2333\n",
      "Epoch 671/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4419 - acc: 0.4143 - val_loss: 2.0713 - val_acc: 0.2467\n",
      "Epoch 672/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4424 - acc: 0.4071 - val_loss: 2.0780 - val_acc: 0.2333\n",
      "Epoch 673/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4427 - acc: 0.4114 - val_loss: 2.0745 - val_acc: 0.2400\n",
      "Epoch 674/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4417 - acc: 0.4057 - val_loss: 2.0969 - val_acc: 0.2400\n",
      "Epoch 675/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4409 - acc: 0.4157 - val_loss: 2.0940 - val_acc: 0.2567\n",
      "Epoch 676/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4408 - acc: 0.4100 - val_loss: 2.0668 - val_acc: 0.2567\n",
      "Epoch 677/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4401 - acc: 0.4114 - val_loss: 2.0927 - val_acc: 0.2367\n",
      "Epoch 678/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4396 - acc: 0.4171 - val_loss: 2.0682 - val_acc: 0.2300\n",
      "Epoch 679/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4412 - acc: 0.4143 - val_loss: 2.0835 - val_acc: 0.2400\n",
      "Epoch 680/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4421 - acc: 0.4143 - val_loss: 2.0821 - val_acc: 0.2367\n",
      "Epoch 681/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4412 - acc: 0.4157 - val_loss: 2.0804 - val_acc: 0.2333\n",
      "Epoch 682/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4404 - acc: 0.4143 - val_loss: 2.0784 - val_acc: 0.2567\n",
      "Epoch 683/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4404 - acc: 0.4043 - val_loss: 2.0680 - val_acc: 0.2500\n",
      "Epoch 684/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4367 - acc: 0.4129 - val_loss: 2.0819 - val_acc: 0.2567\n",
      "Epoch 685/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4398 - acc: 0.4100 - val_loss: 2.0775 - val_acc: 0.2500\n",
      "Epoch 686/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4396 - acc: 0.4143 - val_loss: 2.0973 - val_acc: 0.2533\n",
      "Epoch 687/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4393 - acc: 0.4171 - val_loss: 2.0704 - val_acc: 0.2367\n",
      "Epoch 688/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4398 - acc: 0.4171 - val_loss: 2.0791 - val_acc: 0.2333\n",
      "Epoch 689/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4392 - acc: 0.4129 - val_loss: 2.0836 - val_acc: 0.2433\n",
      "Epoch 690/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4389 - acc: 0.4171 - val_loss: 2.0878 - val_acc: 0.2333\n",
      "Epoch 691/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4378 - acc: 0.4200 - val_loss: 2.0907 - val_acc: 0.2367\n",
      "Epoch 692/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4382 - acc: 0.4157 - val_loss: 2.0746 - val_acc: 0.2333\n",
      "Epoch 693/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4388 - acc: 0.4100 - val_loss: 2.0776 - val_acc: 0.2367\n",
      "Epoch 694/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4377 - acc: 0.4129 - val_loss: 2.1024 - val_acc: 0.2467\n",
      "Epoch 695/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4391 - acc: 0.4100 - val_loss: 2.0802 - val_acc: 0.2533\n",
      "Epoch 696/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4157 - val_loss: 2.0920 - val_acc: 0.2400\n",
      "Epoch 697/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4379 - acc: 0.4143 - val_loss: 2.0964 - val_acc: 0.2500\n",
      "Epoch 698/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4377 - acc: 0.4157 - val_loss: 2.0883 - val_acc: 0.2467\n",
      "Epoch 699/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4372 - acc: 0.4186 - val_loss: 2.0880 - val_acc: 0.2333\n",
      "Epoch 700/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4368 - acc: 0.4129 - val_loss: 2.0865 - val_acc: 0.2500\n",
      "Epoch 701/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4360 - acc: 0.4171 - val_loss: 2.0820 - val_acc: 0.2500\n",
      "Epoch 702/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4369 - acc: 0.4171 - val_loss: 2.0828 - val_acc: 0.2400\n",
      "Epoch 703/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4214 - val_loss: 2.1027 - val_acc: 0.2367\n",
      "Epoch 704/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4367 - acc: 0.4186 - val_loss: 2.0823 - val_acc: 0.2467\n",
      "Epoch 705/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4365 - acc: 0.4100 - val_loss: 2.0784 - val_acc: 0.2367\n",
      "Epoch 706/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4364 - acc: 0.4200 - val_loss: 2.0721 - val_acc: 0.2400\n",
      "Epoch 707/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4358 - acc: 0.4214 - val_loss: 2.0939 - val_acc: 0.2367\n",
      "Epoch 708/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4360 - acc: 0.4186 - val_loss: 2.0844 - val_acc: 0.2367\n",
      "Epoch 709/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4340 - acc: 0.4229 - val_loss: 2.1026 - val_acc: 0.2533\n",
      "Epoch 710/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4357 - acc: 0.4186 - val_loss: 2.0836 - val_acc: 0.2433\n",
      "Epoch 711/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4347 - acc: 0.4114 - val_loss: 2.0938 - val_acc: 0.2367\n",
      "Epoch 712/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4344 - acc: 0.4114 - val_loss: 2.0918 - val_acc: 0.2567\n",
      "Epoch 713/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4352 - acc: 0.4171 - val_loss: 2.0749 - val_acc: 0.2333\n",
      "Epoch 714/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4344 - acc: 0.4214 - val_loss: 2.0923 - val_acc: 0.2400\n",
      "Epoch 715/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4320 - acc: 0.4200 - val_loss: 2.0827 - val_acc: 0.2533\n",
      "Epoch 716/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4345 - acc: 0.4157 - val_loss: 2.0932 - val_acc: 0.2400\n",
      "Epoch 717/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4338 - acc: 0.4186 - val_loss: 2.0947 - val_acc: 0.2333\n",
      "Epoch 718/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4327 - acc: 0.4186 - val_loss: 2.1133 - val_acc: 0.2533\n",
      "Epoch 719/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4342 - acc: 0.4186 - val_loss: 2.1033 - val_acc: 0.2367\n",
      "Epoch 720/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4337 - acc: 0.4171 - val_loss: 2.0932 - val_acc: 0.2433\n",
      "Epoch 721/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4331 - acc: 0.4214 - val_loss: 2.0920 - val_acc: 0.2433\n",
      "Epoch 722/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4324 - acc: 0.4229 - val_loss: 2.1012 - val_acc: 0.2400\n",
      "Epoch 723/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4324 - acc: 0.4214 - val_loss: 2.0889 - val_acc: 0.2333\n",
      "Epoch 724/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4322 - acc: 0.4186 - val_loss: 2.1015 - val_acc: 0.2367\n",
      "Epoch 725/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4318 - acc: 0.4157 - val_loss: 2.1054 - val_acc: 0.2400\n",
      "Epoch 726/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4316 - acc: 0.4186 - val_loss: 2.0872 - val_acc: 0.2333\n",
      "Epoch 727/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4313 - acc: 0.4214 - val_loss: 2.1174 - val_acc: 0.2400\n",
      "Epoch 728/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4314 - acc: 0.4186 - val_loss: 2.0859 - val_acc: 0.2367\n",
      "Epoch 729/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4309 - acc: 0.4243 - val_loss: 2.1088 - val_acc: 0.2533\n",
      "Epoch 730/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4309 - acc: 0.4257 - val_loss: 2.1148 - val_acc: 0.2367\n",
      "Epoch 731/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4315 - acc: 0.4186 - val_loss: 2.0992 - val_acc: 0.2367\n",
      "Epoch 732/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4296 - acc: 0.4243 - val_loss: 2.0967 - val_acc: 0.2367\n",
      "Epoch 733/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4303 - acc: 0.4257 - val_loss: 2.1015 - val_acc: 0.2400\n",
      "Epoch 734/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4310 - acc: 0.4171 - val_loss: 2.1101 - val_acc: 0.2367\n",
      "Epoch 735/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4299 - acc: 0.4200 - val_loss: 2.1013 - val_acc: 0.2367\n",
      "Epoch 736/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4288 - acc: 0.4229 - val_loss: 2.0933 - val_acc: 0.2400\n",
      "Epoch 737/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4304 - acc: 0.4229 - val_loss: 2.1097 - val_acc: 0.2400\n",
      "Epoch 738/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4304 - acc: 0.4214 - val_loss: 2.1003 - val_acc: 0.2433\n",
      "Epoch 739/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4289 - acc: 0.4214 - val_loss: 2.1058 - val_acc: 0.2400\n",
      "Epoch 740/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4292 - acc: 0.4257 - val_loss: 2.1001 - val_acc: 0.2433\n",
      "Epoch 741/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4286 - acc: 0.4214 - val_loss: 2.1102 - val_acc: 0.2567\n",
      "Epoch 742/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4288 - acc: 0.4129 - val_loss: 2.0927 - val_acc: 0.2500\n",
      "Epoch 743/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4276 - acc: 0.4243 - val_loss: 2.0944 - val_acc: 0.2567\n",
      "Epoch 744/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4295 - acc: 0.4229 - val_loss: 2.0944 - val_acc: 0.2400\n",
      "Epoch 745/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4260 - acc: 0.4343 - val_loss: 2.1075 - val_acc: 0.2600\n",
      "Epoch 746/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4291 - acc: 0.4257 - val_loss: 2.1002 - val_acc: 0.2400\n",
      "Epoch 747/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4288 - acc: 0.4171 - val_loss: 2.1119 - val_acc: 0.2433\n",
      "Epoch 748/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4268 - acc: 0.4186 - val_loss: 2.1170 - val_acc: 0.2567\n",
      "Epoch 749/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4259 - acc: 0.4214 - val_loss: 2.0881 - val_acc: 0.2367\n",
      "Epoch 750/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4279 - acc: 0.4286 - val_loss: 2.0938 - val_acc: 0.2367\n",
      "Epoch 751/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4262 - acc: 0.4157 - val_loss: 2.1048 - val_acc: 0.2400\n",
      "Epoch 752/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4270 - acc: 0.4257 - val_loss: 2.1052 - val_acc: 0.2367\n",
      "Epoch 753/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4260 - acc: 0.4257 - val_loss: 2.1265 - val_acc: 0.2433\n",
      "Epoch 754/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4274 - acc: 0.4214 - val_loss: 2.1113 - val_acc: 0.2367\n",
      "Epoch 755/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4260 - acc: 0.4286 - val_loss: 2.1110 - val_acc: 0.2500\n",
      "Epoch 756/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4259 - acc: 0.4129 - val_loss: 2.1063 - val_acc: 0.2400\n",
      "Epoch 757/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4271 - acc: 0.4157 - val_loss: 2.1121 - val_acc: 0.2467\n",
      "Epoch 758/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4257 - acc: 0.4243 - val_loss: 2.1074 - val_acc: 0.2500\n",
      "Epoch 759/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4259 - acc: 0.4271 - val_loss: 2.1036 - val_acc: 0.2400\n",
      "Epoch 760/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4255 - acc: 0.4300 - val_loss: 2.1089 - val_acc: 0.2400\n",
      "Epoch 761/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4255 - acc: 0.4186 - val_loss: 2.1252 - val_acc: 0.2467\n",
      "Epoch 762/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4262 - acc: 0.4257 - val_loss: 2.1160 - val_acc: 0.2433\n",
      "Epoch 763/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4255 - acc: 0.4214 - val_loss: 2.1017 - val_acc: 0.2367\n",
      "Epoch 764/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4261 - acc: 0.4271 - val_loss: 2.1202 - val_acc: 0.2467\n",
      "Epoch 765/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4240 - acc: 0.4143 - val_loss: 2.1317 - val_acc: 0.2367\n",
      "Epoch 766/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4255 - acc: 0.4171 - val_loss: 2.1043 - val_acc: 0.2400\n",
      "Epoch 767/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4254 - acc: 0.4257 - val_loss: 2.1081 - val_acc: 0.2400\n",
      "Epoch 768/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4300 - val_loss: 2.0999 - val_acc: 0.2400\n",
      "Epoch 769/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4242 - acc: 0.4229 - val_loss: 2.1074 - val_acc: 0.2367\n",
      "Epoch 770/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4240 - acc: 0.4229 - val_loss: 2.1007 - val_acc: 0.2367\n",
      "Epoch 771/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4232 - acc: 0.4171 - val_loss: 2.1189 - val_acc: 0.2400\n",
      "Epoch 772/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4239 - acc: 0.4200 - val_loss: 2.1173 - val_acc: 0.2367\n",
      "Epoch 773/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4238 - acc: 0.4171 - val_loss: 2.1257 - val_acc: 0.2500\n",
      "Epoch 774/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4227 - acc: 0.4200 - val_loss: 2.1185 - val_acc: 0.2433\n",
      "Epoch 775/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4218 - acc: 0.4300 - val_loss: 2.1199 - val_acc: 0.2533\n",
      "Epoch 776/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4228 - acc: 0.4229 - val_loss: 2.1187 - val_acc: 0.2433\n",
      "Epoch 777/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4217 - acc: 0.4229 - val_loss: 2.1285 - val_acc: 0.2400\n",
      "Epoch 778/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4228 - acc: 0.4186 - val_loss: 2.1155 - val_acc: 0.2400\n",
      "Epoch 779/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4231 - acc: 0.4271 - val_loss: 2.1249 - val_acc: 0.2367\n",
      "Epoch 780/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4227 - acc: 0.4186 - val_loss: 2.1071 - val_acc: 0.2400\n",
      "Epoch 781/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4234 - acc: 0.4200 - val_loss: 2.1124 - val_acc: 0.2467\n",
      "Epoch 782/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4222 - acc: 0.4200 - val_loss: 2.0973 - val_acc: 0.2400\n",
      "Epoch 783/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4224 - acc: 0.4271 - val_loss: 2.1160 - val_acc: 0.2433\n",
      "Epoch 784/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4219 - acc: 0.4271 - val_loss: 2.1201 - val_acc: 0.2433\n",
      "Epoch 785/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4217 - acc: 0.4271 - val_loss: 2.1295 - val_acc: 0.2400\n",
      "Epoch 786/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4216 - acc: 0.4243 - val_loss: 2.1169 - val_acc: 0.2500\n",
      "Epoch 787/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4221 - acc: 0.4257 - val_loss: 2.1035 - val_acc: 0.2400\n",
      "Epoch 788/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4214 - acc: 0.4257 - val_loss: 2.1152 - val_acc: 0.2400\n",
      "Epoch 789/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4210 - acc: 0.4229 - val_loss: 2.1111 - val_acc: 0.2400\n",
      "Epoch 790/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4213 - acc: 0.4286 - val_loss: 2.1179 - val_acc: 0.2433\n",
      "Epoch 791/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4203 - acc: 0.4200 - val_loss: 2.1192 - val_acc: 0.2500\n",
      "Epoch 792/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4208 - acc: 0.4200 - val_loss: 2.1082 - val_acc: 0.2400\n",
      "Epoch 793/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4201 - acc: 0.4286 - val_loss: 2.1258 - val_acc: 0.2500\n",
      "Epoch 794/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4206 - acc: 0.4229 - val_loss: 2.1245 - val_acc: 0.2433\n",
      "Epoch 795/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4204 - acc: 0.4257 - val_loss: 2.1180 - val_acc: 0.2367\n",
      "Epoch 796/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4197 - acc: 0.4243 - val_loss: 2.1186 - val_acc: 0.2367\n",
      "Epoch 797/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4200 - acc: 0.4214 - val_loss: 2.1248 - val_acc: 0.2367\n",
      "Epoch 798/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4197 - acc: 0.4314 - val_loss: 2.1255 - val_acc: 0.2400\n",
      "Epoch 799/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4187 - acc: 0.4271 - val_loss: 2.1189 - val_acc: 0.2600\n",
      "Epoch 800/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4197 - acc: 0.4200 - val_loss: 2.1256 - val_acc: 0.2400\n",
      "Epoch 801/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4198 - acc: 0.4257 - val_loss: 2.1241 - val_acc: 0.2400\n",
      "Epoch 802/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4199 - acc: 0.4286 - val_loss: 2.1558 - val_acc: 0.2400\n",
      "Epoch 803/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4181 - acc: 0.4286 - val_loss: 2.1184 - val_acc: 0.2500\n",
      "Epoch 804/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4184 - acc: 0.4257 - val_loss: 2.1198 - val_acc: 0.2433\n",
      "Epoch 805/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4179 - acc: 0.4286 - val_loss: 2.1148 - val_acc: 0.2500\n",
      "Epoch 806/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4186 - acc: 0.4286 - val_loss: 2.1186 - val_acc: 0.2433\n",
      "Epoch 807/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4187 - acc: 0.4314 - val_loss: 2.1214 - val_acc: 0.2400\n",
      "Epoch 808/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4180 - acc: 0.4214 - val_loss: 2.1177 - val_acc: 0.2467\n",
      "Epoch 809/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4188 - acc: 0.4200 - val_loss: 2.1297 - val_acc: 0.2467\n",
      "Epoch 810/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4175 - acc: 0.4314 - val_loss: 2.1294 - val_acc: 0.2433\n",
      "Epoch 811/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4179 - acc: 0.4286 - val_loss: 2.1248 - val_acc: 0.2467\n",
      "Epoch 812/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4177 - acc: 0.4257 - val_loss: 2.1166 - val_acc: 0.2433\n",
      "Epoch 813/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4178 - acc: 0.4329 - val_loss: 2.1310 - val_acc: 0.2367\n",
      "Epoch 814/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4164 - acc: 0.4243 - val_loss: 2.1227 - val_acc: 0.2600\n",
      "Epoch 815/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4177 - acc: 0.4243 - val_loss: 2.1263 - val_acc: 0.2467\n",
      "Epoch 816/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4170 - acc: 0.4343 - val_loss: 2.1190 - val_acc: 0.2467\n",
      "Epoch 817/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4155 - acc: 0.4229 - val_loss: 2.1240 - val_acc: 0.2400\n",
      "Epoch 818/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4160 - acc: 0.4271 - val_loss: 2.1342 - val_acc: 0.2400\n",
      "Epoch 819/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4386 - val_loss: 2.1382 - val_acc: 0.2633\n",
      "Epoch 820/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4167 - acc: 0.4300 - val_loss: 2.1306 - val_acc: 0.2600\n",
      "Epoch 821/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4169 - acc: 0.4229 - val_loss: 2.1414 - val_acc: 0.2433\n",
      "Epoch 822/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4167 - acc: 0.4271 - val_loss: 2.1266 - val_acc: 0.2467\n",
      "Epoch 823/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4153 - acc: 0.4271 - val_loss: 2.1362 - val_acc: 0.2400\n",
      "Epoch 824/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4146 - acc: 0.4257 - val_loss: 2.1334 - val_acc: 0.2467\n",
      "Epoch 825/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4149 - acc: 0.4329 - val_loss: 2.1444 - val_acc: 0.2467\n",
      "Epoch 826/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4159 - acc: 0.4257 - val_loss: 2.1380 - val_acc: 0.2467\n",
      "Epoch 827/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4147 - acc: 0.4300 - val_loss: 2.1395 - val_acc: 0.2400\n",
      "Epoch 828/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4154 - acc: 0.4343 - val_loss: 2.1276 - val_acc: 0.2400\n",
      "Epoch 829/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4146 - acc: 0.4271 - val_loss: 2.1346 - val_acc: 0.2433\n",
      "Epoch 830/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4142 - acc: 0.4314 - val_loss: 2.1349 - val_acc: 0.2433\n",
      "Epoch 831/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4140 - acc: 0.4386 - val_loss: 2.1461 - val_acc: 0.2467\n",
      "Epoch 832/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4149 - acc: 0.4329 - val_loss: 2.1492 - val_acc: 0.2400\n",
      "Epoch 833/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4271 - val_loss: 2.1547 - val_acc: 0.2333\n",
      "Epoch 834/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4145 - acc: 0.4300 - val_loss: 2.1616 - val_acc: 0.2400\n",
      "Epoch 835/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4134 - acc: 0.4257 - val_loss: 2.1323 - val_acc: 0.2400\n",
      "Epoch 836/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4133 - acc: 0.4314 - val_loss: 2.1379 - val_acc: 0.2467\n",
      "Epoch 837/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4139 - acc: 0.4257 - val_loss: 2.1276 - val_acc: 0.2367\n",
      "Epoch 838/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4144 - acc: 0.4257 - val_loss: 2.1405 - val_acc: 0.2500\n",
      "Epoch 839/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4124 - acc: 0.4314 - val_loss: 2.1353 - val_acc: 0.2367\n",
      "Epoch 840/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4118 - acc: 0.4357 - val_loss: 2.1274 - val_acc: 0.2500\n",
      "Epoch 841/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4131 - acc: 0.4329 - val_loss: 2.1362 - val_acc: 0.2433\n",
      "Epoch 842/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4135 - acc: 0.4329 - val_loss: 2.1501 - val_acc: 0.2467\n",
      "Epoch 843/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4129 - acc: 0.4300 - val_loss: 2.1349 - val_acc: 0.2400\n",
      "Epoch 844/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4119 - acc: 0.4286 - val_loss: 2.1498 - val_acc: 0.2433\n",
      "Epoch 845/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4286 - val_loss: 2.1328 - val_acc: 0.2467\n",
      "Epoch 846/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4127 - acc: 0.4286 - val_loss: 2.1373 - val_acc: 0.2433\n",
      "Epoch 847/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.4271 - val_loss: 2.1424 - val_acc: 0.2433\n",
      "Epoch 848/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4115 - acc: 0.4329 - val_loss: 2.1437 - val_acc: 0.2600\n",
      "Epoch 849/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4121 - acc: 0.4300 - val_loss: 2.1431 - val_acc: 0.2467\n",
      "Epoch 850/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4107 - acc: 0.4257 - val_loss: 2.1376 - val_acc: 0.2500\n",
      "Epoch 851/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4109 - acc: 0.4171 - val_loss: 2.1350 - val_acc: 0.2400\n",
      "Epoch 852/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4123 - acc: 0.4329 - val_loss: 2.1357 - val_acc: 0.2400\n",
      "Epoch 853/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4101 - acc: 0.4271 - val_loss: 2.1323 - val_acc: 0.2367\n",
      "Epoch 854/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4084 - acc: 0.4343 - val_loss: 2.1593 - val_acc: 0.2633\n",
      "Epoch 855/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4108 - acc: 0.4286 - val_loss: 2.1358 - val_acc: 0.2433\n",
      "Epoch 856/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4112 - acc: 0.4314 - val_loss: 2.1542 - val_acc: 0.2467\n",
      "Epoch 857/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4107 - acc: 0.4286 - val_loss: 2.1483 - val_acc: 0.2500\n",
      "Epoch 858/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4104 - acc: 0.4286 - val_loss: 2.1435 - val_acc: 0.2400\n",
      "Epoch 859/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4088 - acc: 0.4357 - val_loss: 2.1443 - val_acc: 0.2367\n",
      "Epoch 860/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4095 - acc: 0.4300 - val_loss: 2.1614 - val_acc: 0.2500\n",
      "Epoch 861/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4100 - acc: 0.4300 - val_loss: 2.1550 - val_acc: 0.2533\n",
      "Epoch 862/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4110 - acc: 0.4214 - val_loss: 2.1554 - val_acc: 0.2467\n",
      "Epoch 863/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4329 - val_loss: 2.1597 - val_acc: 0.2533\n",
      "Epoch 864/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4090 - acc: 0.4314 - val_loss: 2.1516 - val_acc: 0.2533\n",
      "Epoch 865/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4083 - acc: 0.4371 - val_loss: 2.1579 - val_acc: 0.2667\n",
      "Epoch 866/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4086 - acc: 0.4329 - val_loss: 2.1247 - val_acc: 0.2467\n",
      "Epoch 867/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4089 - acc: 0.4329 - val_loss: 2.1553 - val_acc: 0.2400\n",
      "Epoch 868/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4093 - acc: 0.4300 - val_loss: 2.1494 - val_acc: 0.2467\n",
      "Epoch 869/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4087 - acc: 0.4329 - val_loss: 2.1464 - val_acc: 0.2400\n",
      "Epoch 870/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4069 - acc: 0.4343 - val_loss: 2.1605 - val_acc: 0.2467\n",
      "Epoch 871/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4085 - acc: 0.4300 - val_loss: 2.1650 - val_acc: 0.2433\n",
      "Epoch 872/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4074 - acc: 0.4314 - val_loss: 2.1476 - val_acc: 0.2633\n",
      "Epoch 873/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4080 - acc: 0.4286 - val_loss: 2.1295 - val_acc: 0.2367\n",
      "Epoch 874/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4085 - acc: 0.4300 - val_loss: 2.1407 - val_acc: 0.2400\n",
      "Epoch 875/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4076 - acc: 0.4343 - val_loss: 2.1613 - val_acc: 0.2467\n",
      "Epoch 876/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4074 - acc: 0.4314 - val_loss: 2.1381 - val_acc: 0.2500\n",
      "Epoch 877/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4070 - acc: 0.4314 - val_loss: 2.1603 - val_acc: 0.2433\n",
      "Epoch 878/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4082 - acc: 0.4271 - val_loss: 2.1561 - val_acc: 0.2433\n",
      "Epoch 879/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4048 - acc: 0.4357 - val_loss: 2.1575 - val_acc: 0.2367\n",
      "Epoch 880/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4089 - acc: 0.4300 - val_loss: 2.1449 - val_acc: 0.2367\n",
      "Epoch 881/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4068 - acc: 0.4314 - val_loss: 2.1746 - val_acc: 0.2467\n",
      "Epoch 882/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4072 - acc: 0.4343 - val_loss: 2.1603 - val_acc: 0.2533\n",
      "Epoch 883/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4081 - acc: 0.4214 - val_loss: 2.1380 - val_acc: 0.2433\n",
      "Epoch 884/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4314 - val_loss: 2.1670 - val_acc: 0.2633\n",
      "Epoch 885/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4068 - acc: 0.4286 - val_loss: 2.1327 - val_acc: 0.2433\n",
      "Epoch 886/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4073 - acc: 0.4343 - val_loss: 2.1542 - val_acc: 0.2533\n",
      "Epoch 887/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4063 - acc: 0.4329 - val_loss: 2.1492 - val_acc: 0.2500\n",
      "Epoch 888/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4063 - acc: 0.4329 - val_loss: 2.1544 - val_acc: 0.2467\n",
      "Epoch 889/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4047 - acc: 0.4371 - val_loss: 2.1682 - val_acc: 0.2600\n",
      "Epoch 890/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4065 - acc: 0.4329 - val_loss: 2.1514 - val_acc: 0.2600\n",
      "Epoch 891/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4061 - acc: 0.4286 - val_loss: 2.1602 - val_acc: 0.2533\n",
      "Epoch 892/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4061 - acc: 0.4371 - val_loss: 2.1628 - val_acc: 0.2533\n",
      "Epoch 893/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4055 - acc: 0.4329 - val_loss: 2.1670 - val_acc: 0.2533\n",
      "Epoch 894/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4058 - acc: 0.4386 - val_loss: 2.1545 - val_acc: 0.2433\n",
      "Epoch 895/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4050 - acc: 0.4257 - val_loss: 2.1669 - val_acc: 0.2533\n",
      "Epoch 896/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4329 - val_loss: 2.1616 - val_acc: 0.2667\n",
      "Epoch 897/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4055 - acc: 0.4300 - val_loss: 2.1669 - val_acc: 0.2600\n",
      "Epoch 898/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4371 - val_loss: 2.1717 - val_acc: 0.2500\n",
      "Epoch 899/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4049 - acc: 0.4300 - val_loss: 2.1500 - val_acc: 0.2500\n",
      "Epoch 900/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4038 - acc: 0.4414 - val_loss: 2.1546 - val_acc: 0.2533\n",
      "Epoch 901/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4343 - val_loss: 2.1698 - val_acc: 0.2667\n",
      "Epoch 902/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4343 - val_loss: 2.1597 - val_acc: 0.2533\n",
      "Epoch 903/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4029 - acc: 0.4343 - val_loss: 2.1715 - val_acc: 0.2600\n",
      "Epoch 904/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4343 - val_loss: 2.1637 - val_acc: 0.2367\n",
      "Epoch 905/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4047 - acc: 0.4400 - val_loss: 2.1484 - val_acc: 0.2433\n",
      "Epoch 906/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4329 - val_loss: 2.1472 - val_acc: 0.2433\n",
      "Epoch 907/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4040 - acc: 0.4343 - val_loss: 2.1700 - val_acc: 0.2533\n",
      "Epoch 908/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4036 - acc: 0.4314 - val_loss: 2.1608 - val_acc: 0.2467\n",
      "Epoch 909/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4032 - acc: 0.4329 - val_loss: 2.1678 - val_acc: 0.2533\n",
      "Epoch 910/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.4343 - val_loss: 2.1495 - val_acc: 0.2467\n",
      "Epoch 911/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4028 - acc: 0.4357 - val_loss: 2.1686 - val_acc: 0.2433\n",
      "Epoch 912/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4032 - acc: 0.4357 - val_loss: 2.1570 - val_acc: 0.2433\n",
      "Epoch 913/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4040 - acc: 0.4386 - val_loss: 2.1756 - val_acc: 0.2500\n",
      "Epoch 914/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4021 - acc: 0.4357 - val_loss: 2.1758 - val_acc: 0.2633\n",
      "Epoch 915/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4343 - val_loss: 2.1638 - val_acc: 0.2533\n",
      "Epoch 916/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4025 - acc: 0.4343 - val_loss: 2.1744 - val_acc: 0.2433\n",
      "Epoch 917/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4017 - acc: 0.4429 - val_loss: 2.1646 - val_acc: 0.2433\n",
      "Epoch 918/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4314 - val_loss: 2.1666 - val_acc: 0.2600\n",
      "Epoch 919/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4021 - acc: 0.4314 - val_loss: 2.1750 - val_acc: 0.2467\n",
      "Epoch 920/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4300 - val_loss: 2.1580 - val_acc: 0.2500\n",
      "Epoch 921/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4020 - acc: 0.4329 - val_loss: 2.1602 - val_acc: 0.2433\n",
      "Epoch 922/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4003 - acc: 0.4386 - val_loss: 2.1758 - val_acc: 0.2533\n",
      "Epoch 923/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4002 - acc: 0.4400 - val_loss: 2.1590 - val_acc: 0.2600\n",
      "Epoch 924/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4027 - acc: 0.4414 - val_loss: 2.1677 - val_acc: 0.2533\n",
      "Epoch 925/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4007 - acc: 0.4314 - val_loss: 2.1722 - val_acc: 0.2433\n",
      "Epoch 926/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4010 - acc: 0.4414 - val_loss: 2.1636 - val_acc: 0.2533\n",
      "Epoch 927/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4013 - acc: 0.4271 - val_loss: 2.1688 - val_acc: 0.2467\n",
      "Epoch 928/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4400 - val_loss: 2.1910 - val_acc: 0.2433\n",
      "Epoch 929/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4008 - acc: 0.4343 - val_loss: 2.1811 - val_acc: 0.2467\n",
      "Epoch 930/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4010 - acc: 0.4343 - val_loss: 2.1650 - val_acc: 0.2533\n",
      "Epoch 931/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4357 - val_loss: 2.1661 - val_acc: 0.2467\n",
      "Epoch 932/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4199 - acc: 0.441 - ETA: 0s - loss: 1.4000 - acc: 0.4429 - val_loss: 2.1570 - val_acc: 0.2500\n",
      "Epoch 933/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4005 - acc: 0.4314 - val_loss: 2.1688 - val_acc: 0.2500\n",
      "Epoch 934/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3996 - acc: 0.4371 - val_loss: 2.1751 - val_acc: 0.2533\n",
      "Epoch 935/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4007 - acc: 0.4371 - val_loss: 2.1730 - val_acc: 0.2500\n",
      "Epoch 936/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.4002 - acc: 0.4400 - val_loss: 2.1801 - val_acc: 0.2567\n",
      "Epoch 937/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3999 - acc: 0.4314 - val_loss: 2.1735 - val_acc: 0.2433\n",
      "Epoch 938/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3991 - acc: 0.4371 - val_loss: 2.1771 - val_acc: 0.2533\n",
      "Epoch 939/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3990 - acc: 0.4357 - val_loss: 2.1882 - val_acc: 0.2567\n",
      "Epoch 940/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3997 - acc: 0.4343 - val_loss: 2.1665 - val_acc: 0.2433\n",
      "Epoch 941/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3992 - acc: 0.4329 - val_loss: 2.1711 - val_acc: 0.2467\n",
      "Epoch 942/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4371 - val_loss: 2.1561 - val_acc: 0.2367\n",
      "Epoch 943/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3992 - acc: 0.4371 - val_loss: 2.1819 - val_acc: 0.2467\n",
      "Epoch 944/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3980 - acc: 0.4386 - val_loss: 2.1831 - val_acc: 0.2533\n",
      "Epoch 945/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3984 - acc: 0.4314 - val_loss: 2.1832 - val_acc: 0.2533\n",
      "Epoch 946/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3985 - acc: 0.4343 - val_loss: 2.1723 - val_acc: 0.2567\n",
      "Epoch 947/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4386 - val_loss: 2.1884 - val_acc: 0.2633\n",
      "Epoch 948/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4286 - val_loss: 2.1630 - val_acc: 0.2400\n",
      "Epoch 949/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3979 - acc: 0.4400 - val_loss: 2.1751 - val_acc: 0.2533\n",
      "Epoch 950/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3967 - acc: 0.4400 - val_loss: 2.1696 - val_acc: 0.2433\n",
      "Epoch 951/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4371 - val_loss: 2.1807 - val_acc: 0.2567\n",
      "Epoch 952/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.4400 - val_loss: 2.1805 - val_acc: 0.2467\n",
      "Epoch 953/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4386 - val_loss: 2.1802 - val_acc: 0.2600\n",
      "Epoch 954/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4414 - val_loss: 2.1737 - val_acc: 0.2567\n",
      "Epoch 955/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3975 - acc: 0.4400 - val_loss: 2.1874 - val_acc: 0.2500\n",
      "Epoch 956/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3968 - acc: 0.4386 - val_loss: 2.1928 - val_acc: 0.2433\n",
      "Epoch 957/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3964 - acc: 0.4371 - val_loss: 2.1804 - val_acc: 0.2467\n",
      "Epoch 958/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3976 - acc: 0.4343 - val_loss: 2.2071 - val_acc: 0.2433\n",
      "Epoch 959/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3974 - acc: 0.4343 - val_loss: 2.1686 - val_acc: 0.2400\n",
      "Epoch 960/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3966 - acc: 0.4314 - val_loss: 2.1845 - val_acc: 0.2467\n",
      "Epoch 961/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3961 - acc: 0.4414 - val_loss: 2.1560 - val_acc: 0.2433\n",
      "Epoch 962/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3969 - acc: 0.4357 - val_loss: 2.1803 - val_acc: 0.2467\n",
      "Epoch 963/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3962 - acc: 0.4371 - val_loss: 2.1783 - val_acc: 0.2500\n",
      "Epoch 964/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3957 - acc: 0.4400 - val_loss: 2.1736 - val_acc: 0.2500\n",
      "Epoch 965/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3950 - acc: 0.4414 - val_loss: 2.1729 - val_acc: 0.2467\n",
      "Epoch 966/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3962 - acc: 0.4400 - val_loss: 2.1969 - val_acc: 0.2500\n",
      "Epoch 967/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3963 - acc: 0.4357 - val_loss: 2.1877 - val_acc: 0.2467\n",
      "Epoch 968/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3952 - acc: 0.4343 - val_loss: 2.1762 - val_acc: 0.2400\n",
      "Epoch 969/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3955 - acc: 0.4443 - val_loss: 2.1823 - val_acc: 0.2467\n",
      "Epoch 970/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3954 - acc: 0.4357 - val_loss: 2.1949 - val_acc: 0.2433\n",
      "Epoch 971/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3955 - acc: 0.4414 - val_loss: 2.1768 - val_acc: 0.2433\n",
      "Epoch 972/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3951 - acc: 0.4386 - val_loss: 2.1856 - val_acc: 0.2467\n",
      "Epoch 973/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3940 - acc: 0.4357 - val_loss: 2.1846 - val_acc: 0.2533\n",
      "Epoch 974/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3947 - acc: 0.4329 - val_loss: 2.1881 - val_acc: 0.2500\n",
      "Epoch 975/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3945 - acc: 0.4429 - val_loss: 2.1975 - val_acc: 0.2533\n",
      "Epoch 976/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3942 - acc: 0.4414 - val_loss: 2.1797 - val_acc: 0.2433\n",
      "Epoch 977/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4386 - val_loss: 2.1774 - val_acc: 0.2367\n",
      "Epoch 978/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3939 - acc: 0.4514 - val_loss: 2.1859 - val_acc: 0.2500\n",
      "Epoch 979/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3944 - acc: 0.4386 - val_loss: 2.1872 - val_acc: 0.2433\n",
      "Epoch 980/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3943 - acc: 0.4386 - val_loss: 2.1973 - val_acc: 0.2533\n",
      "Epoch 981/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3932 - acc: 0.4343 - val_loss: 2.1982 - val_acc: 0.2500\n",
      "Epoch 982/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3939 - acc: 0.4400 - val_loss: 2.1893 - val_acc: 0.2533\n",
      "Epoch 983/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3938 - acc: 0.4343 - val_loss: 2.1950 - val_acc: 0.2500\n",
      "Epoch 984/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3936 - acc: 0.4357 - val_loss: 2.1862 - val_acc: 0.2567\n",
      "Epoch 985/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3931 - acc: 0.4371 - val_loss: 2.1963 - val_acc: 0.2433\n",
      "Epoch 986/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3932 - acc: 0.4414 - val_loss: 2.1921 - val_acc: 0.2567\n",
      "Epoch 987/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3925 - acc: 0.4371 - val_loss: 2.1834 - val_acc: 0.2400\n",
      "Epoch 988/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4443 - val_loss: 2.2093 - val_acc: 0.2633\n",
      "Epoch 989/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3941 - acc: 0.4371 - val_loss: 2.2066 - val_acc: 0.2500\n",
      "Epoch 990/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4443 - val_loss: 2.2209 - val_acc: 0.2500\n",
      "Epoch 991/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3928 - acc: 0.4429 - val_loss: 2.1883 - val_acc: 0.2467\n",
      "Epoch 992/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3926 - acc: 0.4371 - val_loss: 2.1986 - val_acc: 0.2467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 993/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3925 - acc: 0.4486 - val_loss: 2.1934 - val_acc: 0.2433\n",
      "Epoch 994/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3921 - acc: 0.4343 - val_loss: 2.1780 - val_acc: 0.2367\n",
      "Epoch 995/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3930 - acc: 0.4400 - val_loss: 2.1981 - val_acc: 0.2467\n",
      "Epoch 996/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3916 - acc: 0.4457 - val_loss: 2.1997 - val_acc: 0.2467\n",
      "Epoch 997/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3914 - acc: 0.4443 - val_loss: 2.2021 - val_acc: 0.2467\n",
      "Epoch 998/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3923 - acc: 0.4429 - val_loss: 2.2110 - val_acc: 0.2500\n",
      "Epoch 999/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3920 - acc: 0.4371 - val_loss: 2.1971 - val_acc: 0.2467\n",
      "Epoch 1000/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3919 - acc: 0.4386 - val_loss: 2.2072 - val_acc: 0.2467\n",
      "Epoch 1001/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3900 - acc: 0.4429 - val_loss: 2.1993 - val_acc: 0.2633\n",
      "Epoch 1002/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3922 - acc: 0.4371 - val_loss: 2.1957 - val_acc: 0.2500\n",
      "Epoch 1003/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3901 - acc: 0.4414 - val_loss: 2.2094 - val_acc: 0.2533\n",
      "Epoch 1004/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3924 - acc: 0.4386 - val_loss: 2.2153 - val_acc: 0.2533\n",
      "Epoch 1005/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3911 - acc: 0.4386 - val_loss: 2.1995 - val_acc: 0.2467\n",
      "Epoch 1006/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3907 - acc: 0.4371 - val_loss: 2.2186 - val_acc: 0.2433\n",
      "Epoch 1007/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3902 - acc: 0.4414 - val_loss: 2.2208 - val_acc: 0.2533\n",
      "Epoch 1008/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3909 - acc: 0.4386 - val_loss: 2.1921 - val_acc: 0.2533\n",
      "Epoch 1009/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3903 - acc: 0.4429 - val_loss: 2.2181 - val_acc: 0.2467\n",
      "Epoch 1010/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3904 - acc: 0.4414 - val_loss: 2.1962 - val_acc: 0.2500\n",
      "Epoch 1011/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3897 - acc: 0.4414 - val_loss: 2.2043 - val_acc: 0.2433\n",
      "Epoch 1012/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3890 - acc: 0.4357 - val_loss: 2.2247 - val_acc: 0.2467\n",
      "Epoch 1013/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3890 - acc: 0.4443 - val_loss: 2.1936 - val_acc: 0.2333\n",
      "Epoch 1014/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3890 - acc: 0.4386 - val_loss: 2.1956 - val_acc: 0.2533\n",
      "Epoch 1015/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3879 - acc: 0.4400 - val_loss: 2.2001 - val_acc: 0.2367\n",
      "Epoch 1016/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3905 - acc: 0.4386 - val_loss: 2.2019 - val_acc: 0.2400\n",
      "Epoch 1017/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3902 - acc: 0.4386 - val_loss: 2.2029 - val_acc: 0.2500\n",
      "Epoch 1018/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3898 - acc: 0.4414 - val_loss: 2.1982 - val_acc: 0.2500\n",
      "Epoch 1019/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3887 - acc: 0.4400 - val_loss: 2.1991 - val_acc: 0.2400\n",
      "Epoch 1020/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3884 - acc: 0.4471 - val_loss: 2.2006 - val_acc: 0.2467\n",
      "Epoch 1021/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3894 - acc: 0.4357 - val_loss: 2.2032 - val_acc: 0.2467\n",
      "Epoch 1022/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3882 - acc: 0.4443 - val_loss: 2.2068 - val_acc: 0.2400\n",
      "Epoch 1023/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3894 - acc: 0.4429 - val_loss: 2.1943 - val_acc: 0.2500\n",
      "Epoch 1024/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3886 - acc: 0.4471 - val_loss: 2.1938 - val_acc: 0.2400\n",
      "Epoch 1025/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3883 - acc: 0.4443 - val_loss: 2.2003 - val_acc: 0.2500\n",
      "Epoch 1026/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3888 - acc: 0.4457 - val_loss: 2.1828 - val_acc: 0.2333\n",
      "Epoch 1027/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3880 - acc: 0.4400 - val_loss: 2.2055 - val_acc: 0.2533\n",
      "Epoch 1028/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3880 - acc: 0.4371 - val_loss: 2.1984 - val_acc: 0.2367\n",
      "Epoch 1029/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3883 - acc: 0.4457 - val_loss: 2.1971 - val_acc: 0.2433\n",
      "Epoch 1030/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3874 - acc: 0.4500 - val_loss: 2.2071 - val_acc: 0.2467\n",
      "Epoch 1031/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3873 - acc: 0.4400 - val_loss: 2.2120 - val_acc: 0.2433\n",
      "Epoch 1032/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3886 - acc: 0.4429 - val_loss: 2.2149 - val_acc: 0.2400\n",
      "Epoch 1033/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3882 - acc: 0.4443 - val_loss: 2.2261 - val_acc: 0.2367\n",
      "Epoch 1034/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3860 - acc: 0.4400 - val_loss: 2.2048 - val_acc: 0.2300\n",
      "Epoch 1035/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3881 - acc: 0.4443 - val_loss: 2.2065 - val_acc: 0.2433\n",
      "Epoch 1036/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3874 - acc: 0.4429 - val_loss: 2.2058 - val_acc: 0.2467\n",
      "Epoch 1037/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3869 - acc: 0.4400 - val_loss: 2.2051 - val_acc: 0.2333\n",
      "Epoch 1038/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3878 - acc: 0.4443 - val_loss: 2.2262 - val_acc: 0.2433\n",
      "Epoch 1039/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3874 - acc: 0.4457 - val_loss: 2.2131 - val_acc: 0.2467\n",
      "Epoch 1040/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3871 - acc: 0.4443 - val_loss: 2.2067 - val_acc: 0.2367\n",
      "Epoch 1041/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3862 - acc: 0.4371 - val_loss: 2.2250 - val_acc: 0.2433\n",
      "Epoch 1042/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3853 - acc: 0.4414 - val_loss: 2.1994 - val_acc: 0.2367\n",
      "Epoch 1043/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3874 - acc: 0.4329 - val_loss: 2.2180 - val_acc: 0.2433\n",
      "Epoch 1044/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3864 - acc: 0.4386 - val_loss: 2.2109 - val_acc: 0.2333\n",
      "Epoch 1045/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3840 - acc: 0.4357 - val_loss: 2.2082 - val_acc: 0.2300\n",
      "Epoch 1046/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3870 - acc: 0.4414 - val_loss: 2.2141 - val_acc: 0.2367\n",
      "Epoch 1047/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3848 - acc: 0.4443 - val_loss: 2.2074 - val_acc: 0.2333\n",
      "Epoch 1048/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3863 - acc: 0.4400 - val_loss: 2.2035 - val_acc: 0.2333\n",
      "Epoch 1049/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3858 - acc: 0.4429 - val_loss: 2.2077 - val_acc: 0.2433\n",
      "Epoch 1050/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3848 - acc: 0.4414 - val_loss: 2.2162 - val_acc: 0.2333\n",
      "Epoch 1051/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3861 - acc: 0.4457 - val_loss: 2.2091 - val_acc: 0.2400\n",
      "Epoch 1052/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3848 - acc: 0.4400 - val_loss: 2.2198 - val_acc: 0.2367\n",
      "Epoch 1053/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3850 - acc: 0.4414 - val_loss: 2.2164 - val_acc: 0.2333\n",
      "Epoch 1054/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3850 - acc: 0.4500 - val_loss: 2.1985 - val_acc: 0.2400\n",
      "Epoch 1055/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3846 - acc: 0.4443 - val_loss: 2.2043 - val_acc: 0.2367\n",
      "Epoch 1056/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3845 - acc: 0.4414 - val_loss: 2.2213 - val_acc: 0.2333\n",
      "Epoch 1057/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3838 - acc: 0.4386 - val_loss: 2.2275 - val_acc: 0.2433\n",
      "Epoch 1058/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3851 - acc: 0.4443 - val_loss: 2.2199 - val_acc: 0.2400\n",
      "Epoch 1059/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3837 - acc: 0.4429 - val_loss: 2.2145 - val_acc: 0.2467\n",
      "Epoch 1060/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3840 - acc: 0.4429 - val_loss: 2.2057 - val_acc: 0.2467\n",
      "Epoch 1061/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3844 - acc: 0.4414 - val_loss: 2.2263 - val_acc: 0.2433\n",
      "Epoch 1062/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3849 - acc: 0.4400 - val_loss: 2.2153 - val_acc: 0.2367\n",
      "Epoch 1063/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3836 - acc: 0.4429 - val_loss: 2.2082 - val_acc: 0.2400\n",
      "Epoch 1064/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3839 - acc: 0.4400 - val_loss: 2.2227 - val_acc: 0.2333\n",
      "Epoch 1065/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3841 - acc: 0.4357 - val_loss: 2.2089 - val_acc: 0.2367\n",
      "Epoch 1066/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3839 - acc: 0.4386 - val_loss: 2.2119 - val_acc: 0.2333\n",
      "Epoch 1067/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3835 - acc: 0.4429 - val_loss: 2.2354 - val_acc: 0.2433\n",
      "Epoch 1068/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3833 - acc: 0.4414 - val_loss: 2.2327 - val_acc: 0.2400\n",
      "Epoch 1069/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3834 - acc: 0.4400 - val_loss: 2.2281 - val_acc: 0.2433\n",
      "Epoch 1070/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3825 - acc: 0.4414 - val_loss: 2.2206 - val_acc: 0.2367\n",
      "Epoch 1071/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3821 - acc: 0.4443 - val_loss: 2.2426 - val_acc: 0.2267\n",
      "Epoch 1072/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3829 - acc: 0.4514 - val_loss: 2.2197 - val_acc: 0.2433\n",
      "Epoch 1073/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3824 - acc: 0.4443 - val_loss: 2.2128 - val_acc: 0.2367\n",
      "Epoch 1074/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3828 - acc: 0.4529 - val_loss: 2.2080 - val_acc: 0.2400\n",
      "Epoch 1075/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3827 - acc: 0.4471 - val_loss: 2.2176 - val_acc: 0.2367\n",
      "Epoch 1076/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3829 - acc: 0.4443 - val_loss: 2.2102 - val_acc: 0.2367\n",
      "Epoch 1077/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3826 - acc: 0.4443 - val_loss: 2.2085 - val_acc: 0.2367\n",
      "Epoch 1078/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3822 - acc: 0.4486 - val_loss: 2.2283 - val_acc: 0.2367\n",
      "Epoch 1079/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3819 - acc: 0.4457 - val_loss: 2.2206 - val_acc: 0.2367\n",
      "Epoch 1080/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3819 - acc: 0.4400 - val_loss: 2.2200 - val_acc: 0.2433\n",
      "Epoch 1081/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3821 - acc: 0.4429 - val_loss: 2.2218 - val_acc: 0.2333\n",
      "Epoch 1082/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3809 - acc: 0.4486 - val_loss: 2.2310 - val_acc: 0.2533\n",
      "Epoch 1083/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3823 - acc: 0.4386 - val_loss: 2.2289 - val_acc: 0.2300\n",
      "Epoch 1084/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3822 - acc: 0.4457 - val_loss: 2.2348 - val_acc: 0.2333\n",
      "Epoch 1085/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3811 - acc: 0.4429 - val_loss: 2.2245 - val_acc: 0.2367\n",
      "Epoch 1086/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3816 - acc: 0.4514 - val_loss: 2.2365 - val_acc: 0.2300\n",
      "Epoch 1087/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3807 - acc: 0.4471 - val_loss: 2.2268 - val_acc: 0.2333\n",
      "Epoch 1088/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3807 - acc: 0.4500 - val_loss: 2.2178 - val_acc: 0.2567\n",
      "Epoch 1089/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3820 - acc: 0.4457 - val_loss: 2.2259 - val_acc: 0.2433\n",
      "Epoch 1090/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3806 - acc: 0.4457 - val_loss: 2.2308 - val_acc: 0.2400\n",
      "Epoch 1091/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3806 - acc: 0.4457 - val_loss: 2.2200 - val_acc: 0.2333\n",
      "Epoch 1092/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3804 - acc: 0.4443 - val_loss: 2.2311 - val_acc: 0.2433\n",
      "Epoch 1093/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3808 - acc: 0.4443 - val_loss: 2.2279 - val_acc: 0.2433\n",
      "Epoch 1094/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3791 - acc: 0.4529 - val_loss: 2.2162 - val_acc: 0.2467\n",
      "Epoch 1095/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3817 - acc: 0.4414 - val_loss: 2.2289 - val_acc: 0.2400\n",
      "Epoch 1096/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3800 - acc: 0.4500 - val_loss: 2.2279 - val_acc: 0.2333\n",
      "Epoch 1097/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3801 - acc: 0.4500 - val_loss: 2.2244 - val_acc: 0.2433\n",
      "Epoch 1098/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3807 - acc: 0.4386 - val_loss: 2.2383 - val_acc: 0.2333\n",
      "Epoch 1099/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3806 - acc: 0.4429 - val_loss: 2.2435 - val_acc: 0.2367\n",
      "Epoch 1100/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3802 - acc: 0.4471 - val_loss: 2.2340 - val_acc: 0.2433\n",
      "Epoch 1101/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3798 - acc: 0.4500 - val_loss: 2.2287 - val_acc: 0.2400\n",
      "Epoch 1102/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3789 - acc: 0.4400 - val_loss: 2.2381 - val_acc: 0.2333\n",
      "Epoch 1103/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3793 - acc: 0.4414 - val_loss: 2.2367 - val_acc: 0.2433\n",
      "Epoch 1104/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3799 - acc: 0.4429 - val_loss: 2.2243 - val_acc: 0.2333\n",
      "Epoch 1105/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3792 - acc: 0.4529 - val_loss: 2.2224 - val_acc: 0.2333\n",
      "Epoch 1106/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3786 - acc: 0.4443 - val_loss: 2.2369 - val_acc: 0.2467\n",
      "Epoch 1107/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3799 - acc: 0.4471 - val_loss: 2.2125 - val_acc: 0.2433\n",
      "Epoch 1108/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3796 - acc: 0.4500 - val_loss: 2.2353 - val_acc: 0.2400\n",
      "Epoch 1109/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3794 - acc: 0.4471 - val_loss: 2.2433 - val_acc: 0.2333\n",
      "Epoch 1110/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3785 - acc: 0.4386 - val_loss: 2.2118 - val_acc: 0.2367\n",
      "Epoch 1111/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3781 - acc: 0.4486 - val_loss: 2.2344 - val_acc: 0.2367\n",
      "Epoch 1112/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3784 - acc: 0.4414 - val_loss: 2.2375 - val_acc: 0.2300\n",
      "Epoch 1113/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3776 - acc: 0.4429 - val_loss: 2.2369 - val_acc: 0.2400\n",
      "Epoch 1114/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3784 - acc: 0.4443 - val_loss: 2.2357 - val_acc: 0.2333\n",
      "Epoch 1115/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3793 - acc: 0.4486 - val_loss: 2.2270 - val_acc: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1116/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3777 - acc: 0.4529 - val_loss: 2.2355 - val_acc: 0.2367\n",
      "Epoch 1117/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3777 - acc: 0.4500 - val_loss: 2.2320 - val_acc: 0.2400\n",
      "Epoch 1118/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3779 - acc: 0.4529 - val_loss: 2.2346 - val_acc: 0.2367\n",
      "Epoch 1119/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3771 - acc: 0.4400 - val_loss: 2.2360 - val_acc: 0.2333\n",
      "Epoch 1120/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3773 - acc: 0.4529 - val_loss: 2.2498 - val_acc: 0.2433\n",
      "Epoch 1121/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3781 - acc: 0.4443 - val_loss: 2.2274 - val_acc: 0.2400\n",
      "Epoch 1122/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3776 - acc: 0.4500 - val_loss: 2.2392 - val_acc: 0.2333\n",
      "Epoch 1123/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3769 - acc: 0.4414 - val_loss: 2.2317 - val_acc: 0.2433\n",
      "Epoch 1124/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3778 - acc: 0.4486 - val_loss: 2.2336 - val_acc: 0.2367\n",
      "Epoch 1125/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3777 - acc: 0.4471 - val_loss: 2.2329 - val_acc: 0.2333\n",
      "Epoch 1126/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3762 - acc: 0.4529 - val_loss: 2.2330 - val_acc: 0.2467\n",
      "Epoch 1127/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3768 - acc: 0.4529 - val_loss: 2.2345 - val_acc: 0.2400\n",
      "Epoch 1128/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3778 - acc: 0.4443 - val_loss: 2.2515 - val_acc: 0.2400\n",
      "Epoch 1129/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3758 - acc: 0.4443 - val_loss: 2.2424 - val_acc: 0.2200\n",
      "Epoch 1130/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3768 - acc: 0.4443 - val_loss: 2.2388 - val_acc: 0.2300\n",
      "Epoch 1131/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3746 - acc: 0.4457 - val_loss: 2.2496 - val_acc: 0.2500\n",
      "Epoch 1132/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3779 - acc: 0.4414 - val_loss: 2.2409 - val_acc: 0.2367\n",
      "Epoch 1133/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3754 - acc: 0.4543 - val_loss: 2.2468 - val_acc: 0.2367\n",
      "Epoch 1134/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3758 - acc: 0.4429 - val_loss: 2.2387 - val_acc: 0.2233\n",
      "Epoch 1135/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3765 - acc: 0.4471 - val_loss: 2.2384 - val_acc: 0.2400\n",
      "Epoch 1136/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3758 - acc: 0.4457 - val_loss: 2.2338 - val_acc: 0.2333\n",
      "Epoch 1137/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3767 - acc: 0.4514 - val_loss: 2.2356 - val_acc: 0.2400\n",
      "Epoch 1138/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3756 - acc: 0.4500 - val_loss: 2.2363 - val_acc: 0.2433\n",
      "Epoch 1139/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3761 - acc: 0.4500 - val_loss: 2.2316 - val_acc: 0.2400\n",
      "Epoch 1140/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3741 - acc: 0.4471 - val_loss: 2.2266 - val_acc: 0.2333\n",
      "Epoch 1141/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3756 - acc: 0.4471 - val_loss: 2.2553 - val_acc: 0.2400\n",
      "Epoch 1142/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3758 - acc: 0.4529 - val_loss: 2.2502 - val_acc: 0.2400\n",
      "Epoch 1143/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3747 - acc: 0.4443 - val_loss: 2.2491 - val_acc: 0.2233\n",
      "Epoch 1144/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3727 - acc: 0.4414 - val_loss: 2.2571 - val_acc: 0.2233\n",
      "Epoch 1145/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3757 - acc: 0.4557 - val_loss: 2.2406 - val_acc: 0.2367\n",
      "Epoch 1146/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3744 - acc: 0.4500 - val_loss: 2.2539 - val_acc: 0.2233\n",
      "Epoch 1147/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3741 - acc: 0.4443 - val_loss: 2.2395 - val_acc: 0.2300\n",
      "Epoch 1148/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3753 - acc: 0.4557 - val_loss: 2.2548 - val_acc: 0.2367\n",
      "Epoch 1149/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3753 - acc: 0.4457 - val_loss: 2.2480 - val_acc: 0.2300\n",
      "Epoch 1150/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3751 - acc: 0.4414 - val_loss: 2.2266 - val_acc: 0.2300\n",
      "Epoch 1151/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3752 - acc: 0.4471 - val_loss: 2.2354 - val_acc: 0.2333\n",
      "Epoch 1152/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3739 - acc: 0.4543 - val_loss: 2.2407 - val_acc: 0.2400\n",
      "Epoch 1153/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3752 - acc: 0.4486 - val_loss: 2.2470 - val_acc: 0.2367\n",
      "Epoch 1154/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3737 - acc: 0.4500 - val_loss: 2.2220 - val_acc: 0.2367\n",
      "Epoch 1155/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3739 - acc: 0.4514 - val_loss: 2.2431 - val_acc: 0.2300\n",
      "Epoch 1156/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3744 - acc: 0.4400 - val_loss: 2.2373 - val_acc: 0.2300\n",
      "Epoch 1157/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3748 - acc: 0.4529 - val_loss: 2.2411 - val_acc: 0.2367\n",
      "Epoch 1158/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3736 - acc: 0.4529 - val_loss: 2.2624 - val_acc: 0.2400\n",
      "Epoch 1159/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3738 - acc: 0.4457 - val_loss: 2.2435 - val_acc: 0.2300\n",
      "Epoch 1160/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3747 - acc: 0.4486 - val_loss: 2.2625 - val_acc: 0.2233\n",
      "Epoch 1161/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3734 - acc: 0.4514 - val_loss: 2.2505 - val_acc: 0.2333\n",
      "Epoch 1162/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3734 - acc: 0.4543 - val_loss: 2.2504 - val_acc: 0.2367\n",
      "Epoch 1163/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3734 - acc: 0.4514 - val_loss: 2.2515 - val_acc: 0.2367\n",
      "Epoch 1164/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3727 - acc: 0.4486 - val_loss: 2.2582 - val_acc: 0.2200\n",
      "Epoch 1165/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3743 - acc: 0.4529 - val_loss: 2.2323 - val_acc: 0.2333\n",
      "Epoch 1166/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3726 - acc: 0.4600 - val_loss: 2.2419 - val_acc: 0.2300\n",
      "Epoch 1167/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3729 - acc: 0.4429 - val_loss: 2.2432 - val_acc: 0.2300\n",
      "Epoch 1168/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3733 - acc: 0.4471 - val_loss: 2.2434 - val_acc: 0.2300\n",
      "Epoch 1169/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3726 - acc: 0.4500 - val_loss: 2.2503 - val_acc: 0.2400\n",
      "Epoch 1170/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3720 - acc: 0.4500 - val_loss: 2.2424 - val_acc: 0.2300\n",
      "Epoch 1171/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3718 - acc: 0.4500 - val_loss: 2.2402 - val_acc: 0.2333\n",
      "Epoch 1172/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3719 - acc: 0.4557 - val_loss: 2.2562 - val_acc: 0.2367\n",
      "Epoch 1173/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3719 - acc: 0.4500 - val_loss: 2.2489 - val_acc: 0.2267\n",
      "Epoch 1174/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3728 - acc: 0.4471 - val_loss: 2.2390 - val_acc: 0.2367\n",
      "Epoch 1175/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3710 - acc: 0.4529 - val_loss: 2.2505 - val_acc: 0.2300\n",
      "Epoch 1176/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3722 - acc: 0.4457 - val_loss: 2.2485 - val_acc: 0.2400\n",
      "Epoch 1177/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3720 - acc: 0.4514 - val_loss: 2.2582 - val_acc: 0.2333\n",
      "Epoch 1178/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3713 - acc: 0.4457 - val_loss: 2.2531 - val_acc: 0.2233\n",
      "Epoch 1179/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3716 - acc: 0.4543 - val_loss: 2.2515 - val_acc: 0.2267\n",
      "Epoch 1180/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3721 - acc: 0.4514 - val_loss: 2.2535 - val_acc: 0.2333\n",
      "Epoch 1181/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3710 - acc: 0.4557 - val_loss: 2.2641 - val_acc: 0.2467\n",
      "Epoch 1182/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3722 - acc: 0.4486 - val_loss: 2.2486 - val_acc: 0.2300\n",
      "Epoch 1183/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3716 - acc: 0.4486 - val_loss: 2.2526 - val_acc: 0.2400\n",
      "Epoch 1184/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3702 - acc: 0.4543 - val_loss: 2.2440 - val_acc: 0.2300\n",
      "Epoch 1185/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3715 - acc: 0.4543 - val_loss: 2.2474 - val_acc: 0.2267\n",
      "Epoch 1186/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3712 - acc: 0.4486 - val_loss: 2.2610 - val_acc: 0.2333\n",
      "Epoch 1187/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3705 - acc: 0.4471 - val_loss: 2.2548 - val_acc: 0.2267\n",
      "Epoch 1188/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3706 - acc: 0.4586 - val_loss: 2.2611 - val_acc: 0.2367\n",
      "Epoch 1189/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3695 - acc: 0.4529 - val_loss: 2.2544 - val_acc: 0.2367\n",
      "Epoch 1190/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3703 - acc: 0.4471 - val_loss: 2.2674 - val_acc: 0.2367\n",
      "Epoch 1191/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3708 - acc: 0.4471 - val_loss: 2.2599 - val_acc: 0.2233\n",
      "Epoch 1192/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3709 - acc: 0.4557 - val_loss: 2.2479 - val_acc: 0.2300\n",
      "Epoch 1193/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3709 - acc: 0.4557 - val_loss: 2.2515 - val_acc: 0.2267\n",
      "Epoch 1194/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3701 - acc: 0.4500 - val_loss: 2.2496 - val_acc: 0.2267\n",
      "Epoch 1195/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3706 - acc: 0.4557 - val_loss: 2.2461 - val_acc: 0.2200\n",
      "Epoch 1196/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3694 - acc: 0.4529 - val_loss: 2.2596 - val_acc: 0.2300\n",
      "Epoch 1197/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3697 - acc: 0.4514 - val_loss: 2.2402 - val_acc: 0.2300\n",
      "Epoch 1198/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3693 - acc: 0.4514 - val_loss: 2.2557 - val_acc: 0.2400\n",
      "Epoch 1199/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3695 - acc: 0.4514 - val_loss: 2.2554 - val_acc: 0.2267\n",
      "Epoch 1200/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3703 - acc: 0.4529 - val_loss: 2.2600 - val_acc: 0.2333\n",
      "Epoch 1201/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3694 - acc: 0.4571 - val_loss: 2.2562 - val_acc: 0.2333\n",
      "Epoch 1202/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3702 - acc: 0.4471 - val_loss: 2.2617 - val_acc: 0.2400\n",
      "Epoch 1203/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3694 - acc: 0.4557 - val_loss: 2.2400 - val_acc: 0.2400\n",
      "Epoch 1204/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3693 - acc: 0.4571 - val_loss: 2.2419 - val_acc: 0.2300\n",
      "Epoch 1205/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3681 - acc: 0.4543 - val_loss: 2.2521 - val_acc: 0.2367\n",
      "Epoch 1206/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3692 - acc: 0.4557 - val_loss: 2.2618 - val_acc: 0.2400\n",
      "Epoch 1207/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3678 - acc: 0.4529 - val_loss: 2.2627 - val_acc: 0.2200\n",
      "Epoch 1208/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3674 - acc: 0.4529 - val_loss: 2.2577 - val_acc: 0.2200\n",
      "Epoch 1209/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3669 - acc: 0.4514 - val_loss: 2.2535 - val_acc: 0.2333\n",
      "Epoch 1210/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3689 - acc: 0.4571 - val_loss: 2.2558 - val_acc: 0.2333\n",
      "Epoch 1211/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3686 - acc: 0.4529 - val_loss: 2.2601 - val_acc: 0.2300\n",
      "Epoch 1212/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3671 - acc: 0.4529 - val_loss: 2.2645 - val_acc: 0.2400\n",
      "Epoch 1213/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3694 - acc: 0.4486 - val_loss: 2.2554 - val_acc: 0.2400\n",
      "Epoch 1214/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3677 - acc: 0.4557 - val_loss: 2.2697 - val_acc: 0.2333\n",
      "Epoch 1215/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3673 - acc: 0.4529 - val_loss: 2.2594 - val_acc: 0.2400\n",
      "Epoch 1216/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3681 - acc: 0.4557 - val_loss: 2.2515 - val_acc: 0.2400\n",
      "Epoch 1217/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3676 - acc: 0.4529 - val_loss: 2.2628 - val_acc: 0.2367\n",
      "Epoch 1218/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3672 - acc: 0.4557 - val_loss: 2.2587 - val_acc: 0.2333\n",
      "Epoch 1219/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3671 - acc: 0.4486 - val_loss: 2.2649 - val_acc: 0.2367\n",
      "Epoch 1220/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3682 - acc: 0.4500 - val_loss: 2.2497 - val_acc: 0.2333\n",
      "Epoch 1221/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3673 - acc: 0.4529 - val_loss: 2.2671 - val_acc: 0.2200\n",
      "Epoch 1222/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3675 - acc: 0.4571 - val_loss: 2.2702 - val_acc: 0.2367\n",
      "Epoch 1223/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3674 - acc: 0.4543 - val_loss: 2.2627 - val_acc: 0.2400\n",
      "Epoch 1224/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3665 - acc: 0.4600 - val_loss: 2.2707 - val_acc: 0.2400\n",
      "Epoch 1225/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3661 - acc: 0.4500 - val_loss: 2.2679 - val_acc: 0.2267\n",
      "Epoch 1226/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3676 - acc: 0.4529 - val_loss: 2.2593 - val_acc: 0.2300\n",
      "Epoch 1227/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3648 - acc: 0.4571 - val_loss: 2.2567 - val_acc: 0.2367\n",
      "Epoch 1228/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3668 - acc: 0.4600 - val_loss: 2.2692 - val_acc: 0.2367\n",
      "Epoch 1229/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3669 - acc: 0.4457 - val_loss: 2.2608 - val_acc: 0.2333\n",
      "Epoch 1230/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3664 - acc: 0.4557 - val_loss: 2.2567 - val_acc: 0.2300\n",
      "Epoch 1231/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3668 - acc: 0.4500 - val_loss: 2.2452 - val_acc: 0.2233\n",
      "Epoch 1232/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3666 - acc: 0.4514 - val_loss: 2.2728 - val_acc: 0.2333\n",
      "Epoch 1233/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3672 - acc: 0.4514 - val_loss: 2.2553 - val_acc: 0.2367\n",
      "Epoch 1234/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3658 - acc: 0.4629 - val_loss: 2.2494 - val_acc: 0.2333\n",
      "Epoch 1235/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3658 - acc: 0.4600 - val_loss: 2.2549 - val_acc: 0.2367\n",
      "Epoch 1236/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3661 - acc: 0.4514 - val_loss: 2.2555 - val_acc: 0.2300\n",
      "Epoch 1237/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3663 - acc: 0.4543 - val_loss: 2.2407 - val_acc: 0.2333\n",
      "Epoch 1238/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3649 - acc: 0.4557 - val_loss: 2.2703 - val_acc: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1239/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3660 - acc: 0.4500 - val_loss: 2.2557 - val_acc: 0.2333\n",
      "Epoch 1240/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3656 - acc: 0.4529 - val_loss: 2.2561 - val_acc: 0.2300\n",
      "Epoch 1241/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3655 - acc: 0.4529 - val_loss: 2.2621 - val_acc: 0.2367\n",
      "Epoch 1242/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3649 - acc: 0.4486 - val_loss: 2.2593 - val_acc: 0.2300\n",
      "Epoch 1243/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3772 - acc: 0.456 - ETA: 0s - loss: 1.3652 - acc: 0.4629 - val_loss: 2.2740 - val_acc: 0.2367\n",
      "Epoch 1244/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3653 - acc: 0.4529 - val_loss: 2.2491 - val_acc: 0.2333\n",
      "Epoch 1245/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3651 - acc: 0.4514 - val_loss: 2.2521 - val_acc: 0.2300\n",
      "Epoch 1246/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3642 - acc: 0.4557 - val_loss: 2.2754 - val_acc: 0.2400\n",
      "Epoch 1247/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3640 - acc: 0.4586 - val_loss: 2.2666 - val_acc: 0.2433\n",
      "Epoch 1248/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3649 - acc: 0.4457 - val_loss: 2.2694 - val_acc: 0.2333\n",
      "Epoch 1249/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3651 - acc: 0.4529 - val_loss: 2.2608 - val_acc: 0.2300\n",
      "Epoch 1250/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3637 - acc: 0.4557 - val_loss: 2.2922 - val_acc: 0.2200\n",
      "Epoch 1251/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3651 - acc: 0.4529 - val_loss: 2.2713 - val_acc: 0.2300\n",
      "Epoch 1252/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3637 - acc: 0.4543 - val_loss: 2.2725 - val_acc: 0.2367\n",
      "Epoch 1253/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3649 - acc: 0.4486 - val_loss: 2.2601 - val_acc: 0.2300\n",
      "Epoch 1254/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3642 - acc: 0.4614 - val_loss: 2.2605 - val_acc: 0.2367\n",
      "Epoch 1255/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3643 - acc: 0.4571 - val_loss: 2.2611 - val_acc: 0.2367\n",
      "Epoch 1256/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3632 - acc: 0.4571 - val_loss: 2.2762 - val_acc: 0.2300\n",
      "Epoch 1257/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3641 - acc: 0.4529 - val_loss: 2.2805 - val_acc: 0.2400\n",
      "Epoch 1258/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3639 - acc: 0.4586 - val_loss: 2.2561 - val_acc: 0.2367\n",
      "Epoch 1259/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3640 - acc: 0.4571 - val_loss: 2.2750 - val_acc: 0.2400\n",
      "Epoch 1260/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3631 - acc: 0.4500 - val_loss: 2.2738 - val_acc: 0.2400\n",
      "Epoch 1261/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3639 - acc: 0.4529 - val_loss: 2.2688 - val_acc: 0.2333\n",
      "Epoch 1262/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3640 - acc: 0.4543 - val_loss: 2.2691 - val_acc: 0.2300\n",
      "Epoch 1263/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3633 - acc: 0.4686 - val_loss: 2.2701 - val_acc: 0.2300\n",
      "Epoch 1264/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3634 - acc: 0.4557 - val_loss: 2.2661 - val_acc: 0.2300\n",
      "Epoch 1265/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3636 - acc: 0.4557 - val_loss: 2.2658 - val_acc: 0.2267\n",
      "Epoch 1266/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3634 - acc: 0.4529 - val_loss: 2.2564 - val_acc: 0.2333\n",
      "Epoch 1267/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3630 - acc: 0.4529 - val_loss: 2.2585 - val_acc: 0.2300\n",
      "Epoch 1268/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3638 - acc: 0.4543 - val_loss: 2.2698 - val_acc: 0.2300\n",
      "Epoch 1269/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3629 - acc: 0.4614 - val_loss: 2.2822 - val_acc: 0.2333\n",
      "Epoch 1270/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3624 - acc: 0.4614 - val_loss: 2.2732 - val_acc: 0.2333\n",
      "Epoch 1271/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3628 - acc: 0.4600 - val_loss: 2.2642 - val_acc: 0.2333\n",
      "Epoch 1272/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3622 - acc: 0.4657 - val_loss: 2.2729 - val_acc: 0.2333\n",
      "Epoch 1273/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3625 - acc: 0.4586 - val_loss: 2.2694 - val_acc: 0.2300\n",
      "Epoch 1274/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3627 - acc: 0.4557 - val_loss: 2.2665 - val_acc: 0.2333\n",
      "Epoch 1275/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3618 - acc: 0.4571 - val_loss: 2.2746 - val_acc: 0.2300\n",
      "Epoch 1276/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3631 - acc: 0.4557 - val_loss: 2.2728 - val_acc: 0.2333\n",
      "Epoch 1277/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3622 - acc: 0.4543 - val_loss: 2.2738 - val_acc: 0.2333\n",
      "Epoch 1278/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3615 - acc: 0.4543 - val_loss: 2.2870 - val_acc: 0.2333\n",
      "Epoch 1279/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3623 - acc: 0.4557 - val_loss: 2.2702 - val_acc: 0.2333\n",
      "Epoch 1280/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3610 - acc: 0.4586 - val_loss: 2.2807 - val_acc: 0.2333\n",
      "Epoch 1281/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3612 - acc: 0.4557 - val_loss: 2.2517 - val_acc: 0.2333\n",
      "Epoch 1282/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3618 - acc: 0.4629 - val_loss: 2.2743 - val_acc: 0.2367\n",
      "Epoch 1283/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3613 - acc: 0.4629 - val_loss: 2.2910 - val_acc: 0.2400\n",
      "Epoch 1284/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3616 - acc: 0.4543 - val_loss: 2.2850 - val_acc: 0.2400\n",
      "Epoch 1285/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3589 - acc: 0.4529 - val_loss: 2.2610 - val_acc: 0.2300\n",
      "Epoch 1286/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3589 - acc: 0.4529 - val_loss: 2.2875 - val_acc: 0.2467\n",
      "Epoch 1287/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3618 - acc: 0.4571 - val_loss: 2.2820 - val_acc: 0.2400\n",
      "Epoch 1288/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3600 - acc: 0.4543 - val_loss: 2.2821 - val_acc: 0.2333\n",
      "Epoch 1289/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3610 - acc: 0.4600 - val_loss: 2.2751 - val_acc: 0.2333\n",
      "Epoch 1290/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3608 - acc: 0.4586 - val_loss: 2.2811 - val_acc: 0.2333\n",
      "Epoch 1291/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3612 - acc: 0.4543 - val_loss: 2.2844 - val_acc: 0.2267\n",
      "Epoch 1292/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3606 - acc: 0.4557 - val_loss: 2.2899 - val_acc: 0.2400\n",
      "Epoch 1293/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3606 - acc: 0.4600 - val_loss: 2.2794 - val_acc: 0.2300\n",
      "Epoch 1294/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3613 - acc: 0.4586 - val_loss: 2.2849 - val_acc: 0.2300\n",
      "Epoch 1295/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3597 - acc: 0.4543 - val_loss: 2.2742 - val_acc: 0.2400\n",
      "Epoch 1296/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3609 - acc: 0.4614 - val_loss: 2.2745 - val_acc: 0.2300\n",
      "Epoch 1297/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3601 - acc: 0.4600 - val_loss: 2.2715 - val_acc: 0.2367\n",
      "Epoch 1298/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3607 - acc: 0.4586 - val_loss: 2.2739 - val_acc: 0.2300\n",
      "Epoch 1299/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3591 - acc: 0.4600 - val_loss: 2.2839 - val_acc: 0.2367\n",
      "Epoch 1300/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3589 - acc: 0.4671 - val_loss: 2.2750 - val_acc: 0.2267\n",
      "Epoch 1301/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3586 - acc: 0.4529 - val_loss: 2.2633 - val_acc: 0.2367\n",
      "Epoch 1302/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3602 - acc: 0.4586 - val_loss: 2.2814 - val_acc: 0.2367\n",
      "Epoch 1303/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3601 - acc: 0.4629 - val_loss: 2.2778 - val_acc: 0.2367\n",
      "Epoch 1304/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3595 - acc: 0.4657 - val_loss: 2.2875 - val_acc: 0.2333\n",
      "Epoch 1305/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3588 - acc: 0.4629 - val_loss: 2.2784 - val_acc: 0.2300\n",
      "Epoch 1306/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3592 - acc: 0.4557 - val_loss: 2.2764 - val_acc: 0.2400\n",
      "Epoch 1307/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3596 - acc: 0.4614 - val_loss: 2.2756 - val_acc: 0.2333\n",
      "Epoch 1308/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3585 - acc: 0.4614 - val_loss: 2.2673 - val_acc: 0.2367\n",
      "Epoch 1309/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3579 - acc: 0.4614 - val_loss: 2.2822 - val_acc: 0.2300\n",
      "Epoch 1310/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3595 - acc: 0.4643 - val_loss: 2.2838 - val_acc: 0.2333\n",
      "Epoch 1311/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3596 - acc: 0.4629 - val_loss: 2.2811 - val_acc: 0.2333\n",
      "Epoch 1312/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3587 - acc: 0.4600 - val_loss: 2.2761 - val_acc: 0.2400\n",
      "Epoch 1313/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3587 - acc: 0.4586 - val_loss: 2.2740 - val_acc: 0.2333\n",
      "Epoch 1314/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3583 - acc: 0.4571 - val_loss: 2.2827 - val_acc: 0.2233\n",
      "Epoch 1315/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3577 - acc: 0.4614 - val_loss: 2.2734 - val_acc: 0.2367\n",
      "Epoch 1316/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3588 - acc: 0.4600 - val_loss: 2.2906 - val_acc: 0.2400\n",
      "Epoch 1317/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3579 - acc: 0.4571 - val_loss: 2.2789 - val_acc: 0.2300\n",
      "Epoch 1318/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3585 - acc: 0.4586 - val_loss: 2.2680 - val_acc: 0.2300\n",
      "Epoch 1319/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3587 - acc: 0.4643 - val_loss: 2.2856 - val_acc: 0.2400\n",
      "Epoch 1320/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3581 - acc: 0.4543 - val_loss: 2.2775 - val_acc: 0.2400\n",
      "Epoch 1321/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3574 - acc: 0.4600 - val_loss: 2.2759 - val_acc: 0.2300\n",
      "Epoch 1322/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3578 - acc: 0.4586 - val_loss: 2.2730 - val_acc: 0.2333\n",
      "Epoch 1323/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3576 - acc: 0.4557 - val_loss: 2.3096 - val_acc: 0.2433\n",
      "Epoch 1324/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3580 - acc: 0.4643 - val_loss: 2.2821 - val_acc: 0.2333\n",
      "Epoch 1325/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3564 - acc: 0.4586 - val_loss: 2.3007 - val_acc: 0.2267\n",
      "Epoch 1326/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3584 - acc: 0.4643 - val_loss: 2.2806 - val_acc: 0.2333\n",
      "Epoch 1327/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3573 - acc: 0.4629 - val_loss: 2.2616 - val_acc: 0.2300\n",
      "Epoch 1328/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3572 - acc: 0.4686 - val_loss: 2.3068 - val_acc: 0.2400\n",
      "Epoch 1329/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3574 - acc: 0.4643 - val_loss: 2.2876 - val_acc: 0.2367\n",
      "Epoch 1330/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3576 - acc: 0.4557 - val_loss: 2.2961 - val_acc: 0.2300\n",
      "Epoch 1331/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3573 - acc: 0.4629 - val_loss: 2.2861 - val_acc: 0.2333\n",
      "Epoch 1332/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3569 - acc: 0.4657 - val_loss: 2.2908 - val_acc: 0.2333\n",
      "Epoch 1333/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3565 - acc: 0.4643 - val_loss: 2.2787 - val_acc: 0.2333\n",
      "Epoch 1334/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3561 - acc: 0.4614 - val_loss: 2.3005 - val_acc: 0.2433\n",
      "Epoch 1335/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3572 - acc: 0.4557 - val_loss: 2.2745 - val_acc: 0.2300\n",
      "Epoch 1336/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3573 - acc: 0.4643 - val_loss: 2.2963 - val_acc: 0.2333\n",
      "Epoch 1337/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3567 - acc: 0.4629 - val_loss: 2.2797 - val_acc: 0.2333\n",
      "Epoch 1338/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3562 - acc: 0.4557 - val_loss: 2.2837 - val_acc: 0.2300\n",
      "Epoch 1339/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3563 - acc: 0.4614 - val_loss: 2.3116 - val_acc: 0.2433\n",
      "Epoch 1340/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3567 - acc: 0.4600 - val_loss: 2.2914 - val_acc: 0.2333\n",
      "Epoch 1341/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3564 - acc: 0.4671 - val_loss: 2.2886 - val_acc: 0.2400\n",
      "Epoch 1342/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3565 - acc: 0.4671 - val_loss: 2.2858 - val_acc: 0.2300\n",
      "Epoch 1343/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3557 - acc: 0.4629 - val_loss: 2.2908 - val_acc: 0.2400\n",
      "Epoch 1344/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3561 - acc: 0.4600 - val_loss: 2.2929 - val_acc: 0.2333\n",
      "Epoch 1345/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3554 - acc: 0.4686 - val_loss: 2.3030 - val_acc: 0.2400\n",
      "Epoch 1346/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3554 - acc: 0.4629 - val_loss: 2.2698 - val_acc: 0.2267\n",
      "Epoch 1347/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3564 - acc: 0.4629 - val_loss: 2.2937 - val_acc: 0.2400\n",
      "Epoch 1348/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3548 - acc: 0.4629 - val_loss: 2.2869 - val_acc: 0.2367\n",
      "Epoch 1349/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3550 - acc: 0.4600 - val_loss: 2.2911 - val_acc: 0.2300\n",
      "Epoch 1350/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3555 - acc: 0.4586 - val_loss: 2.2957 - val_acc: 0.2267\n",
      "Epoch 1351/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3557 - acc: 0.4671 - val_loss: 2.2966 - val_acc: 0.2333\n",
      "Epoch 1352/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3552 - acc: 0.4614 - val_loss: 2.2785 - val_acc: 0.2367\n",
      "Epoch 1353/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3551 - acc: 0.4614 - val_loss: 2.2801 - val_acc: 0.2300\n",
      "Epoch 1354/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3553 - acc: 0.4614 - val_loss: 2.2860 - val_acc: 0.2333\n",
      "Epoch 1355/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3544 - acc: 0.4657 - val_loss: 2.2954 - val_acc: 0.2300\n",
      "Epoch 1356/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3553 - acc: 0.4586 - val_loss: 2.2938 - val_acc: 0.2333\n",
      "Epoch 1357/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3557 - acc: 0.4629 - val_loss: 2.2960 - val_acc: 0.2333\n",
      "Epoch 1358/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3547 - acc: 0.4586 - val_loss: 2.2873 - val_acc: 0.2300\n",
      "Epoch 1359/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3540 - acc: 0.4671 - val_loss: 2.2959 - val_acc: 0.2333\n",
      "Epoch 1360/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3545 - acc: 0.4643 - val_loss: 2.2917 - val_acc: 0.2367\n",
      "Epoch 1361/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3552 - acc: 0.4614 - val_loss: 2.3013 - val_acc: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1362/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3541 - acc: 0.4600 - val_loss: 2.2803 - val_acc: 0.2300\n",
      "Epoch 1363/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3545 - acc: 0.4657 - val_loss: 2.2905 - val_acc: 0.2300\n",
      "Epoch 1364/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3540 - acc: 0.4600 - val_loss: 2.3002 - val_acc: 0.2333\n",
      "Epoch 1365/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3547 - acc: 0.4671 - val_loss: 2.2777 - val_acc: 0.2300\n",
      "Epoch 1366/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3543 - acc: 0.4586 - val_loss: 2.2944 - val_acc: 0.2433\n",
      "Epoch 1367/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3540 - acc: 0.4657 - val_loss: 2.2809 - val_acc: 0.2367\n",
      "Epoch 1368/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3540 - acc: 0.4643 - val_loss: 2.2976 - val_acc: 0.2367\n",
      "Epoch 1369/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3548 - acc: 0.4671 - val_loss: 2.3093 - val_acc: 0.2433\n",
      "Epoch 1370/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3535 - acc: 0.4657 - val_loss: 2.3083 - val_acc: 0.2400\n",
      "Epoch 1371/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3544 - acc: 0.4657 - val_loss: 2.2971 - val_acc: 0.2333\n",
      "Epoch 1372/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3540 - acc: 0.4671 - val_loss: 2.2973 - val_acc: 0.2333\n",
      "Epoch 1373/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3536 - acc: 0.4686 - val_loss: 2.3006 - val_acc: 0.2300\n",
      "Epoch 1374/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3535 - acc: 0.4614 - val_loss: 2.3108 - val_acc: 0.2367\n",
      "Epoch 1375/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3538 - acc: 0.4729 - val_loss: 2.2876 - val_acc: 0.2300\n",
      "Epoch 1376/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3531 - acc: 0.4714 - val_loss: 2.2894 - val_acc: 0.2300\n",
      "Epoch 1377/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3538 - acc: 0.4629 - val_loss: 2.2747 - val_acc: 0.2300\n",
      "Epoch 1378/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3535 - acc: 0.4586 - val_loss: 2.3072 - val_acc: 0.2333\n",
      "Epoch 1379/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3529 - acc: 0.4571 - val_loss: 2.3056 - val_acc: 0.2367\n",
      "Epoch 1380/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3523 - acc: 0.4600 - val_loss: 2.3009 - val_acc: 0.2333\n",
      "Epoch 1381/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3530 - acc: 0.4671 - val_loss: 2.2941 - val_acc: 0.2333\n",
      "Epoch 1382/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3515 - acc: 0.4686 - val_loss: 2.3130 - val_acc: 0.2433\n",
      "Epoch 1383/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3523 - acc: 0.4614 - val_loss: 2.3045 - val_acc: 0.2333\n",
      "Epoch 1384/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3528 - acc: 0.4643 - val_loss: 2.2954 - val_acc: 0.2300\n",
      "Epoch 1385/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3518 - acc: 0.4657 - val_loss: 2.3050 - val_acc: 0.2367\n",
      "Epoch 1386/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3521 - acc: 0.4629 - val_loss: 2.3000 - val_acc: 0.2267\n",
      "Epoch 1387/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3522 - acc: 0.4671 - val_loss: 2.3112 - val_acc: 0.2267\n",
      "Epoch 1388/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3524 - acc: 0.4614 - val_loss: 2.3042 - val_acc: 0.2300\n",
      "Epoch 1389/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3513 - acc: 0.4629 - val_loss: 2.3101 - val_acc: 0.2367\n",
      "Epoch 1390/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3523 - acc: 0.4643 - val_loss: 2.3036 - val_acc: 0.2333\n",
      "Epoch 1391/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3525 - acc: 0.4657 - val_loss: 2.3048 - val_acc: 0.2333\n",
      "Epoch 1392/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3525 - acc: 0.4614 - val_loss: 2.3167 - val_acc: 0.2400\n",
      "Epoch 1393/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3518 - acc: 0.4643 - val_loss: 2.3067 - val_acc: 0.2433\n",
      "Epoch 1394/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3504 - acc: 0.4643 - val_loss: 2.2977 - val_acc: 0.2233\n",
      "Epoch 1395/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3515 - acc: 0.4686 - val_loss: 2.3103 - val_acc: 0.2233\n",
      "Epoch 1396/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3525 - acc: 0.4614 - val_loss: 2.2982 - val_acc: 0.2367\n",
      "Epoch 1397/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3507 - acc: 0.4643 - val_loss: 2.3014 - val_acc: 0.2367\n",
      "Epoch 1398/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3513 - acc: 0.4629 - val_loss: 2.3177 - val_acc: 0.2433\n",
      "Epoch 1399/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3507 - acc: 0.4743 - val_loss: 2.3304 - val_acc: 0.2467\n",
      "Epoch 1400/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3506 - acc: 0.4643 - val_loss: 2.3269 - val_acc: 0.2433\n",
      "Epoch 1401/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3513 - acc: 0.4700 - val_loss: 2.3073 - val_acc: 0.2300\n",
      "Epoch 1402/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3506 - acc: 0.4700 - val_loss: 2.3000 - val_acc: 0.2300\n",
      "Epoch 1403/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3492 - acc: 0.4629 - val_loss: 2.3089 - val_acc: 0.2500\n",
      "Epoch 1404/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3510 - acc: 0.4614 - val_loss: 2.3158 - val_acc: 0.2433\n",
      "Epoch 1405/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3506 - acc: 0.4614 - val_loss: 2.3079 - val_acc: 0.2400\n",
      "Epoch 1406/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3511 - acc: 0.4643 - val_loss: 2.2992 - val_acc: 0.2333\n",
      "Epoch 1407/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3506 - acc: 0.4686 - val_loss: 2.3030 - val_acc: 0.2333\n",
      "Epoch 1408/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3502 - acc: 0.4657 - val_loss: 2.3023 - val_acc: 0.2300\n",
      "Epoch 1409/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3500 - acc: 0.4657 - val_loss: 2.2900 - val_acc: 0.2400\n",
      "Epoch 1410/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3505 - acc: 0.4686 - val_loss: 2.3067 - val_acc: 0.2367\n",
      "Epoch 1411/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3499 - acc: 0.4686 - val_loss: 2.3289 - val_acc: 0.2433\n",
      "Epoch 1412/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3510 - acc: 0.4629 - val_loss: 2.2990 - val_acc: 0.2433\n",
      "Epoch 1413/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3502 - acc: 0.4600 - val_loss: 2.3179 - val_acc: 0.2333\n",
      "Epoch 1414/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3510 - acc: 0.4643 - val_loss: 2.2917 - val_acc: 0.2333\n",
      "Epoch 1415/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3498 - acc: 0.4686 - val_loss: 2.3104 - val_acc: 0.2433\n",
      "Epoch 1416/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3500 - acc: 0.4714 - val_loss: 2.3104 - val_acc: 0.2333\n",
      "Epoch 1417/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3504 - acc: 0.4629 - val_loss: 2.3012 - val_acc: 0.2367\n",
      "Epoch 1418/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3490 - acc: 0.4643 - val_loss: 2.2993 - val_acc: 0.2367\n",
      "Epoch 1419/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3501 - acc: 0.4643 - val_loss: 2.2943 - val_acc: 0.2367\n",
      "Epoch 1420/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3496 - acc: 0.4643 - val_loss: 2.3229 - val_acc: 0.2333\n",
      "Epoch 1421/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3499 - acc: 0.4643 - val_loss: 2.3123 - val_acc: 0.2367\n",
      "Epoch 1422/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3484 - acc: 0.4614 - val_loss: 2.3046 - val_acc: 0.2400\n",
      "Epoch 1423/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3491 - acc: 0.4629 - val_loss: 2.3194 - val_acc: 0.2300\n",
      "Epoch 1424/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3500 - acc: 0.4614 - val_loss: 2.3144 - val_acc: 0.2367\n",
      "Epoch 1425/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3482 - acc: 0.4600 - val_loss: 2.3158 - val_acc: 0.2300\n",
      "Epoch 1426/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3489 - acc: 0.4671 - val_loss: 2.3122 - val_acc: 0.2400\n",
      "Epoch 1427/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3484 - acc: 0.4671 - val_loss: 2.3251 - val_acc: 0.2433\n",
      "Epoch 1428/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3488 - acc: 0.4700 - val_loss: 2.3069 - val_acc: 0.2367\n",
      "Epoch 1429/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3489 - acc: 0.4629 - val_loss: 2.2903 - val_acc: 0.2300\n",
      "Epoch 1430/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3487 - acc: 0.4643 - val_loss: 2.3210 - val_acc: 0.2300\n",
      "Epoch 1431/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3480 - acc: 0.4714 - val_loss: 2.2922 - val_acc: 0.2300\n",
      "Epoch 1432/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3485 - acc: 0.4714 - val_loss: 2.3196 - val_acc: 0.2433\n",
      "Epoch 1433/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3492 - acc: 0.4643 - val_loss: 2.3083 - val_acc: 0.2300\n",
      "Epoch 1434/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3491 - acc: 0.4657 - val_loss: 2.3017 - val_acc: 0.2367\n",
      "Epoch 1435/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3494 - acc: 0.4643 - val_loss: 2.3140 - val_acc: 0.2433\n",
      "Epoch 1436/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3466 - acc: 0.4614 - val_loss: 2.3309 - val_acc: 0.2500\n",
      "Epoch 1437/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3488 - acc: 0.4671 - val_loss: 2.3132 - val_acc: 0.2367\n",
      "Epoch 1438/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3489 - acc: 0.4600 - val_loss: 2.3018 - val_acc: 0.2333\n",
      "Epoch 1439/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3724 - acc: 0.460 - ETA: 0s - loss: 1.3482 - acc: 0.4729 - val_loss: 2.3140 - val_acc: 0.2367\n",
      "Epoch 1440/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3484 - acc: 0.4657 - val_loss: 2.3205 - val_acc: 0.2367\n",
      "Epoch 1441/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3480 - acc: 0.4686 - val_loss: 2.3101 - val_acc: 0.2300\n",
      "Epoch 1442/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3483 - acc: 0.4686 - val_loss: 2.3217 - val_acc: 0.2333\n",
      "Epoch 1443/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3476 - acc: 0.4643 - val_loss: 2.3189 - val_acc: 0.2367\n",
      "Epoch 1444/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3482 - acc: 0.4657 - val_loss: 2.3088 - val_acc: 0.2333\n",
      "Epoch 1445/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3470 - acc: 0.4586 - val_loss: 2.3115 - val_acc: 0.2333\n",
      "Epoch 1446/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3472 - acc: 0.4700 - val_loss: 2.3448 - val_acc: 0.2433\n",
      "Epoch 1447/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3484 - acc: 0.4629 - val_loss: 2.3152 - val_acc: 0.2367\n",
      "Epoch 1448/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3465 - acc: 0.4629 - val_loss: 2.3131 - val_acc: 0.2267\n",
      "Epoch 1449/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3469 - acc: 0.4686 - val_loss: 2.2993 - val_acc: 0.2367\n",
      "Epoch 1450/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3475 - acc: 0.4757 - val_loss: 2.3001 - val_acc: 0.2333\n",
      "Epoch 1451/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3468 - acc: 0.4714 - val_loss: 2.3189 - val_acc: 0.2400\n",
      "Epoch 1452/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3471 - acc: 0.4671 - val_loss: 2.3108 - val_acc: 0.2300\n",
      "Epoch 1453/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3467 - acc: 0.4643 - val_loss: 2.3083 - val_acc: 0.2300\n",
      "Epoch 1454/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3470 - acc: 0.4643 - val_loss: 2.3055 - val_acc: 0.2333\n",
      "Epoch 1455/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3471 - acc: 0.4700 - val_loss: 2.3246 - val_acc: 0.2400\n",
      "Epoch 1456/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3471 - acc: 0.4686 - val_loss: 2.3145 - val_acc: 0.2367\n",
      "Epoch 1457/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3468 - acc: 0.4643 - val_loss: 2.3233 - val_acc: 0.2400\n",
      "Epoch 1458/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3470 - acc: 0.4629 - val_loss: 2.3195 - val_acc: 0.2333\n",
      "Epoch 1459/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3467 - acc: 0.4657 - val_loss: 2.3100 - val_acc: 0.2400\n",
      "Epoch 1460/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3458 - acc: 0.4743 - val_loss: 2.3139 - val_acc: 0.2300\n",
      "Epoch 1461/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3457 - acc: 0.4657 - val_loss: 2.3014 - val_acc: 0.2367\n",
      "Epoch 1462/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3463 - acc: 0.4686 - val_loss: 2.3365 - val_acc: 0.2400\n",
      "Epoch 1463/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3464 - acc: 0.4629 - val_loss: 2.3175 - val_acc: 0.2367\n",
      "Epoch 1464/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3464 - acc: 0.4600 - val_loss: 2.3065 - val_acc: 0.2400\n",
      "Epoch 1465/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3464 - acc: 0.4643 - val_loss: 2.3273 - val_acc: 0.2333\n",
      "Epoch 1466/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3460 - acc: 0.4686 - val_loss: 2.3114 - val_acc: 0.2300\n",
      "Epoch 1467/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4700 - val_loss: 2.3216 - val_acc: 0.2433\n",
      "Epoch 1468/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3478 - acc: 0.4686 - val_loss: 2.3153 - val_acc: 0.2433\n",
      "Epoch 1469/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3463 - acc: 0.4714 - val_loss: 2.3385 - val_acc: 0.2433\n",
      "Epoch 1470/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3459 - acc: 0.4686 - val_loss: 2.3195 - val_acc: 0.2400\n",
      "Epoch 1471/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3445 - acc: 0.4686 - val_loss: 2.3193 - val_acc: 0.2433\n",
      "Epoch 1472/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3458 - acc: 0.4686 - val_loss: 2.3335 - val_acc: 0.2367\n",
      "Epoch 1473/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3451 - acc: 0.4671 - val_loss: 2.3081 - val_acc: 0.2300\n",
      "Epoch 1474/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3447 - acc: 0.4700 - val_loss: 2.3182 - val_acc: 0.2400\n",
      "Epoch 1475/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3446 - acc: 0.4671 - val_loss: 2.3281 - val_acc: 0.2333\n",
      "Epoch 1476/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3454 - acc: 0.4686 - val_loss: 2.3167 - val_acc: 0.2333\n",
      "Epoch 1477/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3453 - acc: 0.4657 - val_loss: 2.3125 - val_acc: 0.2367\n",
      "Epoch 1478/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3446 - acc: 0.4729 - val_loss: 2.3053 - val_acc: 0.2367\n",
      "Epoch 1479/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3446 - acc: 0.4600 - val_loss: 2.3184 - val_acc: 0.2333\n",
      "Epoch 1480/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3451 - acc: 0.4671 - val_loss: 2.3022 - val_acc: 0.2333\n",
      "Epoch 1481/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3444 - acc: 0.4700 - val_loss: 2.3216 - val_acc: 0.2267\n",
      "Epoch 1482/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3447 - acc: 0.4671 - val_loss: 2.3399 - val_acc: 0.2367\n",
      "Epoch 1483/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3454 - acc: 0.4643 - val_loss: 2.3291 - val_acc: 0.2333\n",
      "Epoch 1484/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3444 - acc: 0.4629 - val_loss: 2.3232 - val_acc: 0.2300\n",
      "Epoch 1485/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3442 - acc: 0.4686 - val_loss: 2.3356 - val_acc: 0.2333\n",
      "Epoch 1486/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3446 - acc: 0.4700 - val_loss: 2.3120 - val_acc: 0.2367\n",
      "Epoch 1487/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3440 - acc: 0.4657 - val_loss: 2.3316 - val_acc: 0.2433\n",
      "Epoch 1488/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3435 - acc: 0.4600 - val_loss: 2.3068 - val_acc: 0.2267\n",
      "Epoch 1489/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3442 - acc: 0.4629 - val_loss: 2.3358 - val_acc: 0.2333\n",
      "Epoch 1490/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3435 - acc: 0.4700 - val_loss: 2.3331 - val_acc: 0.2400\n",
      "Epoch 1491/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3440 - acc: 0.4671 - val_loss: 2.3383 - val_acc: 0.2400\n",
      "Epoch 1492/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3422 - acc: 0.4686 - val_loss: 2.3365 - val_acc: 0.2300\n",
      "Epoch 1493/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3455 - acc: 0.4600 - val_loss: 2.3263 - val_acc: 0.2333\n",
      "Epoch 1494/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3439 - acc: 0.4743 - val_loss: 2.3217 - val_acc: 0.2400\n",
      "Epoch 1495/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3428 - acc: 0.4671 - val_loss: 2.3173 - val_acc: 0.2367\n",
      "Epoch 1496/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4686 - val_loss: 2.3225 - val_acc: 0.2400\n",
      "Epoch 1497/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3427 - acc: 0.4614 - val_loss: 2.3288 - val_acc: 0.2367\n",
      "Epoch 1498/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4629 - val_loss: 2.3463 - val_acc: 0.2400\n",
      "Epoch 1499/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3438 - acc: 0.4657 - val_loss: 2.3223 - val_acc: 0.2400\n",
      "Epoch 1500/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3433 - acc: 0.4714 - val_loss: 2.3318 - val_acc: 0.2333\n",
      "Epoch 1501/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3433 - acc: 0.4700 - val_loss: 2.3247 - val_acc: 0.2367\n",
      "Epoch 1502/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4700 - val_loss: 2.3430 - val_acc: 0.2400\n",
      "Epoch 1503/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3434 - acc: 0.4629 - val_loss: 2.3251 - val_acc: 0.2333\n",
      "Epoch 1504/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3428 - acc: 0.4743 - val_loss: 2.3315 - val_acc: 0.2400\n",
      "Epoch 1505/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3425 - acc: 0.4614 - val_loss: 2.3274 - val_acc: 0.2367\n",
      "Epoch 1506/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3435 - acc: 0.4729 - val_loss: 2.3197 - val_acc: 0.2367\n",
      "Epoch 1507/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3426 - acc: 0.4671 - val_loss: 2.3260 - val_acc: 0.2367\n",
      "Epoch 1508/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3429 - acc: 0.4614 - val_loss: 2.3400 - val_acc: 0.2433\n",
      "Epoch 1509/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3427 - acc: 0.4714 - val_loss: 2.3274 - val_acc: 0.2333\n",
      "Epoch 1510/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3429 - acc: 0.4743 - val_loss: 2.3535 - val_acc: 0.2433\n",
      "Epoch 1511/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3424 - acc: 0.4729 - val_loss: 2.3510 - val_acc: 0.2433\n",
      "Epoch 1512/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3413 - acc: 0.4671 - val_loss: 2.3204 - val_acc: 0.2433\n",
      "Epoch 1513/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3427 - acc: 0.4686 - val_loss: 2.3124 - val_acc: 0.2367\n",
      "Epoch 1514/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3424 - acc: 0.4671 - val_loss: 2.3344 - val_acc: 0.2367\n",
      "Epoch 1515/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3420 - acc: 0.4729 - val_loss: 2.3439 - val_acc: 0.2400\n",
      "Epoch 1516/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3428 - acc: 0.4743 - val_loss: 2.3326 - val_acc: 0.2333\n",
      "Epoch 1517/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3417 - acc: 0.4743 - val_loss: 2.3189 - val_acc: 0.2400\n",
      "Epoch 1518/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3415 - acc: 0.4714 - val_loss: 2.3151 - val_acc: 0.2400\n",
      "Epoch 1519/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3415 - acc: 0.4700 - val_loss: 2.3182 - val_acc: 0.2300\n",
      "Epoch 1520/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3418 - acc: 0.4714 - val_loss: 2.3405 - val_acc: 0.2433\n",
      "Epoch 1521/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3415 - acc: 0.4743 - val_loss: 2.3239 - val_acc: 0.2367\n",
      "Epoch 1522/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3420 - acc: 0.4729 - val_loss: 2.3272 - val_acc: 0.2433\n",
      "Epoch 1523/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3414 - acc: 0.4729 - val_loss: 2.3182 - val_acc: 0.2400\n",
      "Epoch 1524/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3419 - acc: 0.4743 - val_loss: 2.3340 - val_acc: 0.2400\n",
      "Epoch 1525/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3410 - acc: 0.4729 - val_loss: 2.3412 - val_acc: 0.2433\n",
      "Epoch 1526/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3412 - acc: 0.4771 - val_loss: 2.3416 - val_acc: 0.2433\n",
      "Epoch 1527/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3395 - acc: 0.4686 - val_loss: 2.3321 - val_acc: 0.2367\n",
      "Epoch 1528/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3413 - acc: 0.4771 - val_loss: 2.3406 - val_acc: 0.2400\n",
      "Epoch 1529/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3418 - acc: 0.4614 - val_loss: 2.3383 - val_acc: 0.2333\n",
      "Epoch 1530/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3414 - acc: 0.4614 - val_loss: 2.3298 - val_acc: 0.2300\n",
      "Epoch 1531/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3414 - acc: 0.4700 - val_loss: 2.3393 - val_acc: 0.2333\n",
      "Epoch 1532/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3409 - acc: 0.4771 - val_loss: 2.3242 - val_acc: 0.2400\n",
      "Epoch 1533/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3408 - acc: 0.4729 - val_loss: 2.3329 - val_acc: 0.2367\n",
      "Epoch 1534/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3419 - acc: 0.4714 - val_loss: 2.3230 - val_acc: 0.2367\n",
      "Epoch 1535/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3414 - acc: 0.4700 - val_loss: 2.3399 - val_acc: 0.2333\n",
      "Epoch 1536/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3403 - acc: 0.4671 - val_loss: 2.3328 - val_acc: 0.2333\n",
      "Epoch 1537/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3403 - acc: 0.4714 - val_loss: 2.3342 - val_acc: 0.2367\n",
      "Epoch 1538/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3398 - acc: 0.4757 - val_loss: 2.3217 - val_acc: 0.2333\n",
      "Epoch 1539/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3399 - acc: 0.4686 - val_loss: 2.3153 - val_acc: 0.2400\n",
      "Epoch 1540/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3398 - acc: 0.4671 - val_loss: 2.3282 - val_acc: 0.2400\n",
      "Epoch 1541/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.4657 - val_loss: 2.3424 - val_acc: 0.2400\n",
      "Epoch 1542/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3401 - acc: 0.4700 - val_loss: 2.3382 - val_acc: 0.2333\n",
      "Epoch 1543/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3401 - acc: 0.4757 - val_loss: 2.3313 - val_acc: 0.2300\n",
      "Epoch 1544/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.4786 - val_loss: 2.3333 - val_acc: 0.2333\n",
      "Epoch 1545/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3395 - acc: 0.4729 - val_loss: 2.3223 - val_acc: 0.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1546/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3401 - acc: 0.4843 - val_loss: 2.3361 - val_acc: 0.2333\n",
      "Epoch 1547/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3400 - acc: 0.4714 - val_loss: 2.3459 - val_acc: 0.2333\n",
      "Epoch 1548/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.4657 - val_loss: 2.3395 - val_acc: 0.2433\n",
      "Epoch 1549/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3385 - acc: 0.4729 - val_loss: 2.3493 - val_acc: 0.2367\n",
      "Epoch 1550/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3400 - acc: 0.4729 - val_loss: 2.3523 - val_acc: 0.2333\n",
      "Epoch 1551/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3394 - acc: 0.4729 - val_loss: 2.3301 - val_acc: 0.2333\n",
      "Epoch 1552/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3394 - acc: 0.4714 - val_loss: 2.3447 - val_acc: 0.2433\n",
      "Epoch 1553/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3393 - acc: 0.4786 - val_loss: 2.3462 - val_acc: 0.2367\n",
      "Epoch 1554/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3398 - acc: 0.4629 - val_loss: 2.3372 - val_acc: 0.2333\n",
      "Epoch 1555/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3394 - acc: 0.4700 - val_loss: 2.3379 - val_acc: 0.2333\n",
      "Epoch 1556/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3386 - acc: 0.4714 - val_loss: 2.3311 - val_acc: 0.2400\n",
      "Epoch 1557/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3384 - acc: 0.4714 - val_loss: 2.3362 - val_acc: 0.2367\n",
      "Epoch 1558/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3386 - acc: 0.4700 - val_loss: 2.3145 - val_acc: 0.2367\n",
      "Epoch 1559/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3391 - acc: 0.4714 - val_loss: 2.3461 - val_acc: 0.2433\n",
      "Epoch 1560/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3392 - acc: 0.4700 - val_loss: 2.3335 - val_acc: 0.2400\n",
      "Epoch 1561/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3382 - acc: 0.4671 - val_loss: 2.3396 - val_acc: 0.2433\n",
      "Epoch 1562/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3390 - acc: 0.4643 - val_loss: 2.3618 - val_acc: 0.2367\n",
      "Epoch 1563/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3394 - acc: 0.4686 - val_loss: 2.3417 - val_acc: 0.2400\n",
      "Epoch 1564/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3372 - acc: 0.4700 - val_loss: 2.3339 - val_acc: 0.2367\n",
      "Epoch 1565/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3394 - acc: 0.4700 - val_loss: 2.3338 - val_acc: 0.2400\n",
      "Epoch 1566/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3381 - acc: 0.4686 - val_loss: 2.3353 - val_acc: 0.2367\n",
      "Epoch 1567/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3386 - acc: 0.4757 - val_loss: 2.3411 - val_acc: 0.2400\n",
      "Epoch 1568/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3383 - acc: 0.4743 - val_loss: 2.3351 - val_acc: 0.2333\n",
      "Epoch 1569/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3386 - acc: 0.4757 - val_loss: 2.3575 - val_acc: 0.2400\n",
      "Epoch 1570/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3374 - acc: 0.4729 - val_loss: 2.3627 - val_acc: 0.2333\n",
      "Epoch 1571/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3390 - acc: 0.4686 - val_loss: 2.3516 - val_acc: 0.2333\n",
      "Epoch 1572/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3383 - acc: 0.4743 - val_loss: 2.3178 - val_acc: 0.2367\n",
      "Epoch 1573/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3380 - acc: 0.4771 - val_loss: 2.3588 - val_acc: 0.2333\n",
      "Epoch 1574/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3385 - acc: 0.4671 - val_loss: 2.3379 - val_acc: 0.2333\n",
      "Epoch 1575/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3372 - acc: 0.4757 - val_loss: 2.3330 - val_acc: 0.2400\n",
      "Epoch 1576/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3374 - acc: 0.4700 - val_loss: 2.3541 - val_acc: 0.2400\n",
      "Epoch 1577/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3384 - acc: 0.4686 - val_loss: 2.3388 - val_acc: 0.2367\n",
      "Epoch 1578/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3365 - acc: 0.4743 - val_loss: 2.3395 - val_acc: 0.2433\n",
      "Epoch 1579/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3376 - acc: 0.4686 - val_loss: 2.3396 - val_acc: 0.2367\n",
      "Epoch 1580/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3368 - acc: 0.4786 - val_loss: 2.3485 - val_acc: 0.2433\n",
      "Epoch 1581/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3376 - acc: 0.4686 - val_loss: 2.3523 - val_acc: 0.2333\n",
      "Epoch 1582/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3376 - acc: 0.4771 - val_loss: 2.3485 - val_acc: 0.2333\n",
      "Epoch 1583/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3364 - acc: 0.4714 - val_loss: 2.3439 - val_acc: 0.2433\n",
      "Epoch 1584/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3375 - acc: 0.4671 - val_loss: 2.3545 - val_acc: 0.2400\n",
      "Epoch 1585/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3374 - acc: 0.4771 - val_loss: 2.3439 - val_acc: 0.2400\n",
      "Epoch 1586/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3363 - acc: 0.4714 - val_loss: 2.3564 - val_acc: 0.2400\n",
      "Epoch 1587/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3366 - acc: 0.4686 - val_loss: 2.3584 - val_acc: 0.2333\n",
      "Epoch 1588/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3366 - acc: 0.4700 - val_loss: 2.3316 - val_acc: 0.2400\n",
      "Epoch 1589/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3370 - acc: 0.4729 - val_loss: 2.3595 - val_acc: 0.2333\n",
      "Epoch 1590/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3363 - acc: 0.4700 - val_loss: 2.3474 - val_acc: 0.2400\n",
      "Epoch 1591/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3369 - acc: 0.4729 - val_loss: 2.3390 - val_acc: 0.2400\n",
      "Epoch 1592/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3364 - acc: 0.4671 - val_loss: 2.3501 - val_acc: 0.2400\n",
      "Epoch 1593/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3352 - acc: 0.4700 - val_loss: 2.3615 - val_acc: 0.2333\n",
      "Epoch 1594/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3367 - acc: 0.4686 - val_loss: 2.3483 - val_acc: 0.2367\n",
      "Epoch 1595/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3355 - acc: 0.4700 - val_loss: 2.3435 - val_acc: 0.2400\n",
      "Epoch 1596/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3360 - acc: 0.4729 - val_loss: 2.3303 - val_acc: 0.2367\n",
      "Epoch 1597/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3361 - acc: 0.4714 - val_loss: 2.3409 - val_acc: 0.2367\n",
      "Epoch 1598/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3368 - acc: 0.4743 - val_loss: 2.3478 - val_acc: 0.2333\n",
      "Epoch 1599/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3357 - acc: 0.4714 - val_loss: 2.3472 - val_acc: 0.2400\n",
      "Epoch 1600/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3359 - acc: 0.4686 - val_loss: 2.3544 - val_acc: 0.2367\n",
      "Epoch 1601/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3364 - acc: 0.4729 - val_loss: 2.3390 - val_acc: 0.2400\n",
      "Epoch 1602/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3355 - acc: 0.4743 - val_loss: 2.3554 - val_acc: 0.2433\n",
      "Epoch 1603/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3356 - acc: 0.4757 - val_loss: 2.3578 - val_acc: 0.2400\n",
      "Epoch 1604/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3354 - acc: 0.4686 - val_loss: 2.3525 - val_acc: 0.2400\n",
      "Epoch 1605/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3359 - acc: 0.4743 - val_loss: 2.3688 - val_acc: 0.2400\n",
      "Epoch 1606/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3355 - acc: 0.4643 - val_loss: 2.3430 - val_acc: 0.2333\n",
      "Epoch 1607/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3366 - acc: 0.4757 - val_loss: 2.3427 - val_acc: 0.2367\n",
      "Epoch 1608/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3340 - acc: 0.4686 - val_loss: 2.3486 - val_acc: 0.2400\n",
      "Epoch 1609/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3354 - acc: 0.4771 - val_loss: 2.3602 - val_acc: 0.2400\n",
      "Epoch 1610/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3359 - acc: 0.4671 - val_loss: 2.3479 - val_acc: 0.2367\n",
      "Epoch 1611/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3345 - acc: 0.4757 - val_loss: 2.3634 - val_acc: 0.2300\n",
      "Epoch 1612/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3351 - acc: 0.4686 - val_loss: 2.3515 - val_acc: 0.2367\n",
      "Epoch 1613/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3346 - acc: 0.4729 - val_loss: 2.3535 - val_acc: 0.2333\n",
      "Epoch 1614/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3351 - acc: 0.4629 - val_loss: 2.3511 - val_acc: 0.2333\n",
      "Epoch 1615/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3354 - acc: 0.4800 - val_loss: 2.3661 - val_acc: 0.2333\n",
      "Epoch 1616/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3349 - acc: 0.4771 - val_loss: 2.3566 - val_acc: 0.2400\n",
      "Epoch 1617/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3339 - acc: 0.4771 - val_loss: 2.3409 - val_acc: 0.2367\n",
      "Epoch 1618/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3354 - acc: 0.4729 - val_loss: 2.3540 - val_acc: 0.2367\n",
      "Epoch 1619/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3344 - acc: 0.4729 - val_loss: 2.3534 - val_acc: 0.2400\n",
      "Epoch 1620/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3340 - acc: 0.4757 - val_loss: 2.3690 - val_acc: 0.2367\n",
      "Epoch 1621/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3346 - acc: 0.4771 - val_loss: 2.3755 - val_acc: 0.2300\n",
      "Epoch 1622/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3341 - acc: 0.4743 - val_loss: 2.3377 - val_acc: 0.2400\n",
      "Epoch 1623/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3336 - acc: 0.4686 - val_loss: 2.3470 - val_acc: 0.2333\n",
      "Epoch 1624/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3327 - acc: 0.4757 - val_loss: 2.3580 - val_acc: 0.2433\n",
      "Epoch 1625/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3348 - acc: 0.4743 - val_loss: 2.3517 - val_acc: 0.2367\n",
      "Epoch 1626/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3339 - acc: 0.4657 - val_loss: 2.3689 - val_acc: 0.2333\n",
      "Epoch 1627/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3343 - acc: 0.4714 - val_loss: 2.3795 - val_acc: 0.2467\n",
      "Epoch 1628/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3338 - acc: 0.4771 - val_loss: 2.3641 - val_acc: 0.2400\n",
      "Epoch 1629/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3343 - acc: 0.4700 - val_loss: 2.3363 - val_acc: 0.2333\n",
      "Epoch 1630/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3339 - acc: 0.4729 - val_loss: 2.3579 - val_acc: 0.2400\n",
      "Epoch 1631/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3348 - acc: 0.4671 - val_loss: 2.3556 - val_acc: 0.2367\n",
      "Epoch 1632/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3337 - acc: 0.4786 - val_loss: 2.3758 - val_acc: 0.2433\n",
      "Epoch 1633/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3341 - acc: 0.4714 - val_loss: 2.3576 - val_acc: 0.2433\n",
      "Epoch 1634/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3335 - acc: 0.4800 - val_loss: 2.3479 - val_acc: 0.2400\n",
      "Epoch 1635/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3330 - acc: 0.4700 - val_loss: 2.3692 - val_acc: 0.2333\n",
      "Epoch 1636/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3336 - acc: 0.4743 - val_loss: 2.3699 - val_acc: 0.2433\n",
      "Epoch 1637/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3314 - acc: 0.4743 - val_loss: 2.3675 - val_acc: 0.2433\n",
      "Epoch 1638/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3336 - acc: 0.4771 - val_loss: 2.3653 - val_acc: 0.2433\n",
      "Epoch 1639/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3328 - acc: 0.4671 - val_loss: 2.3700 - val_acc: 0.2333\n",
      "Epoch 1640/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3329 - acc: 0.4700 - val_loss: 2.3814 - val_acc: 0.2433\n",
      "Epoch 1641/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3335 - acc: 0.4800 - val_loss: 2.3620 - val_acc: 0.2367\n",
      "Epoch 1642/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3336 - acc: 0.4771 - val_loss: 2.3595 - val_acc: 0.2400\n",
      "Epoch 1643/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3331 - acc: 0.4757 - val_loss: 2.3730 - val_acc: 0.2400\n",
      "Epoch 1644/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3327 - acc: 0.4700 - val_loss: 2.3662 - val_acc: 0.2333\n",
      "Epoch 1645/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3312 - acc: 0.4800 - val_loss: 2.3673 - val_acc: 0.2433\n",
      "Epoch 1646/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3317 - acc: 0.4643 - val_loss: 2.3481 - val_acc: 0.2400\n",
      "Epoch 1647/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3320 - acc: 0.4814 - val_loss: 2.3781 - val_acc: 0.2433\n",
      "Epoch 1648/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3325 - acc: 0.4743 - val_loss: 2.3690 - val_acc: 0.2400\n",
      "Epoch 1649/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3317 - acc: 0.4743 - val_loss: 2.3664 - val_acc: 0.2300\n",
      "Epoch 1650/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3313 - acc: 0.4771 - val_loss: 2.3623 - val_acc: 0.2400\n",
      "Epoch 1651/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3334 - acc: 0.4743 - val_loss: 2.3516 - val_acc: 0.2400\n",
      "Epoch 1652/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3321 - acc: 0.4729 - val_loss: 2.3606 - val_acc: 0.2367\n",
      "Epoch 1653/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3314 - acc: 0.4686 - val_loss: 2.3897 - val_acc: 0.2467\n",
      "Epoch 1654/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3344 - acc: 0.4700 - val_loss: 2.3579 - val_acc: 0.2400\n",
      "Epoch 1655/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3314 - acc: 0.4757 - val_loss: 2.3618 - val_acc: 0.2433\n",
      "Epoch 1656/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3328 - acc: 0.4743 - val_loss: 2.3602 - val_acc: 0.2400\n",
      "Epoch 1657/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3324 - acc: 0.4757 - val_loss: 2.3537 - val_acc: 0.2400\n",
      "Epoch 1658/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3321 - acc: 0.4757 - val_loss: 2.3689 - val_acc: 0.2333\n",
      "Epoch 1659/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3320 - acc: 0.4800 - val_loss: 2.3570 - val_acc: 0.2400\n",
      "Epoch 1660/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3317 - acc: 0.4714 - val_loss: 2.3684 - val_acc: 0.2333\n",
      "Epoch 1661/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3313 - acc: 0.4714 - val_loss: 2.3495 - val_acc: 0.2400\n",
      "Epoch 1662/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3315 - acc: 0.4757 - val_loss: 2.3672 - val_acc: 0.2400\n",
      "Epoch 1663/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3318 - acc: 0.4771 - val_loss: 2.3764 - val_acc: 0.2400\n",
      "Epoch 1664/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3320 - acc: 0.4743 - val_loss: 2.3652 - val_acc: 0.2367\n",
      "Epoch 1665/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3310 - acc: 0.4643 - val_loss: 2.3602 - val_acc: 0.2400\n",
      "Epoch 1666/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3311 - acc: 0.4786 - val_loss: 2.3620 - val_acc: 0.2400\n",
      "Epoch 1667/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3308 - acc: 0.4786 - val_loss: 2.3491 - val_acc: 0.2333\n",
      "Epoch 1668/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3312 - acc: 0.4714 - val_loss: 2.3557 - val_acc: 0.2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1669/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3313 - acc: 0.4771 - val_loss: 2.3668 - val_acc: 0.2367\n",
      "Epoch 1670/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3304 - acc: 0.4786 - val_loss: 2.3799 - val_acc: 0.2400\n",
      "Epoch 1671/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3312 - acc: 0.4700 - val_loss: 2.3781 - val_acc: 0.2333\n",
      "Epoch 1672/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3306 - acc: 0.4729 - val_loss: 2.3678 - val_acc: 0.2333\n",
      "Epoch 1673/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3304 - acc: 0.4771 - val_loss: 2.3902 - val_acc: 0.2400\n",
      "Epoch 1674/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3297 - acc: 0.4786 - val_loss: 2.3869 - val_acc: 0.2333\n",
      "Epoch 1675/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3307 - acc: 0.4700 - val_loss: 2.3620 - val_acc: 0.2333\n",
      "Epoch 1676/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3310 - acc: 0.4729 - val_loss: 2.3710 - val_acc: 0.2367\n",
      "Epoch 1677/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3294 - acc: 0.4743 - val_loss: 2.3565 - val_acc: 0.2433\n",
      "Epoch 1678/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3309 - acc: 0.4743 - val_loss: 2.3897 - val_acc: 0.2433\n",
      "Epoch 1679/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3302 - acc: 0.4700 - val_loss: 2.3748 - val_acc: 0.2400\n",
      "Epoch 1680/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3299 - acc: 0.4814 - val_loss: 2.3722 - val_acc: 0.2400\n",
      "Epoch 1681/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3305 - acc: 0.4714 - val_loss: 2.3759 - val_acc: 0.2333\n",
      "Epoch 1682/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3307 - acc: 0.4743 - val_loss: 2.3733 - val_acc: 0.2367\n",
      "Epoch 1683/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3303 - acc: 0.4800 - val_loss: 2.3676 - val_acc: 0.2400\n",
      "Epoch 1684/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3290 - acc: 0.4757 - val_loss: 2.3937 - val_acc: 0.2433\n",
      "Epoch 1685/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3304 - acc: 0.4757 - val_loss: 2.3618 - val_acc: 0.2367\n",
      "Epoch 1686/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3297 - acc: 0.4743 - val_loss: 2.4012 - val_acc: 0.2367\n",
      "Epoch 1687/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3304 - acc: 0.4786 - val_loss: 2.3801 - val_acc: 0.2367\n",
      "Epoch 1688/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3299 - acc: 0.4743 - val_loss: 2.3711 - val_acc: 0.2400\n",
      "Epoch 1689/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3308 - acc: 0.4729 - val_loss: 2.3781 - val_acc: 0.2400\n",
      "Epoch 1690/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3304 - acc: 0.4771 - val_loss: 2.3709 - val_acc: 0.2333\n",
      "Epoch 1691/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3288 - acc: 0.4771 - val_loss: 2.3683 - val_acc: 0.2433\n",
      "Epoch 1692/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3297 - acc: 0.4743 - val_loss: 2.3779 - val_acc: 0.2333\n",
      "Epoch 1693/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3294 - acc: 0.4757 - val_loss: 2.3859 - val_acc: 0.2333\n",
      "Epoch 1694/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3295 - acc: 0.4757 - val_loss: 2.3922 - val_acc: 0.2433\n",
      "Epoch 1695/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3297 - acc: 0.4757 - val_loss: 2.3756 - val_acc: 0.2400\n",
      "Epoch 1696/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3291 - acc: 0.4729 - val_loss: 2.3687 - val_acc: 0.2400\n",
      "Epoch 1697/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3288 - acc: 0.4814 - val_loss: 2.3862 - val_acc: 0.2433\n",
      "Epoch 1698/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3286 - acc: 0.4757 - val_loss: 2.3853 - val_acc: 0.2367\n",
      "Epoch 1699/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3293 - acc: 0.4757 - val_loss: 2.3930 - val_acc: 0.2433\n",
      "Epoch 1700/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3287 - acc: 0.4757 - val_loss: 2.3928 - val_acc: 0.2467\n",
      "Epoch 1701/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3286 - acc: 0.4786 - val_loss: 2.3804 - val_acc: 0.2400\n",
      "Epoch 1702/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3291 - acc: 0.4771 - val_loss: 2.3654 - val_acc: 0.2367\n",
      "Epoch 1703/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3287 - acc: 0.4700 - val_loss: 2.3761 - val_acc: 0.2333\n",
      "Epoch 1704/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3282 - acc: 0.4729 - val_loss: 2.3772 - val_acc: 0.2367\n",
      "Epoch 1705/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3288 - acc: 0.4743 - val_loss: 2.3958 - val_acc: 0.2400\n",
      "Epoch 1706/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3287 - acc: 0.4729 - val_loss: 2.3643 - val_acc: 0.2400\n",
      "Epoch 1707/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3289 - acc: 0.4729 - val_loss: 2.3792 - val_acc: 0.2400\n",
      "Epoch 1708/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3272 - acc: 0.4671 - val_loss: 2.3696 - val_acc: 0.2333\n",
      "Epoch 1709/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3275 - acc: 0.4743 - val_loss: 2.3683 - val_acc: 0.2400\n",
      "Epoch 1710/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3277 - acc: 0.4686 - val_loss: 2.3626 - val_acc: 0.2400\n",
      "Epoch 1711/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3271 - acc: 0.4786 - val_loss: 2.3737 - val_acc: 0.2400\n",
      "Epoch 1712/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3286 - acc: 0.4729 - val_loss: 2.3723 - val_acc: 0.2367\n",
      "Epoch 1713/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3269 - acc: 0.4686 - val_loss: 2.4148 - val_acc: 0.2367\n",
      "Epoch 1714/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3290 - acc: 0.4743 - val_loss: 2.3598 - val_acc: 0.2367\n",
      "Epoch 1715/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3272 - acc: 0.4771 - val_loss: 2.3984 - val_acc: 0.2400\n",
      "Epoch 1716/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3272 - acc: 0.4786 - val_loss: 2.3995 - val_acc: 0.2433\n",
      "Epoch 1717/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3274 - acc: 0.4743 - val_loss: 2.3808 - val_acc: 0.2400\n",
      "Epoch 1718/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3279 - acc: 0.4857 - val_loss: 2.3823 - val_acc: 0.2400\n",
      "Epoch 1719/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3279 - acc: 0.4814 - val_loss: 2.3462 - val_acc: 0.2333\n",
      "Epoch 1720/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3262 - acc: 0.4771 - val_loss: 2.3728 - val_acc: 0.2400\n",
      "Epoch 1721/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3273 - acc: 0.4686 - val_loss: 2.3608 - val_acc: 0.2400\n",
      "Epoch 1722/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3281 - acc: 0.4729 - val_loss: 2.3534 - val_acc: 0.2333\n",
      "Epoch 1723/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3271 - acc: 0.4800 - val_loss: 2.3631 - val_acc: 0.2367\n",
      "Epoch 1724/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3259 - acc: 0.4800 - val_loss: 2.3781 - val_acc: 0.2400\n",
      "Epoch 1725/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3266 - acc: 0.4757 - val_loss: 2.3900 - val_acc: 0.2367\n",
      "Epoch 1726/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3265 - acc: 0.4800 - val_loss: 2.3836 - val_acc: 0.2367\n",
      "Epoch 1727/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3271 - acc: 0.4757 - val_loss: 2.3786 - val_acc: 0.2367\n",
      "Epoch 1728/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3261 - acc: 0.4729 - val_loss: 2.3827 - val_acc: 0.2400\n",
      "Epoch 1729/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3271 - acc: 0.4757 - val_loss: 2.3683 - val_acc: 0.2400\n",
      "Epoch 1730/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3259 - acc: 0.4786 - val_loss: 2.3668 - val_acc: 0.2367\n",
      "Epoch 1731/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3261 - acc: 0.4829 - val_loss: 2.3953 - val_acc: 0.2367\n",
      "Epoch 1732/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3260 - acc: 0.4829 - val_loss: 2.3899 - val_acc: 0.2333\n",
      "Epoch 1733/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3249 - acc: 0.4814 - val_loss: 2.3935 - val_acc: 0.2367\n",
      "Epoch 1734/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3259 - acc: 0.4743 - val_loss: 2.3819 - val_acc: 0.2333\n",
      "Epoch 1735/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3263 - acc: 0.4757 - val_loss: 2.3709 - val_acc: 0.2400\n",
      "Epoch 1736/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3257 - acc: 0.4786 - val_loss: 2.3866 - val_acc: 0.2333\n",
      "Epoch 1737/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3241 - acc: 0.4843 - val_loss: 2.3838 - val_acc: 0.2400\n",
      "Epoch 1738/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3255 - acc: 0.4814 - val_loss: 2.4060 - val_acc: 0.2400\n",
      "Epoch 1739/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3257 - acc: 0.4771 - val_loss: 2.3744 - val_acc: 0.2400\n",
      "Epoch 1740/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3255 - acc: 0.4800 - val_loss: 2.3985 - val_acc: 0.2367\n",
      "Epoch 1741/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3251 - acc: 0.4843 - val_loss: 2.3640 - val_acc: 0.2400\n",
      "Epoch 1742/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3252 - acc: 0.4800 - val_loss: 2.3956 - val_acc: 0.2367\n",
      "Epoch 1743/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3257 - acc: 0.4800 - val_loss: 2.3956 - val_acc: 0.2367\n",
      "Epoch 1744/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3253 - acc: 0.4729 - val_loss: 2.3904 - val_acc: 0.2333\n",
      "Epoch 1745/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3250 - acc: 0.4814 - val_loss: 2.3793 - val_acc: 0.2333\n",
      "Epoch 1746/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3255 - acc: 0.4729 - val_loss: 2.4013 - val_acc: 0.2333\n",
      "Epoch 1747/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3249 - acc: 0.4757 - val_loss: 2.3904 - val_acc: 0.2367\n",
      "Epoch 1748/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3243 - acc: 0.4743 - val_loss: 2.3881 - val_acc: 0.2333\n",
      "Epoch 1749/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3251 - acc: 0.4771 - val_loss: 2.3928 - val_acc: 0.2367\n",
      "Epoch 1750/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3237 - acc: 0.4771 - val_loss: 2.3592 - val_acc: 0.2400\n",
      "Epoch 1751/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3250 - acc: 0.4771 - val_loss: 2.3543 - val_acc: 0.2367\n",
      "Epoch 1752/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3235 - acc: 0.4771 - val_loss: 2.3747 - val_acc: 0.2367\n",
      "Epoch 1753/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3249 - acc: 0.4786 - val_loss: 2.4015 - val_acc: 0.2400\n",
      "Epoch 1754/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3246 - acc: 0.4829 - val_loss: 2.3986 - val_acc: 0.2367\n",
      "Epoch 1755/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4771 - val_loss: 2.3994 - val_acc: 0.2400\n",
      "Epoch 1756/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3246 - acc: 0.4771 - val_loss: 2.3767 - val_acc: 0.2367\n",
      "Epoch 1757/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3243 - acc: 0.4786 - val_loss: 2.4111 - val_acc: 0.2367\n",
      "Epoch 1758/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3241 - acc: 0.4843 - val_loss: 2.3754 - val_acc: 0.2367\n",
      "Epoch 1759/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3241 - acc: 0.4700 - val_loss: 2.4233 - val_acc: 0.2400\n",
      "Epoch 1760/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3244 - acc: 0.4757 - val_loss: 2.3826 - val_acc: 0.2433\n",
      "Epoch 1761/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3248 - acc: 0.4800 - val_loss: 2.3768 - val_acc: 0.2400\n",
      "Epoch 1762/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3238 - acc: 0.4786 - val_loss: 2.4110 - val_acc: 0.2367\n",
      "Epoch 1763/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3245 - acc: 0.4771 - val_loss: 2.3897 - val_acc: 0.2367\n",
      "Epoch 1764/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3228 - acc: 0.4800 - val_loss: 2.3856 - val_acc: 0.2333\n",
      "Epoch 1765/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3242 - acc: 0.4800 - val_loss: 2.3836 - val_acc: 0.2333\n",
      "Epoch 1766/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3232 - acc: 0.4786 - val_loss: 2.3789 - val_acc: 0.2433\n",
      "Epoch 1767/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3235 - acc: 0.4771 - val_loss: 2.3808 - val_acc: 0.2400\n",
      "Epoch 1768/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3236 - acc: 0.4843 - val_loss: 2.3857 - val_acc: 0.2367\n",
      "Epoch 1769/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3232 - acc: 0.4814 - val_loss: 2.3890 - val_acc: 0.2400\n",
      "Epoch 1770/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3237 - acc: 0.4857 - val_loss: 2.3966 - val_acc: 0.2333\n",
      "Epoch 1771/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4757 - val_loss: 2.3806 - val_acc: 0.2367\n",
      "Epoch 1772/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3234 - acc: 0.4757 - val_loss: 2.4008 - val_acc: 0.2367\n",
      "Epoch 1773/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4814 - val_loss: 2.3903 - val_acc: 0.2333\n",
      "Epoch 1774/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3234 - acc: 0.4843 - val_loss: 2.3843 - val_acc: 0.2367\n",
      "Epoch 1775/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3235 - acc: 0.4843 - val_loss: 2.4074 - val_acc: 0.2367\n",
      "Epoch 1776/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3232 - acc: 0.4800 - val_loss: 2.3756 - val_acc: 0.2400\n",
      "Epoch 1777/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3224 - acc: 0.4771 - val_loss: 2.3928 - val_acc: 0.2300\n",
      "Epoch 1778/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3228 - acc: 0.4814 - val_loss: 2.3730 - val_acc: 0.2367\n",
      "Epoch 1779/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3230 - acc: 0.4857 - val_loss: 2.3924 - val_acc: 0.2333\n",
      "Epoch 1780/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4771 - val_loss: 2.4115 - val_acc: 0.2400\n",
      "Epoch 1781/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3222 - acc: 0.4814 - val_loss: 2.4094 - val_acc: 0.2433\n",
      "Epoch 1782/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3230 - acc: 0.4671 - val_loss: 2.3642 - val_acc: 0.2333\n",
      "Epoch 1783/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3226 - acc: 0.4814 - val_loss: 2.4054 - val_acc: 0.2433\n",
      "Epoch 1784/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3220 - acc: 0.4814 - val_loss: 2.3911 - val_acc: 0.2367\n",
      "Epoch 1785/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3228 - acc: 0.4800 - val_loss: 2.3770 - val_acc: 0.2367\n",
      "Epoch 1786/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3230 - acc: 0.4829 - val_loss: 2.3872 - val_acc: 0.2400\n",
      "Epoch 1787/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3221 - acc: 0.4857 - val_loss: 2.3852 - val_acc: 0.2367\n",
      "Epoch 1788/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3228 - acc: 0.4771 - val_loss: 2.3913 - val_acc: 0.2300\n",
      "Epoch 1789/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3224 - acc: 0.4757 - val_loss: 2.3726 - val_acc: 0.2367\n",
      "Epoch 1790/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3222 - acc: 0.4814 - val_loss: 2.3666 - val_acc: 0.2367\n",
      "Epoch 1791/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3230 - acc: 0.4786 - val_loss: 2.3861 - val_acc: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1792/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3215 - acc: 0.4757 - val_loss: 2.4122 - val_acc: 0.2367\n",
      "Epoch 1793/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3221 - acc: 0.4800 - val_loss: 2.4003 - val_acc: 0.2333\n",
      "Epoch 1794/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3215 - acc: 0.4829 - val_loss: 2.3929 - val_acc: 0.2333\n",
      "Epoch 1795/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3223 - acc: 0.4771 - val_loss: 2.4034 - val_acc: 0.2333\n",
      "Epoch 1796/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3216 - acc: 0.4757 - val_loss: 2.4208 - val_acc: 0.2367\n",
      "Epoch 1797/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3210 - acc: 0.4800 - val_loss: 2.4064 - val_acc: 0.2367\n",
      "Epoch 1798/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3222 - acc: 0.4800 - val_loss: 2.3873 - val_acc: 0.2333\n",
      "Epoch 1799/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3216 - acc: 0.4829 - val_loss: 2.4253 - val_acc: 0.2333\n",
      "Epoch 1800/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3221 - acc: 0.4829 - val_loss: 2.4100 - val_acc: 0.2433\n",
      "Epoch 1801/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3210 - acc: 0.4786 - val_loss: 2.3867 - val_acc: 0.2333\n",
      "Epoch 1802/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3210 - acc: 0.4857 - val_loss: 2.4199 - val_acc: 0.2400\n",
      "Epoch 1803/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3212 - acc: 0.4829 - val_loss: 2.3928 - val_acc: 0.2367\n",
      "Epoch 1804/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3215 - acc: 0.4871 - val_loss: 2.3924 - val_acc: 0.2333\n",
      "Epoch 1805/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3215 - acc: 0.4829 - val_loss: 2.3847 - val_acc: 0.2367\n",
      "Epoch 1806/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3215 - acc: 0.4800 - val_loss: 2.3927 - val_acc: 0.2300\n",
      "Epoch 1807/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3209 - acc: 0.4814 - val_loss: 2.4045 - val_acc: 0.2333\n",
      "Epoch 1808/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3204 - acc: 0.4743 - val_loss: 2.4098 - val_acc: 0.2367\n",
      "Epoch 1809/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3208 - acc: 0.4814 - val_loss: 2.3784 - val_acc: 0.2367\n",
      "Epoch 1810/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3212 - acc: 0.4829 - val_loss: 2.3946 - val_acc: 0.2333\n",
      "Epoch 1811/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3201 - acc: 0.4871 - val_loss: 2.3936 - val_acc: 0.2300\n",
      "Epoch 1812/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3201 - acc: 0.4843 - val_loss: 2.3912 - val_acc: 0.2433\n",
      "Epoch 1813/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3212 - acc: 0.4757 - val_loss: 2.3934 - val_acc: 0.2367\n",
      "Epoch 1814/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3211 - acc: 0.4743 - val_loss: 2.3958 - val_acc: 0.2400\n",
      "Epoch 1815/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3204 - acc: 0.4857 - val_loss: 2.4098 - val_acc: 0.2367\n",
      "Epoch 1816/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3196 - acc: 0.4786 - val_loss: 2.3969 - val_acc: 0.2367\n",
      "Epoch 1817/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3200 - acc: 0.4800 - val_loss: 2.3942 - val_acc: 0.2367\n",
      "Epoch 1818/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3210 - acc: 0.4814 - val_loss: 2.3969 - val_acc: 0.2333\n",
      "Epoch 1819/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3205 - acc: 0.4857 - val_loss: 2.4070 - val_acc: 0.2400\n",
      "Epoch 1820/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3213 - acc: 0.4771 - val_loss: 2.3932 - val_acc: 0.2367\n",
      "Epoch 1821/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3199 - acc: 0.4786 - val_loss: 2.4362 - val_acc: 0.2367\n",
      "Epoch 1822/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3200 - acc: 0.4814 - val_loss: 2.4076 - val_acc: 0.2367\n",
      "Epoch 1823/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3194 - acc: 0.4771 - val_loss: 2.4322 - val_acc: 0.2367\n",
      "Epoch 1824/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3209 - acc: 0.4757 - val_loss: 2.4089 - val_acc: 0.2433\n",
      "Epoch 1825/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3199 - acc: 0.4814 - val_loss: 2.4105 - val_acc: 0.2367\n",
      "Epoch 1826/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3200 - acc: 0.4757 - val_loss: 2.3842 - val_acc: 0.2400\n",
      "Epoch 1827/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3201 - acc: 0.4829 - val_loss: 2.4066 - val_acc: 0.2367\n",
      "Epoch 1828/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3184 - acc: 0.4829 - val_loss: 2.3964 - val_acc: 0.2367\n",
      "Epoch 1829/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3209 - acc: 0.4757 - val_loss: 2.3985 - val_acc: 0.2333\n",
      "Epoch 1830/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3189 - acc: 0.4857 - val_loss: 2.4098 - val_acc: 0.2367\n",
      "Epoch 1831/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3195 - acc: 0.4829 - val_loss: 2.4013 - val_acc: 0.2333\n",
      "Epoch 1832/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3193 - acc: 0.4814 - val_loss: 2.4148 - val_acc: 0.2400\n",
      "Epoch 1833/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3195 - acc: 0.4771 - val_loss: 2.4211 - val_acc: 0.2367\n",
      "Epoch 1834/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3195 - acc: 0.4743 - val_loss: 2.4135 - val_acc: 0.2367\n",
      "Epoch 1835/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3192 - acc: 0.4857 - val_loss: 2.4155 - val_acc: 0.2333\n",
      "Epoch 1836/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3197 - acc: 0.4886 - val_loss: 2.3991 - val_acc: 0.2367\n",
      "Epoch 1837/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3192 - acc: 0.4800 - val_loss: 2.3985 - val_acc: 0.2367\n",
      "Epoch 1838/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3188 - acc: 0.4829 - val_loss: 2.3979 - val_acc: 0.2333\n",
      "Epoch 1839/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3180 - acc: 0.4771 - val_loss: 2.4323 - val_acc: 0.2400\n",
      "Epoch 1840/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3196 - acc: 0.4914 - val_loss: 2.4004 - val_acc: 0.2333\n",
      "Epoch 1841/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3175 - acc: 0.4843 - val_loss: 2.3862 - val_acc: 0.2400\n",
      "Epoch 1842/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3189 - acc: 0.4814 - val_loss: 2.4223 - val_acc: 0.2333\n",
      "Epoch 1843/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3186 - acc: 0.4814 - val_loss: 2.4063 - val_acc: 0.2367\n",
      "Epoch 1844/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3187 - acc: 0.4786 - val_loss: 2.4195 - val_acc: 0.2400\n",
      "Epoch 1845/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3182 - acc: 0.4829 - val_loss: 2.3933 - val_acc: 0.2367\n",
      "Epoch 1846/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3187 - acc: 0.4857 - val_loss: 2.4017 - val_acc: 0.2400\n",
      "Epoch 1847/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3192 - acc: 0.4843 - val_loss: 2.3783 - val_acc: 0.2367\n",
      "Epoch 1848/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3189 - acc: 0.4843 - val_loss: 2.4085 - val_acc: 0.2367\n",
      "Epoch 1849/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3186 - acc: 0.4914 - val_loss: 2.3949 - val_acc: 0.2300\n",
      "Epoch 1850/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3184 - acc: 0.4843 - val_loss: 2.4155 - val_acc: 0.2367\n",
      "Epoch 1851/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3185 - acc: 0.4814 - val_loss: 2.4109 - val_acc: 0.2400\n",
      "Epoch 1852/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3182 - acc: 0.4829 - val_loss: 2.4268 - val_acc: 0.2400\n",
      "Epoch 1853/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3186 - acc: 0.4786 - val_loss: 2.4020 - val_acc: 0.2400\n",
      "Epoch 1854/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3177 - acc: 0.4857 - val_loss: 2.4106 - val_acc: 0.2367\n",
      "Epoch 1855/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3180 - acc: 0.4843 - val_loss: 2.4059 - val_acc: 0.2367\n",
      "Epoch 1856/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3166 - acc: 0.4814 - val_loss: 2.4082 - val_acc: 0.2400\n",
      "Epoch 1857/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3187 - acc: 0.4800 - val_loss: 2.4053 - val_acc: 0.2333\n",
      "Epoch 1858/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3181 - acc: 0.4786 - val_loss: 2.4216 - val_acc: 0.2400\n",
      "Epoch 1859/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3168 - acc: 0.4814 - val_loss: 2.3994 - val_acc: 0.2367\n",
      "Epoch 1860/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3174 - acc: 0.4771 - val_loss: 2.4162 - val_acc: 0.2400\n",
      "Epoch 1861/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3175 - acc: 0.4843 - val_loss: 2.4298 - val_acc: 0.2367\n",
      "Epoch 1862/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3179 - acc: 0.4871 - val_loss: 2.4095 - val_acc: 0.2433\n",
      "Epoch 1863/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3178 - acc: 0.4786 - val_loss: 2.4085 - val_acc: 0.2333\n",
      "Epoch 1864/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3170 - acc: 0.4871 - val_loss: 2.4121 - val_acc: 0.2367\n",
      "Epoch 1865/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3169 - acc: 0.4829 - val_loss: 2.4351 - val_acc: 0.2367\n",
      "Epoch 1866/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3174 - acc: 0.4857 - val_loss: 2.4154 - val_acc: 0.2300\n",
      "Epoch 1867/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3175 - acc: 0.4857 - val_loss: 2.4224 - val_acc: 0.2400\n",
      "Epoch 1868/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3171 - acc: 0.4829 - val_loss: 2.4182 - val_acc: 0.2333\n",
      "Epoch 1869/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3166 - acc: 0.4900 - val_loss: 2.4271 - val_acc: 0.2433\n",
      "Epoch 1870/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3177 - acc: 0.4886 - val_loss: 2.3974 - val_acc: 0.2367\n",
      "Epoch 1871/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3171 - acc: 0.4829 - val_loss: 2.4174 - val_acc: 0.2367\n",
      "Epoch 1872/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3171 - acc: 0.4857 - val_loss: 2.4059 - val_acc: 0.2300\n",
      "Epoch 1873/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3166 - acc: 0.4800 - val_loss: 2.4456 - val_acc: 0.2433\n",
      "Epoch 1874/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3164 - acc: 0.4886 - val_loss: 2.4240 - val_acc: 0.2367\n",
      "Epoch 1875/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3170 - acc: 0.4900 - val_loss: 2.4333 - val_acc: 0.2433\n",
      "Epoch 1876/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3164 - acc: 0.4800 - val_loss: 2.4150 - val_acc: 0.2333\n",
      "Epoch 1877/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3170 - acc: 0.4786 - val_loss: 2.4181 - val_acc: 0.2367\n",
      "Epoch 1878/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3165 - acc: 0.4857 - val_loss: 2.4272 - val_acc: 0.2433\n",
      "Epoch 1879/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3167 - acc: 0.4886 - val_loss: 2.4313 - val_acc: 0.2367\n",
      "Epoch 1880/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3166 - acc: 0.4871 - val_loss: 2.4203 - val_acc: 0.2333\n",
      "Epoch 1881/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3161 - acc: 0.4871 - val_loss: 2.4483 - val_acc: 0.2400\n",
      "Epoch 1882/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3165 - acc: 0.4786 - val_loss: 2.3931 - val_acc: 0.2400\n",
      "Epoch 1883/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3167 - acc: 0.4786 - val_loss: 2.4213 - val_acc: 0.2300\n",
      "Epoch 1884/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3164 - acc: 0.4857 - val_loss: 2.4111 - val_acc: 0.2333\n",
      "Epoch 1885/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3160 - acc: 0.4843 - val_loss: 2.4161 - val_acc: 0.2367\n",
      "Epoch 1886/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3165 - acc: 0.4786 - val_loss: 2.4059 - val_acc: 0.2333\n",
      "Epoch 1887/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3160 - acc: 0.4843 - val_loss: 2.4207 - val_acc: 0.2367\n",
      "Epoch 1888/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3148 - acc: 0.4843 - val_loss: 2.4287 - val_acc: 0.2433\n",
      "Epoch 1889/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3160 - acc: 0.4829 - val_loss: 2.4075 - val_acc: 0.2367\n",
      "Epoch 1890/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3157 - acc: 0.4886 - val_loss: 2.4254 - val_acc: 0.2367\n",
      "Epoch 1891/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3158 - acc: 0.4857 - val_loss: 2.4243 - val_acc: 0.2400\n",
      "Epoch 1892/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3159 - acc: 0.4857 - val_loss: 2.4045 - val_acc: 0.2367\n",
      "Epoch 1893/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3152 - acc: 0.4843 - val_loss: 2.4382 - val_acc: 0.2367\n",
      "Epoch 1894/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3156 - acc: 0.4857 - val_loss: 2.4090 - val_acc: 0.2300\n",
      "Epoch 1895/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3157 - acc: 0.4886 - val_loss: 2.4127 - val_acc: 0.2333\n",
      "Epoch 1896/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3155 - acc: 0.4771 - val_loss: 2.4424 - val_acc: 0.2367\n",
      "Epoch 1897/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3156 - acc: 0.4857 - val_loss: 2.4043 - val_acc: 0.2367\n",
      "Epoch 1898/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3155 - acc: 0.4814 - val_loss: 2.4048 - val_acc: 0.2333\n",
      "Epoch 1899/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3152 - acc: 0.4800 - val_loss: 2.4176 - val_acc: 0.2400\n",
      "Epoch 1900/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3151 - acc: 0.4857 - val_loss: 2.4413 - val_acc: 0.2400\n",
      "Epoch 1901/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3154 - acc: 0.4800 - val_loss: 2.4100 - val_acc: 0.2333\n",
      "Epoch 1902/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3154 - acc: 0.4800 - val_loss: 2.4401 - val_acc: 0.2367\n",
      "Epoch 1903/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3142 - acc: 0.4871 - val_loss: 2.4133 - val_acc: 0.2333\n",
      "Epoch 1904/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3151 - acc: 0.4900 - val_loss: 2.4399 - val_acc: 0.2400\n",
      "Epoch 1905/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3155 - acc: 0.4886 - val_loss: 2.4341 - val_acc: 0.2367\n",
      "Epoch 1906/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3162 - acc: 0.4829 - val_loss: 2.4293 - val_acc: 0.2333\n",
      "Epoch 1907/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3144 - acc: 0.4857 - val_loss: 2.4316 - val_acc: 0.2367\n",
      "Epoch 1908/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3159 - acc: 0.4829 - val_loss: 2.4429 - val_acc: 0.2367\n",
      "Epoch 1909/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3143 - acc: 0.4757 - val_loss: 2.4278 - val_acc: 0.2367\n",
      "Epoch 1910/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3148 - acc: 0.4800 - val_loss: 2.4264 - val_acc: 0.2367\n",
      "Epoch 1911/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3145 - acc: 0.4886 - val_loss: 2.4190 - val_acc: 0.2333\n",
      "Epoch 1912/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3148 - acc: 0.4829 - val_loss: 2.4299 - val_acc: 0.2400\n",
      "Epoch 1913/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3144 - acc: 0.4886 - val_loss: 2.4294 - val_acc: 0.2333\n",
      "Epoch 1914/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3129 - acc: 0.4786 - val_loss: 2.4334 - val_acc: 0.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1915/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3141 - acc: 0.4829 - val_loss: 2.4219 - val_acc: 0.2333\n",
      "Epoch 1916/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3140 - acc: 0.4857 - val_loss: 2.4144 - val_acc: 0.2333\n",
      "Epoch 1917/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3148 - acc: 0.4857 - val_loss: 2.4209 - val_acc: 0.2367\n",
      "Epoch 1918/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3141 - acc: 0.4886 - val_loss: 2.4355 - val_acc: 0.2400\n",
      "Epoch 1919/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3143 - acc: 0.4800 - val_loss: 2.4321 - val_acc: 0.2367\n",
      "Epoch 1920/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3154 - acc: 0.4886 - val_loss: 2.4299 - val_acc: 0.2333\n",
      "Epoch 1921/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3139 - acc: 0.4843 - val_loss: 2.4287 - val_acc: 0.2333\n",
      "Epoch 1922/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3132 - acc: 0.4843 - val_loss: 2.4196 - val_acc: 0.2333\n",
      "Epoch 1923/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3142 - acc: 0.4943 - val_loss: 2.4347 - val_acc: 0.2400\n",
      "Epoch 1924/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3136 - acc: 0.4786 - val_loss: 2.4037 - val_acc: 0.2367\n",
      "Epoch 1925/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3136 - acc: 0.4786 - val_loss: 2.4440 - val_acc: 0.2367\n",
      "Epoch 1926/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3132 - acc: 0.4829 - val_loss: 2.4074 - val_acc: 0.2333\n",
      "Epoch 1927/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3136 - acc: 0.4900 - val_loss: 2.4200 - val_acc: 0.2367\n",
      "Epoch 1928/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3129 - acc: 0.4829 - val_loss: 2.4242 - val_acc: 0.2367\n",
      "Epoch 1929/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3133 - acc: 0.4843 - val_loss: 2.4466 - val_acc: 0.2400\n",
      "Epoch 1930/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3141 - acc: 0.4814 - val_loss: 2.4289 - val_acc: 0.2367\n",
      "Epoch 1931/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3138 - acc: 0.4886 - val_loss: 2.4229 - val_acc: 0.2333\n",
      "Epoch 1932/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3133 - acc: 0.4857 - val_loss: 2.4358 - val_acc: 0.2400\n",
      "Epoch 1933/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3130 - acc: 0.4871 - val_loss: 2.4320 - val_acc: 0.2367\n",
      "Epoch 1934/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3134 - acc: 0.4800 - val_loss: 2.4334 - val_acc: 0.2400\n",
      "Epoch 1935/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3130 - acc: 0.4886 - val_loss: 2.4275 - val_acc: 0.2333\n",
      "Epoch 1936/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3130 - acc: 0.4843 - val_loss: 2.4496 - val_acc: 0.2400\n",
      "Epoch 1937/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3123 - acc: 0.4914 - val_loss: 2.4379 - val_acc: 0.2400\n",
      "Epoch 1938/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3129 - acc: 0.4814 - val_loss: 2.4474 - val_acc: 0.2367\n",
      "Epoch 1939/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3131 - acc: 0.4886 - val_loss: 2.4408 - val_acc: 0.2400\n",
      "Epoch 1940/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3131 - acc: 0.4886 - val_loss: 2.4372 - val_acc: 0.2400\n",
      "Epoch 1941/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3123 - acc: 0.4857 - val_loss: 2.4248 - val_acc: 0.2367\n",
      "Epoch 1942/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3127 - acc: 0.4871 - val_loss: 2.4438 - val_acc: 0.2400\n",
      "Epoch 1943/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3124 - acc: 0.4843 - val_loss: 2.4419 - val_acc: 0.2400\n",
      "Epoch 1944/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3124 - acc: 0.4886 - val_loss: 2.4494 - val_acc: 0.2433\n",
      "Epoch 1945/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3122 - acc: 0.4857 - val_loss: 2.4308 - val_acc: 0.2367\n",
      "Epoch 1946/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3127 - acc: 0.4857 - val_loss: 2.4431 - val_acc: 0.2400\n",
      "Epoch 1947/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3130 - acc: 0.4843 - val_loss: 2.4223 - val_acc: 0.2367\n",
      "Epoch 1948/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3116 - acc: 0.4914 - val_loss: 2.4430 - val_acc: 0.2400\n",
      "Epoch 1949/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3133 - acc: 0.4843 - val_loss: 2.4423 - val_acc: 0.2367\n",
      "Epoch 1950/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3132 - acc: 0.4829 - val_loss: 2.4327 - val_acc: 0.2333\n",
      "Epoch 1951/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3118 - acc: 0.4786 - val_loss: 2.4506 - val_acc: 0.2400\n",
      "Epoch 1952/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3123 - acc: 0.4814 - val_loss: 2.4555 - val_acc: 0.2400\n",
      "Epoch 1953/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3121 - acc: 0.4886 - val_loss: 2.4510 - val_acc: 0.2400\n",
      "Epoch 1954/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3120 - acc: 0.4857 - val_loss: 2.4280 - val_acc: 0.2367\n",
      "Epoch 1955/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3124 - acc: 0.4843 - val_loss: 2.4392 - val_acc: 0.2400\n",
      "Epoch 1956/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3118 - acc: 0.4900 - val_loss: 2.4444 - val_acc: 0.2400\n",
      "Epoch 1957/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3115 - acc: 0.4914 - val_loss: 2.4096 - val_acc: 0.2333\n",
      "Epoch 1958/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3127 - acc: 0.4857 - val_loss: 2.4210 - val_acc: 0.2367\n",
      "Epoch 1959/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3117 - acc: 0.4829 - val_loss: 2.4276 - val_acc: 0.2333\n",
      "Epoch 1960/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3121 - acc: 0.4829 - val_loss: 2.4377 - val_acc: 0.2367\n",
      "Epoch 1961/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3115 - acc: 0.4829 - val_loss: 2.4540 - val_acc: 0.2400\n",
      "Epoch 1962/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3116 - acc: 0.4914 - val_loss: 2.4242 - val_acc: 0.2400\n",
      "Epoch 1963/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3112 - acc: 0.4843 - val_loss: 2.4426 - val_acc: 0.2400\n",
      "Epoch 1964/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3119 - acc: 0.4829 - val_loss: 2.4399 - val_acc: 0.2367\n",
      "Epoch 1965/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3111 - acc: 0.4800 - val_loss: 2.4421 - val_acc: 0.2400\n",
      "Epoch 1966/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3115 - acc: 0.4857 - val_loss: 2.4452 - val_acc: 0.2367\n",
      "Epoch 1967/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3113 - acc: 0.4871 - val_loss: 2.4418 - val_acc: 0.2367\n",
      "Epoch 1968/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3107 - acc: 0.4857 - val_loss: 2.4522 - val_acc: 0.2367\n",
      "Epoch 1969/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3119 - acc: 0.4900 - val_loss: 2.4539 - val_acc: 0.2400\n",
      "Epoch 1970/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3112 - acc: 0.4800 - val_loss: 2.4481 - val_acc: 0.2400\n",
      "Epoch 1971/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3112 - acc: 0.4871 - val_loss: 2.4468 - val_acc: 0.2367\n",
      "Epoch 1972/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3098 - acc: 0.4886 - val_loss: 2.4406 - val_acc: 0.2433\n",
      "Epoch 1973/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3116 - acc: 0.4914 - val_loss: 2.4487 - val_acc: 0.2400\n",
      "Epoch 1974/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3108 - acc: 0.4843 - val_loss: 2.4221 - val_acc: 0.2333\n",
      "Epoch 1975/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3107 - acc: 0.4886 - val_loss: 2.4341 - val_acc: 0.2367\n",
      "Epoch 1976/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3112 - acc: 0.4800 - val_loss: 2.4466 - val_acc: 0.2433\n",
      "Epoch 1977/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3108 - acc: 0.4914 - val_loss: 2.4353 - val_acc: 0.2333\n",
      "Epoch 1978/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3111 - acc: 0.4900 - val_loss: 2.4432 - val_acc: 0.2367\n",
      "Epoch 1979/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3101 - acc: 0.4814 - val_loss: 2.4667 - val_acc: 0.2367\n",
      "Epoch 1980/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3090 - acc: 0.4857 - val_loss: 2.4322 - val_acc: 0.2367\n",
      "Epoch 1981/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3105 - acc: 0.4843 - val_loss: 2.4351 - val_acc: 0.2400\n",
      "Epoch 1982/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3102 - acc: 0.4929 - val_loss: 2.4291 - val_acc: 0.2367\n",
      "Epoch 1983/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3103 - acc: 0.4829 - val_loss: 2.4395 - val_acc: 0.2367\n",
      "Epoch 1984/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3107 - acc: 0.4857 - val_loss: 2.4389 - val_acc: 0.2333\n",
      "Epoch 1985/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3100 - acc: 0.4900 - val_loss: 2.4565 - val_acc: 0.2400\n",
      "Epoch 1986/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3109 - acc: 0.4829 - val_loss: 2.4522 - val_acc: 0.2367\n",
      "Epoch 1987/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3102 - acc: 0.4814 - val_loss: 2.4308 - val_acc: 0.2333\n",
      "Epoch 1988/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3117 - acc: 0.4843 - val_loss: 2.4597 - val_acc: 0.2367\n",
      "Epoch 1989/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3103 - acc: 0.4943 - val_loss: 2.4456 - val_acc: 0.2333\n",
      "Epoch 1990/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3098 - acc: 0.4829 - val_loss: 2.4387 - val_acc: 0.2367\n",
      "Epoch 1991/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3097 - acc: 0.4900 - val_loss: 2.4433 - val_acc: 0.2400\n",
      "Epoch 1992/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3092 - acc: 0.4871 - val_loss: 2.4718 - val_acc: 0.2367\n",
      "Epoch 1993/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3103 - acc: 0.4871 - val_loss: 2.4148 - val_acc: 0.2367\n",
      "Epoch 1994/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3106 - acc: 0.4914 - val_loss: 2.4454 - val_acc: 0.2367\n",
      "Epoch 1995/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3095 - acc: 0.4829 - val_loss: 2.4603 - val_acc: 0.2400\n",
      "Epoch 1996/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3093 - acc: 0.4857 - val_loss: 2.4625 - val_acc: 0.2400\n",
      "Epoch 1997/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3095 - acc: 0.4886 - val_loss: 2.4286 - val_acc: 0.2333\n",
      "Epoch 1998/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3089 - acc: 0.4871 - val_loss: 2.4761 - val_acc: 0.2400\n",
      "Epoch 1999/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3101 - acc: 0.4857 - val_loss: 2.4668 - val_acc: 0.2400\n",
      "Epoch 2000/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3095 - acc: 0.4829 - val_loss: 2.4504 - val_acc: 0.2400\n",
      "Epoch 2001/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3093 - acc: 0.4900 - val_loss: 2.4183 - val_acc: 0.2333\n",
      "Epoch 2002/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3096 - acc: 0.4857 - val_loss: 2.4490 - val_acc: 0.2400\n",
      "Epoch 2003/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3099 - acc: 0.4857 - val_loss: 2.4701 - val_acc: 0.2367\n",
      "Epoch 2004/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3097 - acc: 0.4786 - val_loss: 2.4397 - val_acc: 0.2400\n",
      "Epoch 2005/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3089 - acc: 0.4814 - val_loss: 2.4572 - val_acc: 0.2400\n",
      "Epoch 2006/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3089 - acc: 0.4857 - val_loss: 2.4647 - val_acc: 0.2367\n",
      "Epoch 2007/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3079 - acc: 0.4929 - val_loss: 2.4445 - val_acc: 0.2333\n",
      "Epoch 2008/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3084 - acc: 0.4886 - val_loss: 2.4711 - val_acc: 0.2400\n",
      "Epoch 2009/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3098 - acc: 0.4900 - val_loss: 2.4372 - val_acc: 0.2333\n",
      "Epoch 2010/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3087 - acc: 0.4886 - val_loss: 2.4424 - val_acc: 0.2367\n",
      "Epoch 2011/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3091 - acc: 0.4843 - val_loss: 2.4607 - val_acc: 0.2433\n",
      "Epoch 2012/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3090 - acc: 0.4857 - val_loss: 2.4338 - val_acc: 0.2367\n",
      "Epoch 2013/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3082 - acc: 0.4871 - val_loss: 2.4481 - val_acc: 0.2400\n",
      "Epoch 2014/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3083 - acc: 0.4929 - val_loss: 2.4836 - val_acc: 0.2333\n",
      "Epoch 2015/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3081 - acc: 0.4871 - val_loss: 2.4414 - val_acc: 0.2367\n",
      "Epoch 2016/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3084 - acc: 0.4829 - val_loss: 2.4705 - val_acc: 0.2400\n",
      "Epoch 2017/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3082 - acc: 0.4886 - val_loss: 2.4417 - val_acc: 0.2367\n",
      "Epoch 2018/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3079 - acc: 0.4871 - val_loss: 2.4603 - val_acc: 0.2400\n",
      "Epoch 2019/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3086 - acc: 0.4857 - val_loss: 2.4278 - val_acc: 0.2333\n",
      "Epoch 2020/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3085 - acc: 0.4871 - val_loss: 2.4604 - val_acc: 0.2367\n",
      "Epoch 2021/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3082 - acc: 0.4886 - val_loss: 2.4624 - val_acc: 0.2367\n",
      "Epoch 2022/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3084 - acc: 0.4857 - val_loss: 2.4460 - val_acc: 0.2333\n",
      "Epoch 2023/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3087 - acc: 0.4871 - val_loss: 2.4338 - val_acc: 0.2333\n",
      "Epoch 2024/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3089 - acc: 0.4871 - val_loss: 2.4552 - val_acc: 0.2433\n",
      "Epoch 2025/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3084 - acc: 0.4900 - val_loss: 2.4550 - val_acc: 0.2367\n",
      "Epoch 2026/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3081 - acc: 0.4871 - val_loss: 2.4389 - val_acc: 0.2333\n",
      "Epoch 2027/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3084 - acc: 0.4914 - val_loss: 2.4433 - val_acc: 0.2333\n",
      "Epoch 2028/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3077 - acc: 0.4886 - val_loss: 2.4786 - val_acc: 0.2400\n",
      "Epoch 2029/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3070 - acc: 0.4843 - val_loss: 2.4285 - val_acc: 0.2333\n",
      "Epoch 2030/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3071 - acc: 0.4943 - val_loss: 2.4826 - val_acc: 0.2433\n",
      "Epoch 2031/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3073 - acc: 0.4857 - val_loss: 2.4230 - val_acc: 0.2333\n",
      "Epoch 2032/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3082 - acc: 0.4886 - val_loss: 2.4514 - val_acc: 0.2400\n",
      "Epoch 2033/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3075 - acc: 0.4900 - val_loss: 2.4700 - val_acc: 0.2400\n",
      "Epoch 2034/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3082 - acc: 0.4829 - val_loss: 2.4363 - val_acc: 0.2333\n",
      "Epoch 2035/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3071 - acc: 0.4843 - val_loss: 2.4435 - val_acc: 0.2333\n",
      "Epoch 2036/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3080 - acc: 0.4900 - val_loss: 2.4469 - val_acc: 0.2367\n",
      "Epoch 2037/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3067 - acc: 0.4957 - val_loss: 2.4430 - val_acc: 0.2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2038/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3070 - acc: 0.4871 - val_loss: 2.4506 - val_acc: 0.2367\n",
      "Epoch 2039/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3078 - acc: 0.4929 - val_loss: 2.4689 - val_acc: 0.2367\n",
      "Epoch 2040/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3074 - acc: 0.4900 - val_loss: 2.4616 - val_acc: 0.2433\n",
      "Epoch 2041/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3065 - acc: 0.4886 - val_loss: 2.4634 - val_acc: 0.2433\n",
      "Epoch 2042/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3073 - acc: 0.4900 - val_loss: 2.4773 - val_acc: 0.2367\n",
      "Epoch 2043/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3068 - acc: 0.4871 - val_loss: 2.4730 - val_acc: 0.2433\n",
      "Epoch 2044/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3074 - acc: 0.4871 - val_loss: 2.4568 - val_acc: 0.2400\n",
      "Epoch 2045/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3075 - acc: 0.4929 - val_loss: 2.4828 - val_acc: 0.2367\n",
      "Epoch 2046/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3073 - acc: 0.4886 - val_loss: 2.4380 - val_acc: 0.2333\n",
      "Epoch 2047/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3062 - acc: 0.4886 - val_loss: 2.4565 - val_acc: 0.2400\n",
      "Epoch 2048/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3062 - acc: 0.4886 - val_loss: 2.4372 - val_acc: 0.2333\n",
      "Epoch 2049/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3079 - acc: 0.4886 - val_loss: 2.4796 - val_acc: 0.2367\n",
      "Epoch 2050/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3066 - acc: 0.4857 - val_loss: 2.4684 - val_acc: 0.2400\n",
      "Epoch 2051/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3073 - acc: 0.4886 - val_loss: 2.4544 - val_acc: 0.2367\n",
      "Epoch 2052/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3072 - acc: 0.4914 - val_loss: 2.4582 - val_acc: 0.2367\n",
      "Epoch 2053/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3071 - acc: 0.4800 - val_loss: 2.4550 - val_acc: 0.2333\n",
      "Epoch 2054/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3063 - acc: 0.4857 - val_loss: 2.4515 - val_acc: 0.2400\n",
      "Epoch 2055/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3071 - acc: 0.4871 - val_loss: 2.4490 - val_acc: 0.2367\n",
      "Epoch 2056/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3058 - acc: 0.4857 - val_loss: 2.4669 - val_acc: 0.2400\n",
      "Epoch 2057/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3069 - acc: 0.4857 - val_loss: 2.4646 - val_acc: 0.2433\n",
      "Epoch 2058/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3054 - acc: 0.4914 - val_loss: 2.4794 - val_acc: 0.2400\n",
      "Epoch 2059/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3063 - acc: 0.4857 - val_loss: 2.4615 - val_acc: 0.2367\n",
      "Epoch 2060/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3060 - acc: 0.4900 - val_loss: 2.4678 - val_acc: 0.2367\n",
      "Epoch 2061/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3062 - acc: 0.4886 - val_loss: 2.4622 - val_acc: 0.2367\n",
      "Epoch 2062/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3062 - acc: 0.4886 - val_loss: 2.4641 - val_acc: 0.2400\n",
      "Epoch 2063/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3057 - acc: 0.4929 - val_loss: 2.4627 - val_acc: 0.2367\n",
      "Epoch 2064/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3059 - acc: 0.4857 - val_loss: 2.4611 - val_acc: 0.2400\n",
      "Epoch 2065/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3058 - acc: 0.4886 - val_loss: 2.4733 - val_acc: 0.2333\n",
      "Epoch 2066/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3066 - acc: 0.4886 - val_loss: 2.4668 - val_acc: 0.2367\n",
      "Epoch 2067/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3060 - acc: 0.4886 - val_loss: 2.4634 - val_acc: 0.2333\n",
      "Epoch 2068/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3055 - acc: 0.4871 - val_loss: 2.4654 - val_acc: 0.2367\n",
      "Epoch 2069/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3052 - acc: 0.4943 - val_loss: 2.5022 - val_acc: 0.2400\n",
      "Epoch 2070/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3057 - acc: 0.4900 - val_loss: 2.4642 - val_acc: 0.2333\n",
      "Epoch 2071/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3058 - acc: 0.4886 - val_loss: 2.4737 - val_acc: 0.2367\n",
      "Epoch 2072/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3054 - acc: 0.4857 - val_loss: 2.4794 - val_acc: 0.2400\n",
      "Epoch 2073/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3051 - acc: 0.4886 - val_loss: 2.4921 - val_acc: 0.2400\n",
      "Epoch 2074/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3050 - acc: 0.4871 - val_loss: 2.4398 - val_acc: 0.2333\n",
      "Epoch 2075/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3055 - acc: 0.4857 - val_loss: 2.4555 - val_acc: 0.2400\n",
      "Epoch 2076/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3054 - acc: 0.4857 - val_loss: 2.4592 - val_acc: 0.2400\n",
      "Epoch 2077/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3050 - acc: 0.4900 - val_loss: 2.4698 - val_acc: 0.2400\n",
      "Epoch 2078/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4857 - val_loss: 2.4642 - val_acc: 0.2400\n",
      "Epoch 2079/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3052 - acc: 0.4929 - val_loss: 2.4840 - val_acc: 0.2400\n",
      "Epoch 2080/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3054 - acc: 0.4857 - val_loss: 2.4769 - val_acc: 0.2333\n",
      "Epoch 2081/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3060 - acc: 0.4886 - val_loss: 2.4870 - val_acc: 0.2300\n",
      "Epoch 2082/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3050 - acc: 0.4814 - val_loss: 2.4437 - val_acc: 0.2333\n",
      "Epoch 2083/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3062 - acc: 0.4843 - val_loss: 2.4644 - val_acc: 0.2367\n",
      "Epoch 2084/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3040 - acc: 0.4871 - val_loss: 2.4534 - val_acc: 0.2400\n",
      "Epoch 2085/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4829 - val_loss: 2.4582 - val_acc: 0.2367\n",
      "Epoch 2086/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3047 - acc: 0.4871 - val_loss: 2.4548 - val_acc: 0.2367\n",
      "Epoch 2087/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3046 - acc: 0.4929 - val_loss: 2.4871 - val_acc: 0.2400\n",
      "Epoch 2088/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3051 - acc: 0.4857 - val_loss: 2.4557 - val_acc: 0.2400\n",
      "Epoch 2089/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4814 - val_loss: 2.4728 - val_acc: 0.2367\n",
      "Epoch 2090/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3044 - acc: 0.4929 - val_loss: 2.4902 - val_acc: 0.2367\n",
      "Epoch 2091/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3040 - acc: 0.4871 - val_loss: 2.4634 - val_acc: 0.2367\n",
      "Epoch 2092/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3042 - acc: 0.4871 - val_loss: 2.4817 - val_acc: 0.2400\n",
      "Epoch 2093/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3046 - acc: 0.4871 - val_loss: 2.4755 - val_acc: 0.2400\n",
      "Epoch 2094/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3045 - acc: 0.4886 - val_loss: 2.4841 - val_acc: 0.2333\n",
      "Epoch 2095/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3035 - acc: 0.4900 - val_loss: 2.4891 - val_acc: 0.2333\n",
      "Epoch 2096/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3043 - acc: 0.4871 - val_loss: 2.4682 - val_acc: 0.2367\n",
      "Epoch 2097/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3034 - acc: 0.4857 - val_loss: 2.4665 - val_acc: 0.2433\n",
      "Epoch 2098/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3031 - acc: 0.4900 - val_loss: 2.4568 - val_acc: 0.2400\n",
      "Epoch 2099/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.3042 - acc: 0.4871 - val_loss: 2.4679 - val_acc: 0.2400\n",
      "Epoch 2100/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3037 - acc: 0.4857 - val_loss: 2.4655 - val_acc: 0.2367\n",
      "Epoch 2101/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3044 - acc: 0.4929 - val_loss: 2.4576 - val_acc: 0.2367\n",
      "Epoch 2102/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3046 - acc: 0.4914 - val_loss: 2.4772 - val_acc: 0.2400\n",
      "Epoch 2103/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3036 - acc: 0.4843 - val_loss: 2.4801 - val_acc: 0.2367\n",
      "Epoch 2104/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3039 - acc: 0.4914 - val_loss: 2.4531 - val_acc: 0.2333\n",
      "Epoch 2105/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3033 - acc: 0.4871 - val_loss: 2.4764 - val_acc: 0.2400\n",
      "Epoch 2106/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3043 - acc: 0.4886 - val_loss: 2.4516 - val_acc: 0.2333\n",
      "Epoch 2107/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3034 - acc: 0.4914 - val_loss: 2.4607 - val_acc: 0.2367\n",
      "Epoch 2108/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3037 - acc: 0.4857 - val_loss: 2.4801 - val_acc: 0.2400\n",
      "Epoch 2109/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3037 - acc: 0.4914 - val_loss: 2.4728 - val_acc: 0.2367\n",
      "Epoch 2110/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3046 - acc: 0.4914 - val_loss: 2.4696 - val_acc: 0.2433\n",
      "Epoch 2111/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4914 - val_loss: 2.4598 - val_acc: 0.2367\n",
      "Epoch 2112/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3029 - acc: 0.4886 - val_loss: 2.5002 - val_acc: 0.2333\n",
      "Epoch 2113/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3039 - acc: 0.4886 - val_loss: 2.4729 - val_acc: 0.2367\n",
      "Epoch 2114/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3026 - acc: 0.4914 - val_loss: 2.4629 - val_acc: 0.2433\n",
      "Epoch 2115/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3033 - acc: 0.4886 - val_loss: 2.4908 - val_acc: 0.2367\n",
      "Epoch 2116/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3037 - acc: 0.4943 - val_loss: 2.4755 - val_acc: 0.2433\n",
      "Epoch 2117/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3035 - acc: 0.4886 - val_loss: 2.4889 - val_acc: 0.2333\n",
      "Epoch 2118/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3027 - acc: 0.4914 - val_loss: 2.4755 - val_acc: 0.2400\n",
      "Epoch 2119/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3030 - acc: 0.4871 - val_loss: 2.4736 - val_acc: 0.2400\n",
      "Epoch 2120/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3025 - acc: 0.4871 - val_loss: 2.4760 - val_acc: 0.2367\n",
      "Epoch 2121/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3027 - acc: 0.4929 - val_loss: 2.4974 - val_acc: 0.2333\n",
      "Epoch 2122/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3029 - acc: 0.4886 - val_loss: 2.4889 - val_acc: 0.2367\n",
      "Epoch 2123/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3026 - acc: 0.4957 - val_loss: 2.4655 - val_acc: 0.2333\n",
      "Epoch 2124/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3030 - acc: 0.4929 - val_loss: 2.4691 - val_acc: 0.2367\n",
      "Epoch 2125/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3027 - acc: 0.4900 - val_loss: 2.4687 - val_acc: 0.2367\n",
      "Epoch 2126/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3030 - acc: 0.4886 - val_loss: 2.4731 - val_acc: 0.2400\n",
      "Epoch 2127/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3031 - acc: 0.4843 - val_loss: 2.4606 - val_acc: 0.2400\n",
      "Epoch 2128/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3030 - acc: 0.4943 - val_loss: 2.4869 - val_acc: 0.2333\n",
      "Epoch 2129/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3026 - acc: 0.4857 - val_loss: 2.4755 - val_acc: 0.2367\n",
      "Epoch 2130/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3025 - acc: 0.4900 - val_loss: 2.4755 - val_acc: 0.2400\n",
      "Epoch 2131/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3025 - acc: 0.4900 - val_loss: 2.4694 - val_acc: 0.2333\n",
      "Epoch 2132/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3034 - acc: 0.4886 - val_loss: 2.4765 - val_acc: 0.2300\n",
      "Epoch 2133/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3021 - acc: 0.4886 - val_loss: 2.4797 - val_acc: 0.2400\n",
      "Epoch 2134/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3024 - acc: 0.4886 - val_loss: 2.4735 - val_acc: 0.2367\n",
      "Epoch 2135/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3021 - acc: 0.4857 - val_loss: 2.4753 - val_acc: 0.2367\n",
      "Epoch 2136/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3025 - acc: 0.4857 - val_loss: 2.4570 - val_acc: 0.2367\n",
      "Epoch 2137/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3018 - acc: 0.4914 - val_loss: 2.4814 - val_acc: 0.2367\n",
      "Epoch 2138/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3032 - acc: 0.4886 - val_loss: 2.4828 - val_acc: 0.2367\n",
      "Epoch 2139/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3018 - acc: 0.4914 - val_loss: 2.4511 - val_acc: 0.2300\n",
      "Epoch 2140/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3020 - acc: 0.4914 - val_loss: 2.4845 - val_acc: 0.2300\n",
      "Epoch 2141/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3023 - acc: 0.4900 - val_loss: 2.4773 - val_acc: 0.2367\n",
      "Epoch 2142/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4871 - val_loss: 2.4779 - val_acc: 0.2367\n",
      "Epoch 2143/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3022 - acc: 0.4871 - val_loss: 2.4926 - val_acc: 0.2367\n",
      "Epoch 2144/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3021 - acc: 0.4886 - val_loss: 2.4785 - val_acc: 0.2333\n",
      "Epoch 2145/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3011 - acc: 0.4871 - val_loss: 2.4712 - val_acc: 0.2367\n",
      "Epoch 2146/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3007 - acc: 0.4871 - val_loss: 2.4473 - val_acc: 0.2433\n",
      "Epoch 2147/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4900 - val_loss: 2.4847 - val_acc: 0.2367\n",
      "Epoch 2148/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3027 - acc: 0.4800 - val_loss: 2.4799 - val_acc: 0.2400\n",
      "Epoch 2149/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3008 - acc: 0.4886 - val_loss: 2.4491 - val_acc: 0.2367\n",
      "Epoch 2150/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3010 - acc: 0.4871 - val_loss: 2.4654 - val_acc: 0.2367\n",
      "Epoch 2151/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3017 - acc: 0.4857 - val_loss: 2.4983 - val_acc: 0.2333\n",
      "Epoch 2152/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3012 - acc: 0.4900 - val_loss: 2.4968 - val_acc: 0.2333\n",
      "Epoch 2153/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4886 - val_loss: 2.4831 - val_acc: 0.2367\n",
      "Epoch 2154/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3012 - acc: 0.4914 - val_loss: 2.5064 - val_acc: 0.2333\n",
      "Epoch 2155/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3014 - acc: 0.4929 - val_loss: 2.5001 - val_acc: 0.2333\n",
      "Epoch 2156/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3003 - acc: 0.4914 - val_loss: 2.5031 - val_acc: 0.2333\n",
      "Epoch 2157/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3010 - acc: 0.4900 - val_loss: 2.4575 - val_acc: 0.2333\n",
      "Epoch 2158/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3009 - acc: 0.4943 - val_loss: 2.5013 - val_acc: 0.2367\n",
      "Epoch 2159/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3010 - acc: 0.4886 - val_loss: 2.4625 - val_acc: 0.2333\n",
      "Epoch 2160/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3011 - acc: 0.4871 - val_loss: 2.4966 - val_acc: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2161/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3186 - acc: 0.494 - ETA: 0s - loss: 1.3008 - acc: 0.4914 - val_loss: 2.4879 - val_acc: 0.2333\n",
      "Epoch 2162/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3004 - acc: 0.4857 - val_loss: 2.4744 - val_acc: 0.2433\n",
      "Epoch 2163/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3010 - acc: 0.4829 - val_loss: 2.4651 - val_acc: 0.2333\n",
      "Epoch 2164/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4900 - val_loss: 2.4842 - val_acc: 0.2400\n",
      "Epoch 2165/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3007 - acc: 0.4886 - val_loss: 2.4777 - val_acc: 0.2400\n",
      "Epoch 2166/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3009 - acc: 0.4914 - val_loss: 2.4656 - val_acc: 0.2400\n",
      "Epoch 2167/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3004 - acc: 0.4900 - val_loss: 2.4790 - val_acc: 0.2367\n",
      "Epoch 2168/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3007 - acc: 0.4929 - val_loss: 2.4894 - val_acc: 0.2367\n",
      "Epoch 2169/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3003 - acc: 0.4929 - val_loss: 2.4649 - val_acc: 0.2367\n",
      "Epoch 2170/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3002 - acc: 0.4914 - val_loss: 2.4917 - val_acc: 0.2367\n",
      "Epoch 2171/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2998 - acc: 0.4957 - val_loss: 2.4712 - val_acc: 0.2367\n",
      "Epoch 2172/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2999 - acc: 0.4914 - val_loss: 2.5060 - val_acc: 0.2367\n",
      "Epoch 2173/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2993 - acc: 0.4886 - val_loss: 2.4927 - val_acc: 0.2400\n",
      "Epoch 2174/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3000 - acc: 0.4900 - val_loss: 2.4816 - val_acc: 0.2400\n",
      "Epoch 2175/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2995 - acc: 0.4914 - val_loss: 2.4568 - val_acc: 0.2400\n",
      "Epoch 2176/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2998 - acc: 0.4914 - val_loss: 2.4628 - val_acc: 0.2400\n",
      "Epoch 2177/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3002 - acc: 0.4843 - val_loss: 2.4893 - val_acc: 0.2400\n",
      "Epoch 2178/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3003 - acc: 0.4900 - val_loss: 2.4879 - val_acc: 0.2333\n",
      "Epoch 2179/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2993 - acc: 0.4900 - val_loss: 2.4868 - val_acc: 0.2300\n",
      "Epoch 2180/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3008 - acc: 0.4886 - val_loss: 2.4858 - val_acc: 0.2333\n",
      "Epoch 2181/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2997 - acc: 0.4914 - val_loss: 2.4896 - val_acc: 0.2367\n",
      "Epoch 2182/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2998 - acc: 0.4900 - val_loss: 2.4924 - val_acc: 0.2367\n",
      "Epoch 2183/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2994 - acc: 0.4886 - val_loss: 2.5122 - val_acc: 0.2367\n",
      "Epoch 2184/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2997 - acc: 0.4871 - val_loss: 2.4828 - val_acc: 0.2333\n",
      "Epoch 2185/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2991 - acc: 0.4914 - val_loss: 2.4846 - val_acc: 0.2433\n",
      "Epoch 2186/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.3004 - acc: 0.4929 - val_loss: 2.4883 - val_acc: 0.2367\n",
      "Epoch 2187/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2994 - acc: 0.4900 - val_loss: 2.4903 - val_acc: 0.2333\n",
      "Epoch 2188/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2997 - acc: 0.4929 - val_loss: 2.5002 - val_acc: 0.2333\n",
      "Epoch 2189/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2997 - acc: 0.4871 - val_loss: 2.5085 - val_acc: 0.2333\n",
      "Epoch 2190/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4900 - val_loss: 2.4911 - val_acc: 0.2333\n",
      "Epoch 2191/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2988 - acc: 0.4886 - val_loss: 2.4909 - val_acc: 0.2367\n",
      "Epoch 2192/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2990 - acc: 0.4929 - val_loss: 2.4760 - val_acc: 0.2367\n",
      "Epoch 2193/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2990 - acc: 0.4900 - val_loss: 2.4911 - val_acc: 0.2367\n",
      "Epoch 2194/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2996 - acc: 0.4886 - val_loss: 2.5042 - val_acc: 0.2333\n",
      "Epoch 2195/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2986 - acc: 0.4886 - val_loss: 2.5008 - val_acc: 0.2333\n",
      "Epoch 2196/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2993 - acc: 0.4900 - val_loss: 2.4980 - val_acc: 0.2300\n",
      "Epoch 2197/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2989 - acc: 0.4929 - val_loss: 2.4972 - val_acc: 0.2367\n",
      "Epoch 2198/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4886 - val_loss: 2.4876 - val_acc: 0.2333\n",
      "Epoch 2199/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2994 - acc: 0.4900 - val_loss: 2.4961 - val_acc: 0.2300\n",
      "Epoch 2200/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4929 - val_loss: 2.4672 - val_acc: 0.2367\n",
      "Epoch 2201/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4871 - val_loss: 2.5136 - val_acc: 0.2333\n",
      "Epoch 2202/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2971 - acc: 0.4929 - val_loss: 2.4986 - val_acc: 0.2367\n",
      "Epoch 2203/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2987 - acc: 0.4886 - val_loss: 2.5098 - val_acc: 0.2300\n",
      "Epoch 2204/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2982 - acc: 0.4857 - val_loss: 2.4701 - val_acc: 0.2367\n",
      "Epoch 2205/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2983 - acc: 0.4957 - val_loss: 2.5053 - val_acc: 0.2333\n",
      "Epoch 2206/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2988 - acc: 0.4900 - val_loss: 2.4879 - val_acc: 0.2367\n",
      "Epoch 2207/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2986 - acc: 0.4843 - val_loss: 2.5000 - val_acc: 0.2333\n",
      "Epoch 2208/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2982 - acc: 0.4943 - val_loss: 2.4675 - val_acc: 0.2367\n",
      "Epoch 2209/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4900 - val_loss: 2.4584 - val_acc: 0.2400\n",
      "Epoch 2210/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2990 - acc: 0.4871 - val_loss: 2.4862 - val_acc: 0.2333\n",
      "Epoch 2211/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4900 - val_loss: 2.4943 - val_acc: 0.2367\n",
      "Epoch 2212/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4914 - val_loss: 2.5059 - val_acc: 0.2367\n",
      "Epoch 2213/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2978 - acc: 0.4871 - val_loss: 2.4605 - val_acc: 0.2333\n",
      "Epoch 2214/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4914 - val_loss: 2.4983 - val_acc: 0.2367\n",
      "Epoch 2215/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2979 - acc: 0.4914 - val_loss: 2.4876 - val_acc: 0.2400\n",
      "Epoch 2216/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2988 - acc: 0.4871 - val_loss: 2.4845 - val_acc: 0.2400\n",
      "Epoch 2217/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2979 - acc: 0.4900 - val_loss: 2.4913 - val_acc: 0.2333\n",
      "Epoch 2218/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2975 - acc: 0.4957 - val_loss: 2.5009 - val_acc: 0.2333\n",
      "Epoch 2219/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2982 - acc: 0.4900 - val_loss: 2.4823 - val_acc: 0.2333\n",
      "Epoch 2220/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2971 - acc: 0.4857 - val_loss: 2.4771 - val_acc: 0.2400\n",
      "Epoch 2221/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2971 - acc: 0.4943 - val_loss: 2.4829 - val_acc: 0.2467\n",
      "Epoch 2222/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2981 - acc: 0.4857 - val_loss: 2.4674 - val_acc: 0.2333\n",
      "Epoch 2223/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2983 - acc: 0.4943 - val_loss: 2.4963 - val_acc: 0.2367\n",
      "Epoch 2224/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2976 - acc: 0.4929 - val_loss: 2.4940 - val_acc: 0.2367\n",
      "Epoch 2225/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2971 - acc: 0.4929 - val_loss: 2.4916 - val_acc: 0.2433\n",
      "Epoch 2226/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2973 - acc: 0.4957 - val_loss: 2.5138 - val_acc: 0.2367\n",
      "Epoch 2227/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4943 - val_loss: 2.4882 - val_acc: 0.2367\n",
      "Epoch 2228/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2973 - acc: 0.4857 - val_loss: 2.4777 - val_acc: 0.2367\n",
      "Epoch 2229/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4900 - val_loss: 2.4842 - val_acc: 0.2400\n",
      "Epoch 2230/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4971 - val_loss: 2.4927 - val_acc: 0.2400\n",
      "Epoch 2231/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2966 - acc: 0.4914 - val_loss: 2.4913 - val_acc: 0.2400\n",
      "Epoch 2232/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4914 - val_loss: 2.4949 - val_acc: 0.2367\n",
      "Epoch 2233/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2968 - acc: 0.4871 - val_loss: 2.4958 - val_acc: 0.2367\n",
      "Epoch 2234/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2973 - acc: 0.4914 - val_loss: 2.4755 - val_acc: 0.2367\n",
      "Epoch 2235/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2970 - acc: 0.4914 - val_loss: 2.5000 - val_acc: 0.2333\n",
      "Epoch 2236/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2966 - acc: 0.4900 - val_loss: 2.4816 - val_acc: 0.2433\n",
      "Epoch 2237/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2966 - acc: 0.4900 - val_loss: 2.5192 - val_acc: 0.2367\n",
      "Epoch 2238/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2970 - acc: 0.4957 - val_loss: 2.5273 - val_acc: 0.2367\n",
      "Epoch 2239/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2961 - acc: 0.4914 - val_loss: 2.4811 - val_acc: 0.2367\n",
      "Epoch 2240/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2965 - acc: 0.4886 - val_loss: 2.5111 - val_acc: 0.2333\n",
      "Epoch 2241/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2969 - acc: 0.4871 - val_loss: 2.4822 - val_acc: 0.2367\n",
      "Epoch 2242/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2962 - acc: 0.4914 - val_loss: 2.4960 - val_acc: 0.2300\n",
      "Epoch 2243/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2966 - acc: 0.4886 - val_loss: 2.4967 - val_acc: 0.2333\n",
      "Epoch 2244/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2965 - acc: 0.4871 - val_loss: 2.4904 - val_acc: 0.2333\n",
      "Epoch 2245/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4929 - val_loss: 2.5241 - val_acc: 0.2333\n",
      "Epoch 2246/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2961 - acc: 0.5000 - val_loss: 2.4996 - val_acc: 0.2300\n",
      "Epoch 2247/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2960 - acc: 0.4900 - val_loss: 2.4935 - val_acc: 0.2300\n",
      "Epoch 2248/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2962 - acc: 0.4900 - val_loss: 2.5452 - val_acc: 0.2367\n",
      "Epoch 2249/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2968 - acc: 0.4929 - val_loss: 2.4917 - val_acc: 0.2333\n",
      "Epoch 2250/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2956 - acc: 0.4871 - val_loss: 2.4969 - val_acc: 0.2367\n",
      "Epoch 2251/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2972 - acc: 0.4857 - val_loss: 2.5086 - val_acc: 0.2333\n",
      "Epoch 2252/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2957 - acc: 0.4900 - val_loss: 2.4833 - val_acc: 0.2333\n",
      "Epoch 2253/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2957 - acc: 0.4914 - val_loss: 2.5148 - val_acc: 0.2467\n",
      "Epoch 2254/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2965 - acc: 0.4943 - val_loss: 2.5097 - val_acc: 0.2333\n",
      "Epoch 2255/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2954 - acc: 0.4929 - val_loss: 2.5156 - val_acc: 0.2333\n",
      "Epoch 2256/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2957 - acc: 0.4943 - val_loss: 2.4960 - val_acc: 0.2333\n",
      "Epoch 2257/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2955 - acc: 0.4957 - val_loss: 2.5249 - val_acc: 0.2333\n",
      "Epoch 2258/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2956 - acc: 0.4986 - val_loss: 2.4610 - val_acc: 0.2400\n",
      "Epoch 2259/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2945 - acc: 0.4971 - val_loss: 2.5265 - val_acc: 0.2367\n",
      "Epoch 2260/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2955 - acc: 0.4957 - val_loss: 2.5067 - val_acc: 0.2367\n",
      "Epoch 2261/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2948 - acc: 0.4957 - val_loss: 2.5025 - val_acc: 0.2333\n",
      "Epoch 2262/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2955 - acc: 0.4971 - val_loss: 2.4978 - val_acc: 0.2333\n",
      "Epoch 2263/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2948 - acc: 0.4914 - val_loss: 2.5375 - val_acc: 0.2333\n",
      "Epoch 2264/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2954 - acc: 0.4929 - val_loss: 2.5326 - val_acc: 0.2333\n",
      "Epoch 2265/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2951 - acc: 0.4886 - val_loss: 2.4964 - val_acc: 0.2333\n",
      "Epoch 2266/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2946 - acc: 0.4986 - val_loss: 2.5321 - val_acc: 0.2400\n",
      "Epoch 2267/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4871 - val_loss: 2.4895 - val_acc: 0.2300\n",
      "Epoch 2268/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2949 - acc: 0.4943 - val_loss: 2.4810 - val_acc: 0.2300\n",
      "Epoch 2269/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2936 - acc: 0.4943 - val_loss: 2.5238 - val_acc: 0.2433\n",
      "Epoch 2270/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2954 - acc: 0.4971 - val_loss: 2.4922 - val_acc: 0.2367\n",
      "Epoch 2271/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2950 - acc: 0.4857 - val_loss: 2.5243 - val_acc: 0.2333\n",
      "Epoch 2272/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2943 - acc: 0.4943 - val_loss: 2.5291 - val_acc: 0.2333\n",
      "Epoch 2273/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2948 - acc: 0.4957 - val_loss: 2.4956 - val_acc: 0.2367\n",
      "Epoch 2274/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2950 - acc: 0.4914 - val_loss: 2.5068 - val_acc: 0.2367\n",
      "Epoch 2275/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2943 - acc: 0.4986 - val_loss: 2.5055 - val_acc: 0.2300\n",
      "Epoch 2276/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2949 - acc: 0.4900 - val_loss: 2.5099 - val_acc: 0.2333\n",
      "Epoch 2277/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2952 - acc: 0.4900 - val_loss: 2.5119 - val_acc: 0.2367\n",
      "Epoch 2278/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2941 - acc: 0.4900 - val_loss: 2.4998 - val_acc: 0.2333\n",
      "Epoch 2279/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2942 - acc: 0.4871 - val_loss: 2.5119 - val_acc: 0.2367\n",
      "Epoch 2280/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2940 - acc: 0.4914 - val_loss: 2.5203 - val_acc: 0.2367\n",
      "Epoch 2281/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4957 - val_loss: 2.5072 - val_acc: 0.2367\n",
      "Epoch 2282/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2941 - acc: 0.4929 - val_loss: 2.5108 - val_acc: 0.2300\n",
      "Epoch 2283/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2934 - acc: 0.4914 - val_loss: 2.4707 - val_acc: 0.2467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2284/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2953 - acc: 0.4971 - val_loss: 2.4992 - val_acc: 0.2367\n",
      "Epoch 2285/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2953 - acc: 0.4843 - val_loss: 2.5183 - val_acc: 0.2333\n",
      "Epoch 2286/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2942 - acc: 0.4914 - val_loss: 2.5060 - val_acc: 0.2333\n",
      "Epoch 2287/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2944 - acc: 0.4843 - val_loss: 2.4962 - val_acc: 0.2300\n",
      "Epoch 2288/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2939 - acc: 0.4986 - val_loss: 2.5040 - val_acc: 0.2267\n",
      "Epoch 2289/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2944 - acc: 0.4914 - val_loss: 2.5193 - val_acc: 0.2333\n",
      "Epoch 2290/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2941 - acc: 0.4929 - val_loss: 2.4892 - val_acc: 0.2333\n",
      "Epoch 2291/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2936 - acc: 0.4900 - val_loss: 2.5063 - val_acc: 0.2300\n",
      "Epoch 2292/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4943 - val_loss: 2.5129 - val_acc: 0.2367\n",
      "Epoch 2293/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2936 - acc: 0.4957 - val_loss: 2.5131 - val_acc: 0.2333\n",
      "Epoch 2294/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2932 - acc: 0.4943 - val_loss: 2.5158 - val_acc: 0.2333\n",
      "Epoch 2295/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2937 - acc: 0.4929 - val_loss: 2.5132 - val_acc: 0.2367\n",
      "Epoch 2296/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2936 - acc: 0.4886 - val_loss: 2.4998 - val_acc: 0.2333\n",
      "Epoch 2297/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2929 - acc: 0.4943 - val_loss: 2.5094 - val_acc: 0.2333\n",
      "Epoch 2298/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2932 - acc: 0.4957 - val_loss: 2.5271 - val_acc: 0.2367\n",
      "Epoch 2299/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2934 - acc: 0.4914 - val_loss: 2.5156 - val_acc: 0.2367\n",
      "Epoch 2300/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2927 - acc: 0.4914 - val_loss: 2.5088 - val_acc: 0.2367\n",
      "Epoch 2301/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2939 - acc: 0.4900 - val_loss: 2.5057 - val_acc: 0.2333\n",
      "Epoch 2302/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2927 - acc: 0.4929 - val_loss: 2.5306 - val_acc: 0.2367\n",
      "Epoch 2303/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2931 - acc: 0.4943 - val_loss: 2.5002 - val_acc: 0.2300\n",
      "Epoch 2304/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2932 - acc: 0.4914 - val_loss: 2.5151 - val_acc: 0.2333\n",
      "Epoch 2305/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2926 - acc: 0.4900 - val_loss: 2.5400 - val_acc: 0.2433\n",
      "Epoch 2306/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2935 - acc: 0.4929 - val_loss: 2.4678 - val_acc: 0.2367\n",
      "Epoch 2307/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2935 - acc: 0.4943 - val_loss: 2.5002 - val_acc: 0.2333\n",
      "Epoch 2308/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4929 - val_loss: 2.4981 - val_acc: 0.2300\n",
      "Epoch 2309/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2929 - acc: 0.4871 - val_loss: 2.5205 - val_acc: 0.2333\n",
      "Epoch 2310/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2920 - acc: 0.4914 - val_loss: 2.5209 - val_acc: 0.2367\n",
      "Epoch 2311/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2927 - acc: 0.4929 - val_loss: 2.5119 - val_acc: 0.2333\n",
      "Epoch 2312/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2925 - acc: 0.4929 - val_loss: 2.5240 - val_acc: 0.2367\n",
      "Epoch 2313/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2921 - acc: 0.4900 - val_loss: 2.4961 - val_acc: 0.2300\n",
      "Epoch 2314/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4957 - val_loss: 2.5167 - val_acc: 0.2367\n",
      "Epoch 2315/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2923 - acc: 0.4957 - val_loss: 2.5061 - val_acc: 0.2300\n",
      "Epoch 2316/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2923 - acc: 0.4900 - val_loss: 2.5088 - val_acc: 0.2367\n",
      "Epoch 2317/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2908 - acc: 0.4986 - val_loss: 2.5123 - val_acc: 0.2333\n",
      "Epoch 2318/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2924 - acc: 0.4943 - val_loss: 2.5147 - val_acc: 0.2333\n",
      "Epoch 2319/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4886 - val_loss: 2.5288 - val_acc: 0.2367\n",
      "Epoch 2320/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2920 - acc: 0.4914 - val_loss: 2.5084 - val_acc: 0.2333\n",
      "Epoch 2321/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2920 - acc: 0.4971 - val_loss: 2.5210 - val_acc: 0.2333\n",
      "Epoch 2322/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2916 - acc: 0.4957 - val_loss: 2.5215 - val_acc: 0.2367\n",
      "Epoch 2323/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2917 - acc: 0.4943 - val_loss: 2.4984 - val_acc: 0.2333\n",
      "Epoch 2324/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2919 - acc: 0.4986 - val_loss: 2.5247 - val_acc: 0.2333\n",
      "Epoch 2325/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2925 - acc: 0.4843 - val_loss: 2.5087 - val_acc: 0.2267\n",
      "Epoch 2326/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2919 - acc: 0.4957 - val_loss: 2.4991 - val_acc: 0.2267\n",
      "Epoch 2327/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2918 - acc: 0.4929 - val_loss: 2.5095 - val_acc: 0.2267\n",
      "Epoch 2328/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2919 - acc: 0.4929 - val_loss: 2.5273 - val_acc: 0.2300\n",
      "Epoch 2329/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2917 - acc: 0.4971 - val_loss: 2.5361 - val_acc: 0.2333\n",
      "Epoch 2330/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2918 - acc: 0.4929 - val_loss: 2.5101 - val_acc: 0.2300\n",
      "Epoch 2331/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2906 - acc: 0.4986 - val_loss: 2.5107 - val_acc: 0.2333\n",
      "Epoch 2332/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2924 - acc: 0.4871 - val_loss: 2.5145 - val_acc: 0.2367\n",
      "Epoch 2333/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2917 - acc: 0.4929 - val_loss: 2.5204 - val_acc: 0.2300\n",
      "Epoch 2334/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2913 - acc: 0.4914 - val_loss: 2.5167 - val_acc: 0.2367\n",
      "Epoch 2335/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2902 - acc: 0.4929 - val_loss: 2.5245 - val_acc: 0.2333\n",
      "Epoch 2336/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2911 - acc: 0.4857 - val_loss: 2.5379 - val_acc: 0.2367\n",
      "Epoch 2337/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2922 - acc: 0.4900 - val_loss: 2.5201 - val_acc: 0.2333\n",
      "Epoch 2338/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2915 - acc: 0.4971 - val_loss: 2.5208 - val_acc: 0.2367\n",
      "Epoch 2339/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2908 - acc: 0.4957 - val_loss: 2.5036 - val_acc: 0.2300\n",
      "Epoch 2340/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2909 - acc: 0.4929 - val_loss: 2.5224 - val_acc: 0.2333\n",
      "Epoch 2341/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2910 - acc: 0.4886 - val_loss: 2.5195 - val_acc: 0.2333\n",
      "Epoch 2342/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2909 - acc: 0.4957 - val_loss: 2.5104 - val_acc: 0.2333\n",
      "Epoch 2343/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2906 - acc: 0.4914 - val_loss: 2.5312 - val_acc: 0.2367\n",
      "Epoch 2344/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2904 - acc: 0.4943 - val_loss: 2.5184 - val_acc: 0.2367\n",
      "Epoch 2345/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2907 - acc: 0.4914 - val_loss: 2.5251 - val_acc: 0.2367\n",
      "Epoch 2346/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2908 - acc: 0.4971 - val_loss: 2.5349 - val_acc: 0.2367\n",
      "Epoch 2347/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2906 - acc: 0.4886 - val_loss: 2.5395 - val_acc: 0.2400\n",
      "Epoch 2348/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2902 - acc: 0.4914 - val_loss: 2.5258 - val_acc: 0.2367\n",
      "Epoch 2349/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2898 - acc: 0.4929 - val_loss: 2.5263 - val_acc: 0.2400\n",
      "Epoch 2350/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2898 - acc: 0.4900 - val_loss: 2.5313 - val_acc: 0.2400\n",
      "Epoch 2351/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2905 - acc: 0.4929 - val_loss: 2.5256 - val_acc: 0.2300\n",
      "Epoch 2352/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2911 - acc: 0.4900 - val_loss: 2.5358 - val_acc: 0.2367\n",
      "Epoch 2353/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2899 - acc: 0.4886 - val_loss: 2.5115 - val_acc: 0.2333\n",
      "Epoch 2354/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2901 - acc: 0.4943 - val_loss: 2.5353 - val_acc: 0.2300\n",
      "Epoch 2355/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2898 - acc: 0.4900 - val_loss: 2.5379 - val_acc: 0.2333\n",
      "Epoch 2356/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2891 - acc: 0.4886 - val_loss: 2.5201 - val_acc: 0.2400\n",
      "Epoch 2357/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2905 - acc: 0.4886 - val_loss: 2.5060 - val_acc: 0.2333\n",
      "Epoch 2358/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2904 - acc: 0.4914 - val_loss: 2.5392 - val_acc: 0.2367\n",
      "Epoch 2359/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2902 - acc: 0.4843 - val_loss: 2.5152 - val_acc: 0.2333\n",
      "Epoch 2360/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2895 - acc: 0.4914 - val_loss: 2.5302 - val_acc: 0.2400\n",
      "Epoch 2361/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2901 - acc: 0.4957 - val_loss: 2.5246 - val_acc: 0.2333\n",
      "Epoch 2362/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2898 - acc: 0.4914 - val_loss: 2.5126 - val_acc: 0.2333\n",
      "Epoch 2363/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2894 - acc: 0.4943 - val_loss: 2.5154 - val_acc: 0.2367\n",
      "Epoch 2364/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2895 - acc: 0.4943 - val_loss: 2.5313 - val_acc: 0.2367\n",
      "Epoch 2365/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2888 - acc: 0.4900 - val_loss: 2.5432 - val_acc: 0.2467\n",
      "Epoch 2366/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2895 - acc: 0.4971 - val_loss: 2.5363 - val_acc: 0.2367\n",
      "Epoch 2367/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2894 - acc: 0.4886 - val_loss: 2.5181 - val_acc: 0.2333\n",
      "Epoch 2368/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2894 - acc: 0.5000 - val_loss: 2.5282 - val_acc: 0.2367\n",
      "Epoch 2369/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2890 - acc: 0.4957 - val_loss: 2.5206 - val_acc: 0.2367\n",
      "Epoch 2370/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2881 - acc: 0.4957 - val_loss: 2.5157 - val_acc: 0.2400\n",
      "Epoch 2371/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2894 - acc: 0.4943 - val_loss: 2.5416 - val_acc: 0.2433\n",
      "Epoch 2372/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2891 - acc: 0.4943 - val_loss: 2.5042 - val_acc: 0.2300\n",
      "Epoch 2373/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2902 - acc: 0.4914 - val_loss: 2.5409 - val_acc: 0.2367\n",
      "Epoch 2374/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2888 - acc: 0.4929 - val_loss: 2.5256 - val_acc: 0.2367\n",
      "Epoch 2375/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2892 - acc: 0.4914 - val_loss: 2.5236 - val_acc: 0.2300\n",
      "Epoch 2376/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2891 - acc: 0.4986 - val_loss: 2.5319 - val_acc: 0.2333\n",
      "Epoch 2377/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2888 - acc: 0.4886 - val_loss: 2.5235 - val_acc: 0.2300\n",
      "Epoch 2378/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2887 - acc: 0.4971 - val_loss: 2.5269 - val_acc: 0.2333\n",
      "Epoch 2379/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2891 - acc: 0.4971 - val_loss: 2.5316 - val_acc: 0.2333\n",
      "Epoch 2380/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2884 - acc: 0.4929 - val_loss: 2.5110 - val_acc: 0.2333\n",
      "Epoch 2381/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2889 - acc: 0.4986 - val_loss: 2.5343 - val_acc: 0.2367\n",
      "Epoch 2382/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2886 - acc: 0.4900 - val_loss: 2.5322 - val_acc: 0.2367\n",
      "Epoch 2383/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2884 - acc: 0.4986 - val_loss: 2.5244 - val_acc: 0.2333\n",
      "Epoch 2384/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2883 - acc: 0.4871 - val_loss: 2.5276 - val_acc: 0.2367\n",
      "Epoch 2385/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2884 - acc: 0.4957 - val_loss: 2.5412 - val_acc: 0.2400\n",
      "Epoch 2386/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2876 - acc: 0.4900 - val_loss: 2.5333 - val_acc: 0.2333\n",
      "Epoch 2387/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2884 - acc: 0.4986 - val_loss: 2.5457 - val_acc: 0.2367\n",
      "Epoch 2388/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2881 - acc: 0.4943 - val_loss: 2.5361 - val_acc: 0.2400\n",
      "Epoch 2389/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2886 - acc: 0.4929 - val_loss: 2.5220 - val_acc: 0.2333\n",
      "Epoch 2390/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2877 - acc: 0.4929 - val_loss: 2.5522 - val_acc: 0.2433\n",
      "Epoch 2391/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2876 - acc: 0.4957 - val_loss: 2.5443 - val_acc: 0.2400\n",
      "Epoch 2392/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2890 - acc: 0.4957 - val_loss: 2.5368 - val_acc: 0.2367\n",
      "Epoch 2393/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2881 - acc: 0.4986 - val_loss: 2.5266 - val_acc: 0.2400\n",
      "Epoch 2394/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2879 - acc: 0.5071 - val_loss: 2.5332 - val_acc: 0.2400\n",
      "Epoch 2395/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2877 - acc: 0.4871 - val_loss: 2.5462 - val_acc: 0.2400\n",
      "Epoch 2396/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2875 - acc: 0.4914 - val_loss: 2.5577 - val_acc: 0.2400\n",
      "Epoch 2397/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2874 - acc: 0.4943 - val_loss: 2.5392 - val_acc: 0.2367\n",
      "Epoch 2398/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2878 - acc: 0.5043 - val_loss: 2.5354 - val_acc: 0.2367\n",
      "Epoch 2399/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2879 - acc: 0.5000 - val_loss: 2.5608 - val_acc: 0.2433\n",
      "Epoch 2400/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2871 - acc: 0.4929 - val_loss: 2.5353 - val_acc: 0.2333\n",
      "Epoch 2401/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2869 - acc: 0.4886 - val_loss: 2.5676 - val_acc: 0.2367\n",
      "Epoch 2402/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2887 - acc: 0.4857 - val_loss: 2.5153 - val_acc: 0.2367\n",
      "Epoch 2403/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2878 - acc: 0.4929 - val_loss: 2.5329 - val_acc: 0.2367\n",
      "Epoch 2404/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2867 - acc: 0.4957 - val_loss: 2.5310 - val_acc: 0.2367\n",
      "Epoch 2405/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2878 - acc: 0.4943 - val_loss: 2.5115 - val_acc: 0.2333\n",
      "Epoch 2406/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2884 - acc: 0.5000 - val_loss: 2.5292 - val_acc: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2407/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2866 - acc: 0.4914 - val_loss: 2.5251 - val_acc: 0.2267\n",
      "Epoch 2408/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2871 - acc: 0.4943 - val_loss: 2.5291 - val_acc: 0.2400\n",
      "Epoch 2409/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2873 - acc: 0.4914 - val_loss: 2.5310 - val_acc: 0.2333\n",
      "Epoch 2410/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2870 - acc: 0.4900 - val_loss: 2.5461 - val_acc: 0.2367\n",
      "Epoch 2411/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2869 - acc: 0.5029 - val_loss: 2.5277 - val_acc: 0.2333\n",
      "Epoch 2412/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2869 - acc: 0.4986 - val_loss: 2.5709 - val_acc: 0.2400\n",
      "Epoch 2413/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2870 - acc: 0.4900 - val_loss: 2.5479 - val_acc: 0.2367\n",
      "Epoch 2414/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2868 - acc: 0.4943 - val_loss: 2.5481 - val_acc: 0.2367\n",
      "Epoch 2415/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2878 - acc: 0.4943 - val_loss: 2.5420 - val_acc: 0.2367\n",
      "Epoch 2416/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2865 - acc: 0.4957 - val_loss: 2.5223 - val_acc: 0.2333\n",
      "Epoch 2417/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2876 - acc: 0.4957 - val_loss: 2.5477 - val_acc: 0.2333\n",
      "Epoch 2418/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2869 - acc: 0.4929 - val_loss: 2.5559 - val_acc: 0.2367\n",
      "Epoch 2419/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2868 - acc: 0.4886 - val_loss: 2.5286 - val_acc: 0.2367\n",
      "Epoch 2420/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2862 - acc: 0.4914 - val_loss: 2.5211 - val_acc: 0.2333\n",
      "Epoch 2421/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2871 - acc: 0.4929 - val_loss: 2.5199 - val_acc: 0.2333\n",
      "Epoch 2422/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2868 - acc: 0.5000 - val_loss: 2.5460 - val_acc: 0.2367\n",
      "Epoch 2423/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2868 - acc: 0.4929 - val_loss: 2.5516 - val_acc: 0.2367\n",
      "Epoch 2424/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2861 - acc: 0.4943 - val_loss: 2.5253 - val_acc: 0.2333\n",
      "Epoch 2425/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2865 - acc: 0.4943 - val_loss: 2.5424 - val_acc: 0.2333\n",
      "Epoch 2426/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2853 - acc: 0.5000 - val_loss: 2.5240 - val_acc: 0.2267\n",
      "Epoch 2427/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2864 - acc: 0.4943 - val_loss: 2.5425 - val_acc: 0.2333\n",
      "Epoch 2428/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2860 - acc: 0.4986 - val_loss: 2.5252 - val_acc: 0.2333\n",
      "Epoch 2429/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2855 - acc: 0.4943 - val_loss: 2.5463 - val_acc: 0.2367\n",
      "Epoch 2430/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2857 - acc: 0.4943 - val_loss: 2.5240 - val_acc: 0.2367\n",
      "Epoch 2431/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2852 - acc: 0.5014 - val_loss: 2.5378 - val_acc: 0.2367\n",
      "Epoch 2432/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2855 - acc: 0.4986 - val_loss: 2.5419 - val_acc: 0.2333\n",
      "Epoch 2433/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2859 - acc: 0.4986 - val_loss: 2.5276 - val_acc: 0.2300\n",
      "Epoch 2434/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2861 - acc: 0.4900 - val_loss: 2.5285 - val_acc: 0.2367\n",
      "Epoch 2435/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2858 - acc: 0.5029 - val_loss: 2.5625 - val_acc: 0.2400\n",
      "Epoch 2436/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2860 - acc: 0.4943 - val_loss: 2.5548 - val_acc: 0.2400\n",
      "Epoch 2437/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2869 - acc: 0.4914 - val_loss: 2.5357 - val_acc: 0.2367\n",
      "Epoch 2438/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2855 - acc: 0.4900 - val_loss: 2.5396 - val_acc: 0.2367\n",
      "Epoch 2439/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2865 - acc: 0.4929 - val_loss: 2.5568 - val_acc: 0.2400\n",
      "Epoch 2440/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2861 - acc: 0.5000 - val_loss: 2.5593 - val_acc: 0.2400\n",
      "Epoch 2441/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2846 - acc: 0.4900 - val_loss: 2.5342 - val_acc: 0.2300\n",
      "Epoch 2442/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2859 - acc: 0.4929 - val_loss: 2.5557 - val_acc: 0.2400\n",
      "Epoch 2443/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2853 - acc: 0.5029 - val_loss: 2.5579 - val_acc: 0.2400\n",
      "Epoch 2444/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2847 - acc: 0.4943 - val_loss: 2.5430 - val_acc: 0.2367\n",
      "Epoch 2445/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2861 - acc: 0.4971 - val_loss: 2.5347 - val_acc: 0.2367\n",
      "Epoch 2446/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2850 - acc: 0.4971 - val_loss: 2.5468 - val_acc: 0.2367\n",
      "Epoch 2447/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2849 - acc: 0.5000 - val_loss: 2.5612 - val_acc: 0.2367\n",
      "Epoch 2448/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2850 - acc: 0.4929 - val_loss: 2.5409 - val_acc: 0.2400\n",
      "Epoch 2449/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2851 - acc: 0.4943 - val_loss: 2.5536 - val_acc: 0.2367\n",
      "Epoch 2450/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2848 - acc: 0.4943 - val_loss: 2.5307 - val_acc: 0.2333\n",
      "Epoch 2451/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2849 - acc: 0.5014 - val_loss: 2.5499 - val_acc: 0.2367\n",
      "Epoch 2452/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2852 - acc: 0.4957 - val_loss: 2.5551 - val_acc: 0.2367\n",
      "Epoch 2453/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2842 - acc: 0.4929 - val_loss: 2.5392 - val_acc: 0.2300\n",
      "Epoch 2454/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2848 - acc: 0.4929 - val_loss: 2.5712 - val_acc: 0.2433\n",
      "Epoch 2455/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2856 - acc: 0.4943 - val_loss: 2.5544 - val_acc: 0.2367\n",
      "Epoch 2456/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2842 - acc: 0.5000 - val_loss: 2.5698 - val_acc: 0.2433\n",
      "Epoch 2457/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2849 - acc: 0.5000 - val_loss: 2.5548 - val_acc: 0.2400\n",
      "Epoch 2458/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2844 - acc: 0.4943 - val_loss: 2.5350 - val_acc: 0.2367\n",
      "Epoch 2459/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2844 - acc: 0.5029 - val_loss: 2.5428 - val_acc: 0.2367\n",
      "Epoch 2460/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2845 - acc: 0.4971 - val_loss: 2.5408 - val_acc: 0.2333\n",
      "Epoch 2461/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2843 - acc: 0.4929 - val_loss: 2.5447 - val_acc: 0.2367\n",
      "Epoch 2462/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2840 - acc: 0.4943 - val_loss: 2.5558 - val_acc: 0.2367\n",
      "Epoch 2463/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2844 - acc: 0.4971 - val_loss: 2.5415 - val_acc: 0.2367\n",
      "Epoch 2464/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2840 - acc: 0.4957 - val_loss: 2.5404 - val_acc: 0.2367\n",
      "Epoch 2465/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2842 - acc: 0.4929 - val_loss: 2.5580 - val_acc: 0.2367\n",
      "Epoch 2466/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2835 - acc: 0.4957 - val_loss: 2.5429 - val_acc: 0.2400\n",
      "Epoch 2467/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2840 - acc: 0.5000 - val_loss: 2.5603 - val_acc: 0.2367\n",
      "Epoch 2468/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2837 - acc: 0.4929 - val_loss: 2.5340 - val_acc: 0.2367\n",
      "Epoch 2469/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2840 - acc: 0.4957 - val_loss: 2.5613 - val_acc: 0.2333\n",
      "Epoch 2470/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2838 - acc: 0.4943 - val_loss: 2.5563 - val_acc: 0.2433\n",
      "Epoch 2471/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2842 - acc: 0.4929 - val_loss: 2.5346 - val_acc: 0.2367\n",
      "Epoch 2472/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2837 - acc: 0.4943 - val_loss: 2.5559 - val_acc: 0.2333\n",
      "Epoch 2473/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2831 - acc: 0.5000 - val_loss: 2.5809 - val_acc: 0.2333\n",
      "Epoch 2474/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2853 - acc: 0.4943 - val_loss: 2.5472 - val_acc: 0.2300\n",
      "Epoch 2475/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2832 - acc: 0.4957 - val_loss: 2.5636 - val_acc: 0.2367\n",
      "Epoch 2476/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2833 - acc: 0.4943 - val_loss: 2.5395 - val_acc: 0.2367\n",
      "Epoch 2477/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2836 - acc: 0.4971 - val_loss: 2.5449 - val_acc: 0.2367\n",
      "Epoch 2478/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2834 - acc: 0.4943 - val_loss: 2.5615 - val_acc: 0.2400\n",
      "Epoch 2479/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2819 - acc: 0.4986 - val_loss: 2.5492 - val_acc: 0.2400\n",
      "Epoch 2480/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2822 - acc: 0.5014 - val_loss: 2.5557 - val_acc: 0.2300\n",
      "Epoch 2481/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2826 - acc: 0.4986 - val_loss: 2.5587 - val_acc: 0.2367\n",
      "Epoch 2482/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2829 - acc: 0.4886 - val_loss: 2.5671 - val_acc: 0.2367\n",
      "Epoch 2483/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2832 - acc: 0.4971 - val_loss: 2.5520 - val_acc: 0.2367\n",
      "Epoch 2484/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2835 - acc: 0.4900 - val_loss: 2.5529 - val_acc: 0.2400\n",
      "Epoch 2485/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2831 - acc: 0.5000 - val_loss: 2.5519 - val_acc: 0.2367\n",
      "Epoch 2486/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2830 - acc: 0.4957 - val_loss: 2.5428 - val_acc: 0.2367\n",
      "Epoch 2487/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2831 - acc: 0.4971 - val_loss: 2.5736 - val_acc: 0.2400\n",
      "Epoch 2488/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2830 - acc: 0.4886 - val_loss: 2.5485 - val_acc: 0.2400\n",
      "Epoch 2489/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2836 - acc: 0.4957 - val_loss: 2.5625 - val_acc: 0.2433\n",
      "Epoch 2490/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2826 - acc: 0.4943 - val_loss: 2.5572 - val_acc: 0.2400\n",
      "Epoch 2491/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2835 - acc: 0.4929 - val_loss: 2.5388 - val_acc: 0.2333\n",
      "Epoch 2492/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2831 - acc: 0.4971 - val_loss: 2.5449 - val_acc: 0.2400\n",
      "Epoch 2493/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2826 - acc: 0.4971 - val_loss: 2.5768 - val_acc: 0.2433\n",
      "Epoch 2494/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2829 - acc: 0.4886 - val_loss: 2.5444 - val_acc: 0.2367\n",
      "Epoch 2495/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2823 - acc: 0.4957 - val_loss: 2.5911 - val_acc: 0.2433\n",
      "Epoch 2496/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2834 - acc: 0.4943 - val_loss: 2.5717 - val_acc: 0.2367\n",
      "Epoch 2497/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2828 - acc: 0.4971 - val_loss: 2.5655 - val_acc: 0.2467\n",
      "Epoch 2498/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2824 - acc: 0.4957 - val_loss: 2.5785 - val_acc: 0.2400\n",
      "Epoch 2499/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2825 - acc: 0.5014 - val_loss: 2.5420 - val_acc: 0.2333\n",
      "Epoch 2500/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2822 - acc: 0.5014 - val_loss: 2.5436 - val_acc: 0.2367\n",
      "Epoch 2501/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2825 - acc: 0.4971 - val_loss: 2.5890 - val_acc: 0.2400\n",
      "Epoch 2502/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2823 - acc: 0.4957 - val_loss: 2.5630 - val_acc: 0.2367\n",
      "Epoch 2503/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2819 - acc: 0.4957 - val_loss: 2.5477 - val_acc: 0.2400\n",
      "Epoch 2504/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2820 - acc: 0.5000 - val_loss: 2.5618 - val_acc: 0.2367\n",
      "Epoch 2505/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2833 - acc: 0.4871 - val_loss: 2.5568 - val_acc: 0.2367\n",
      "Epoch 2506/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2816 - acc: 0.4943 - val_loss: 2.5610 - val_acc: 0.2400\n",
      "Epoch 2507/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2818 - acc: 0.4957 - val_loss: 2.5506 - val_acc: 0.2400\n",
      "Epoch 2508/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2821 - acc: 0.4943 - val_loss: 2.5810 - val_acc: 0.2433\n",
      "Epoch 2509/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2815 - acc: 0.5014 - val_loss: 2.5580 - val_acc: 0.2400\n",
      "Epoch 2510/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2815 - acc: 0.4957 - val_loss: 2.5698 - val_acc: 0.2367\n",
      "Epoch 2511/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2817 - acc: 0.4971 - val_loss: 2.5423 - val_acc: 0.2367\n",
      "Epoch 2512/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2812 - acc: 0.5000 - val_loss: 2.5743 - val_acc: 0.2433\n",
      "Epoch 2513/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2821 - acc: 0.4971 - val_loss: 2.5522 - val_acc: 0.2367\n",
      "Epoch 2514/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2810 - acc: 0.4929 - val_loss: 2.5522 - val_acc: 0.2367\n",
      "Epoch 2515/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2812 - acc: 0.4986 - val_loss: 2.5485 - val_acc: 0.2333\n",
      "Epoch 2516/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2811 - acc: 0.5000 - val_loss: 2.5676 - val_acc: 0.2400\n",
      "Epoch 2517/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2821 - acc: 0.4943 - val_loss: 2.5745 - val_acc: 0.2333\n",
      "Epoch 2518/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2813 - acc: 0.4957 - val_loss: 2.5677 - val_acc: 0.2300\n",
      "Epoch 2519/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2818 - acc: 0.4986 - val_loss: 2.5737 - val_acc: 0.2400\n",
      "Epoch 2520/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2810 - acc: 0.4900 - val_loss: 2.5843 - val_acc: 0.2400\n",
      "Epoch 2521/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2813 - acc: 0.4943 - val_loss: 2.5651 - val_acc: 0.2333\n",
      "Epoch 2522/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2815 - acc: 0.4914 - val_loss: 2.5560 - val_acc: 0.2367\n",
      "Epoch 2523/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2812 - acc: 0.4986 - val_loss: 2.5493 - val_acc: 0.2333\n",
      "Epoch 2524/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2806 - acc: 0.4914 - val_loss: 2.5508 - val_acc: 0.2367\n",
      "Epoch 2525/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2811 - acc: 0.4957 - val_loss: 2.5687 - val_acc: 0.2333\n",
      "Epoch 2526/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2810 - acc: 0.4986 - val_loss: 2.5570 - val_acc: 0.2333\n",
      "Epoch 2527/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2805 - acc: 0.5029 - val_loss: 2.5739 - val_acc: 0.2433\n",
      "Epoch 2528/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2803 - acc: 0.4957 - val_loss: 2.5634 - val_acc: 0.2333\n",
      "Epoch 2529/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2814 - acc: 0.5000 - val_loss: 2.5824 - val_acc: 0.2433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2530/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2801 - acc: 0.4957 - val_loss: 2.5704 - val_acc: 0.2367\n",
      "Epoch 2531/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2804 - acc: 0.4971 - val_loss: 2.5325 - val_acc: 0.2333\n",
      "Epoch 2532/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2804 - acc: 0.5043 - val_loss: 2.5728 - val_acc: 0.2367\n",
      "Epoch 2533/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2812 - acc: 0.4986 - val_loss: 2.5664 - val_acc: 0.2367\n",
      "Epoch 2534/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2812 - acc: 0.4943 - val_loss: 2.5528 - val_acc: 0.2367\n",
      "Epoch 2535/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2811 - acc: 0.4943 - val_loss: 2.5611 - val_acc: 0.2400\n",
      "Epoch 2536/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2807 - acc: 0.4986 - val_loss: 2.5816 - val_acc: 0.2433\n",
      "Epoch 2537/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2813 - acc: 0.4986 - val_loss: 2.5545 - val_acc: 0.2433\n",
      "Epoch 2538/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2802 - acc: 0.4914 - val_loss: 2.5724 - val_acc: 0.2433\n",
      "Epoch 2539/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2806 - acc: 0.5000 - val_loss: 2.5834 - val_acc: 0.2433\n",
      "Epoch 2540/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2805 - acc: 0.4929 - val_loss: 2.5835 - val_acc: 0.2400\n",
      "Epoch 2541/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2802 - acc: 0.5000 - val_loss: 2.5689 - val_acc: 0.2400\n",
      "Epoch 2542/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2795 - acc: 0.4971 - val_loss: 2.5662 - val_acc: 0.2367\n",
      "Epoch 2543/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2802 - acc: 0.4971 - val_loss: 2.5710 - val_acc: 0.2300\n",
      "Epoch 2544/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2805 - acc: 0.4971 - val_loss: 2.5639 - val_acc: 0.2400\n",
      "Epoch 2545/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2803 - acc: 0.4986 - val_loss: 2.5904 - val_acc: 0.2433\n",
      "Epoch 2546/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2805 - acc: 0.4971 - val_loss: 2.5854 - val_acc: 0.2400\n",
      "Epoch 2547/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2802 - acc: 0.5000 - val_loss: 2.5777 - val_acc: 0.2400\n",
      "Epoch 2548/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2796 - acc: 0.4957 - val_loss: 2.5658 - val_acc: 0.2433\n",
      "Epoch 2549/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2798 - acc: 0.4957 - val_loss: 2.5639 - val_acc: 0.2400\n",
      "Epoch 2550/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2798 - acc: 0.4971 - val_loss: 2.5648 - val_acc: 0.2433\n",
      "Epoch 2551/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2789 - acc: 0.4971 - val_loss: 2.5859 - val_acc: 0.2367\n",
      "Epoch 2552/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2794 - acc: 0.5014 - val_loss: 2.5509 - val_acc: 0.2300\n",
      "Epoch 2553/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2792 - acc: 0.4971 - val_loss: 2.5570 - val_acc: 0.2400\n",
      "Epoch 2554/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2793 - acc: 0.4957 - val_loss: 2.5562 - val_acc: 0.2367\n",
      "Epoch 2555/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2799 - acc: 0.4971 - val_loss: 2.5846 - val_acc: 0.2367\n",
      "Epoch 2556/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2797 - acc: 0.4943 - val_loss: 2.5581 - val_acc: 0.2433\n",
      "Epoch 2557/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.4900 - val_loss: 2.5842 - val_acc: 0.2367\n",
      "Epoch 2558/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2798 - acc: 0.4943 - val_loss: 2.5853 - val_acc: 0.2367\n",
      "Epoch 2559/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.4971 - val_loss: 2.5720 - val_acc: 0.2400\n",
      "Epoch 2560/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2796 - acc: 0.4957 - val_loss: 2.5702 - val_acc: 0.2367\n",
      "Epoch 2561/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.4986 - val_loss: 2.5648 - val_acc: 0.2300\n",
      "Epoch 2562/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2786 - acc: 0.4971 - val_loss: 2.5608 - val_acc: 0.2367\n",
      "Epoch 2563/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.4957 - val_loss: 2.5730 - val_acc: 0.2367\n",
      "Epoch 2564/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2802 - acc: 0.4957 - val_loss: 2.5810 - val_acc: 0.2400\n",
      "Epoch 2565/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.5029 - val_loss: 2.5707 - val_acc: 0.2433\n",
      "Epoch 2566/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2796 - acc: 0.4957 - val_loss: 2.5768 - val_acc: 0.2433\n",
      "Epoch 2567/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2795 - acc: 0.5000 - val_loss: 2.5740 - val_acc: 0.2333\n",
      "Epoch 2568/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2788 - acc: 0.4929 - val_loss: 2.5694 - val_acc: 0.2367\n",
      "Epoch 2569/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2779 - acc: 0.5000 - val_loss: 2.5572 - val_acc: 0.2367\n",
      "Epoch 2570/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2791 - acc: 0.4929 - val_loss: 2.5894 - val_acc: 0.2400\n",
      "Epoch 2571/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2784 - acc: 0.5000 - val_loss: 2.5721 - val_acc: 0.2333\n",
      "Epoch 2572/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2788 - acc: 0.5029 - val_loss: 2.5874 - val_acc: 0.2400\n",
      "Epoch 2573/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2779 - acc: 0.5000 - val_loss: 2.6101 - val_acc: 0.2433\n",
      "Epoch 2574/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2786 - acc: 0.4957 - val_loss: 2.5474 - val_acc: 0.2367\n",
      "Epoch 2575/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2786 - acc: 0.5029 - val_loss: 2.5847 - val_acc: 0.2433\n",
      "Epoch 2576/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2779 - acc: 0.5000 - val_loss: 2.5925 - val_acc: 0.2300\n",
      "Epoch 2577/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2784 - acc: 0.4986 - val_loss: 2.5863 - val_acc: 0.2333\n",
      "Epoch 2578/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2781 - acc: 0.4971 - val_loss: 2.5747 - val_acc: 0.2333\n",
      "Epoch 2579/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2785 - acc: 0.4943 - val_loss: 2.5664 - val_acc: 0.2367\n",
      "Epoch 2580/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2779 - acc: 0.4971 - val_loss: 2.5738 - val_acc: 0.2433\n",
      "Epoch 2581/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2774 - acc: 0.4971 - val_loss: 2.5633 - val_acc: 0.2300\n",
      "Epoch 2582/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2784 - acc: 0.5014 - val_loss: 2.5988 - val_acc: 0.2433\n",
      "Epoch 2583/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2785 - acc: 0.4957 - val_loss: 2.5575 - val_acc: 0.2367\n",
      "Epoch 2584/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.5014 - val_loss: 2.5854 - val_acc: 0.2433\n",
      "Epoch 2585/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.4957 - val_loss: 2.5583 - val_acc: 0.2400\n",
      "Epoch 2586/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2776 - acc: 0.5014 - val_loss: 2.5723 - val_acc: 0.2300\n",
      "Epoch 2587/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2776 - acc: 0.4971 - val_loss: 2.5914 - val_acc: 0.2433\n",
      "Epoch 2588/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2780 - acc: 0.4971 - val_loss: 2.5728 - val_acc: 0.2367\n",
      "Epoch 2589/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.4986 - val_loss: 2.5810 - val_acc: 0.2367\n",
      "Epoch 2590/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2772 - acc: 0.4957 - val_loss: 2.5759 - val_acc: 0.2267\n",
      "Epoch 2591/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2771 - acc: 0.5014 - val_loss: 2.5560 - val_acc: 0.2333\n",
      "Epoch 2592/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2775 - acc: 0.4971 - val_loss: 2.6022 - val_acc: 0.2433\n",
      "Epoch 2593/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2771 - acc: 0.4957 - val_loss: 2.5623 - val_acc: 0.2333\n",
      "Epoch 2594/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2778 - acc: 0.5000 - val_loss: 2.5675 - val_acc: 0.2433\n",
      "Epoch 2595/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.4914 - val_loss: 2.5748 - val_acc: 0.2300\n",
      "Epoch 2596/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2776 - acc: 0.4986 - val_loss: 2.5723 - val_acc: 0.2333\n",
      "Epoch 2597/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2776 - acc: 0.5029 - val_loss: 2.5696 - val_acc: 0.2333\n",
      "Epoch 2598/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2768 - acc: 0.4986 - val_loss: 2.5624 - val_acc: 0.2333\n",
      "Epoch 2599/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.4986 - val_loss: 2.5655 - val_acc: 0.2233\n",
      "Epoch 2600/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.4971 - val_loss: 2.5735 - val_acc: 0.2333\n",
      "Epoch 2601/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2767 - acc: 0.5000 - val_loss: 2.5791 - val_acc: 0.2300\n",
      "Epoch 2602/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2780 - acc: 0.5014 - val_loss: 2.6064 - val_acc: 0.2400\n",
      "Epoch 2603/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2771 - acc: 0.4986 - val_loss: 2.5504 - val_acc: 0.2367\n",
      "Epoch 2604/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.5000 - val_loss: 2.5858 - val_acc: 0.2367\n",
      "Epoch 2605/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2777 - acc: 0.4971 - val_loss: 2.5756 - val_acc: 0.2267\n",
      "Epoch 2606/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2771 - acc: 0.5000 - val_loss: 2.5744 - val_acc: 0.2333\n",
      "Epoch 2607/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.5000 - val_loss: 2.5657 - val_acc: 0.2367\n",
      "Epoch 2608/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2767 - acc: 0.4943 - val_loss: 2.5968 - val_acc: 0.2433\n",
      "Epoch 2609/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2767 - acc: 0.4957 - val_loss: 2.5677 - val_acc: 0.2333\n",
      "Epoch 2610/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2768 - acc: 0.5000 - val_loss: 2.5363 - val_acc: 0.2333\n",
      "Epoch 2611/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2765 - acc: 0.4971 - val_loss: 2.5644 - val_acc: 0.2400\n",
      "Epoch 2612/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2770 - acc: 0.5014 - val_loss: 2.5945 - val_acc: 0.2367\n",
      "Epoch 2613/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.4957 - val_loss: 2.5752 - val_acc: 0.2367\n",
      "Epoch 2614/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2761 - acc: 0.5014 - val_loss: 2.5962 - val_acc: 0.2400\n",
      "Epoch 2615/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2762 - acc: 0.4986 - val_loss: 2.5770 - val_acc: 0.2333\n",
      "Epoch 2616/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2764 - acc: 0.4971 - val_loss: 2.5824 - val_acc: 0.2333\n",
      "Epoch 2617/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.5029 - val_loss: 2.5856 - val_acc: 0.2367\n",
      "Epoch 2618/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2771 - acc: 0.4957 - val_loss: 2.5833 - val_acc: 0.2433\n",
      "Epoch 2619/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2763 - acc: 0.5014 - val_loss: 2.5834 - val_acc: 0.2400\n",
      "Epoch 2620/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2764 - acc: 0.5029 - val_loss: 2.5882 - val_acc: 0.2400\n",
      "Epoch 2621/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2762 - acc: 0.5029 - val_loss: 2.5898 - val_acc: 0.2333\n",
      "Epoch 2622/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2766 - acc: 0.4943 - val_loss: 2.5669 - val_acc: 0.2400\n",
      "Epoch 2623/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2762 - acc: 0.4943 - val_loss: 2.5896 - val_acc: 0.2367\n",
      "Epoch 2624/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2752 - acc: 0.5029 - val_loss: 2.5697 - val_acc: 0.2333\n",
      "Epoch 2625/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2757 - acc: 0.4971 - val_loss: 2.5933 - val_acc: 0.2333\n",
      "Epoch 2626/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2761 - acc: 0.4971 - val_loss: 2.5967 - val_acc: 0.2433\n",
      "Epoch 2627/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2758 - acc: 0.5000 - val_loss: 2.5867 - val_acc: 0.2433\n",
      "Epoch 2628/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2751 - acc: 0.4957 - val_loss: 2.5845 - val_acc: 0.2367\n",
      "Epoch 2629/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2755 - acc: 0.4971 - val_loss: 2.5865 - val_acc: 0.2433\n",
      "Epoch 2630/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2755 - acc: 0.4943 - val_loss: 2.5584 - val_acc: 0.2333\n",
      "Epoch 2631/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2760 - acc: 0.5029 - val_loss: 2.5750 - val_acc: 0.2267\n",
      "Epoch 2632/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2762 - acc: 0.4957 - val_loss: 2.5833 - val_acc: 0.2333\n",
      "Epoch 2633/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2751 - acc: 0.5000 - val_loss: 2.5871 - val_acc: 0.2267\n",
      "Epoch 2634/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2753 - acc: 0.5000 - val_loss: 2.5805 - val_acc: 0.2300\n",
      "Epoch 2635/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2759 - acc: 0.4971 - val_loss: 2.5840 - val_acc: 0.2333\n",
      "Epoch 2636/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2757 - acc: 0.4986 - val_loss: 2.5895 - val_acc: 0.2367\n",
      "Epoch 2637/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2764 - acc: 0.5014 - val_loss: 2.5899 - val_acc: 0.2400\n",
      "Epoch 2638/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2757 - acc: 0.4986 - val_loss: 2.5835 - val_acc: 0.2333\n",
      "Epoch 2639/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2755 - acc: 0.4971 - val_loss: 2.5675 - val_acc: 0.2367\n",
      "Epoch 2640/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2754 - acc: 0.5057 - val_loss: 2.6096 - val_acc: 0.2367\n",
      "Epoch 2641/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2747 - acc: 0.5014 - val_loss: 2.6409 - val_acc: 0.2400\n",
      "Epoch 2642/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2750 - acc: 0.4957 - val_loss: 2.5681 - val_acc: 0.2367\n",
      "Epoch 2643/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2747 - acc: 0.5071 - val_loss: 2.5907 - val_acc: 0.2333\n",
      "Epoch 2644/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2755 - acc: 0.4971 - val_loss: 2.6075 - val_acc: 0.2400\n",
      "Epoch 2645/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2749 - acc: 0.4957 - val_loss: 2.6028 - val_acc: 0.2433\n",
      "Epoch 2646/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2755 - acc: 0.4986 - val_loss: 2.5974 - val_acc: 0.2400\n",
      "Epoch 2647/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2751 - acc: 0.4986 - val_loss: 2.5914 - val_acc: 0.2333\n",
      "Epoch 2648/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2748 - acc: 0.4986 - val_loss: 2.6065 - val_acc: 0.2400\n",
      "Epoch 2649/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2752 - acc: 0.5014 - val_loss: 2.5767 - val_acc: 0.2333\n",
      "Epoch 2650/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2744 - acc: 0.4971 - val_loss: 2.5541 - val_acc: 0.2367\n",
      "Epoch 2651/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2741 - acc: 0.4957 - val_loss: 2.5847 - val_acc: 0.2300\n",
      "Epoch 2652/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2742 - acc: 0.5043 - val_loss: 2.5797 - val_acc: 0.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2653/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2749 - acc: 0.5029 - val_loss: 2.6083 - val_acc: 0.2367\n",
      "Epoch 2654/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2745 - acc: 0.4929 - val_loss: 2.5911 - val_acc: 0.2367\n",
      "Epoch 2655/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2748 - acc: 0.5014 - val_loss: 2.5972 - val_acc: 0.2400\n",
      "Epoch 2656/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2745 - acc: 0.5014 - val_loss: 2.5987 - val_acc: 0.2400\n",
      "Epoch 2657/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2742 - acc: 0.4957 - val_loss: 2.5634 - val_acc: 0.2267\n",
      "Epoch 2658/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2742 - acc: 0.4957 - val_loss: 2.5717 - val_acc: 0.2433\n",
      "Epoch 2659/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2740 - acc: 0.4986 - val_loss: 2.5818 - val_acc: 0.2267\n",
      "Epoch 2660/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2741 - acc: 0.5014 - val_loss: 2.6115 - val_acc: 0.2400\n",
      "Epoch 2661/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2751 - acc: 0.4986 - val_loss: 2.5921 - val_acc: 0.2367\n",
      "Epoch 2662/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2749 - acc: 0.4957 - val_loss: 2.5808 - val_acc: 0.2400\n",
      "Epoch 2663/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2740 - acc: 0.5000 - val_loss: 2.5742 - val_acc: 0.2233\n",
      "Epoch 2664/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2740 - acc: 0.5029 - val_loss: 2.5850 - val_acc: 0.2333\n",
      "Epoch 2665/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2742 - acc: 0.5014 - val_loss: 2.5703 - val_acc: 0.2267\n",
      "Epoch 2666/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2746 - acc: 0.4986 - val_loss: 2.5859 - val_acc: 0.2367\n",
      "Epoch 2667/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2732 - acc: 0.5029 - val_loss: 2.5742 - val_acc: 0.2333\n",
      "Epoch 2668/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2738 - acc: 0.4957 - val_loss: 2.6231 - val_acc: 0.2367\n",
      "Epoch 2669/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2742 - acc: 0.5014 - val_loss: 2.6049 - val_acc: 0.2400\n",
      "Epoch 2670/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2737 - acc: 0.5029 - val_loss: 2.6125 - val_acc: 0.2433\n",
      "Epoch 2671/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2738 - acc: 0.5014 - val_loss: 2.6123 - val_acc: 0.2400\n",
      "Epoch 2672/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2735 - acc: 0.5029 - val_loss: 2.6160 - val_acc: 0.2400\n",
      "Epoch 2673/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2741 - acc: 0.4986 - val_loss: 2.5795 - val_acc: 0.2333\n",
      "Epoch 2674/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2735 - acc: 0.5000 - val_loss: 2.5813 - val_acc: 0.2367\n",
      "Epoch 2675/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2734 - acc: 0.5014 - val_loss: 2.5738 - val_acc: 0.2400\n",
      "Epoch 2676/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2737 - acc: 0.5014 - val_loss: 2.6031 - val_acc: 0.2467\n",
      "Epoch 2677/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2734 - acc: 0.5000 - val_loss: 2.6088 - val_acc: 0.2400\n",
      "Epoch 2678/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2740 - acc: 0.5043 - val_loss: 2.5977 - val_acc: 0.2433\n",
      "Epoch 2679/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2737 - acc: 0.5000 - val_loss: 2.5979 - val_acc: 0.2367\n",
      "Epoch 2680/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2761 - acc: 0.5000 - val_loss: 2.5768 - val_acc: 0.2400\n",
      "Epoch 2681/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2739 - acc: 0.4986 - val_loss: 2.6105 - val_acc: 0.2433\n",
      "Epoch 2682/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2733 - acc: 0.4986 - val_loss: 2.5874 - val_acc: 0.2333\n",
      "Epoch 2683/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2736 - acc: 0.4929 - val_loss: 2.5956 - val_acc: 0.2367\n",
      "Epoch 2684/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2727 - acc: 0.4986 - val_loss: 2.5879 - val_acc: 0.2400\n",
      "Epoch 2685/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2729 - acc: 0.5000 - val_loss: 2.5904 - val_acc: 0.2333\n",
      "Epoch 2686/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2732 - acc: 0.5000 - val_loss: 2.5968 - val_acc: 0.2367\n",
      "Epoch 2687/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2731 - acc: 0.4971 - val_loss: 2.5904 - val_acc: 0.2267\n",
      "Epoch 2688/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2733 - acc: 0.5014 - val_loss: 2.6151 - val_acc: 0.2400\n",
      "Epoch 2689/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2723 - acc: 0.5029 - val_loss: 2.5993 - val_acc: 0.2333\n",
      "Epoch 2690/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2735 - acc: 0.5000 - val_loss: 2.5586 - val_acc: 0.2367\n",
      "Epoch 2691/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2735 - acc: 0.4943 - val_loss: 2.6122 - val_acc: 0.2367\n",
      "Epoch 2692/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2725 - acc: 0.4886 - val_loss: 2.6318 - val_acc: 0.2400\n",
      "Epoch 2693/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2736 - acc: 0.5057 - val_loss: 2.6121 - val_acc: 0.2433\n",
      "Epoch 2694/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2726 - acc: 0.4957 - val_loss: 2.6125 - val_acc: 0.2367\n",
      "Epoch 2695/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2717 - acc: 0.5071 - val_loss: 2.6006 - val_acc: 0.2367\n",
      "Epoch 2696/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2722 - acc: 0.5000 - val_loss: 2.5890 - val_acc: 0.2233\n",
      "Epoch 2697/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2731 - acc: 0.4986 - val_loss: 2.5836 - val_acc: 0.2333\n",
      "Epoch 2698/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2729 - acc: 0.5000 - val_loss: 2.6071 - val_acc: 0.2433\n",
      "Epoch 2699/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2728 - acc: 0.4986 - val_loss: 2.5985 - val_acc: 0.2367\n",
      "Epoch 2700/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2723 - acc: 0.4971 - val_loss: 2.6097 - val_acc: 0.2400\n",
      "Epoch 2701/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2725 - acc: 0.5043 - val_loss: 2.6185 - val_acc: 0.2433\n",
      "Epoch 2702/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2722 - acc: 0.5000 - val_loss: 2.6176 - val_acc: 0.2367\n",
      "Epoch 2703/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2722 - acc: 0.4971 - val_loss: 2.6225 - val_acc: 0.2367\n",
      "Epoch 2704/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2717 - acc: 0.4971 - val_loss: 2.6021 - val_acc: 0.2367\n",
      "Epoch 2705/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2719 - acc: 0.4957 - val_loss: 2.5971 - val_acc: 0.2333\n",
      "Epoch 2706/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2727 - acc: 0.4971 - val_loss: 2.5978 - val_acc: 0.2333\n",
      "Epoch 2707/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2727 - acc: 0.5014 - val_loss: 2.6084 - val_acc: 0.2367\n",
      "Epoch 2708/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2720 - acc: 0.4943 - val_loss: 2.6164 - val_acc: 0.2400\n",
      "Epoch 2709/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2714 - acc: 0.4986 - val_loss: 2.6079 - val_acc: 0.2333\n",
      "Epoch 2710/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2716 - acc: 0.5043 - val_loss: 2.6179 - val_acc: 0.2400\n",
      "Epoch 2711/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2724 - acc: 0.5014 - val_loss: 2.6040 - val_acc: 0.2367\n",
      "Epoch 2712/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2715 - acc: 0.4971 - val_loss: 2.5754 - val_acc: 0.2267\n",
      "Epoch 2713/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2719 - acc: 0.5000 - val_loss: 2.6302 - val_acc: 0.2367\n",
      "Epoch 2714/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2720 - acc: 0.5014 - val_loss: 2.6270 - val_acc: 0.2400\n",
      "Epoch 2715/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2723 - acc: 0.5000 - val_loss: 2.6068 - val_acc: 0.2433\n",
      "Epoch 2716/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2714 - acc: 0.4986 - val_loss: 2.5939 - val_acc: 0.2333\n",
      "Epoch 2717/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2714 - acc: 0.4914 - val_loss: 2.6031 - val_acc: 0.2333\n",
      "Epoch 2718/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2717 - acc: 0.5029 - val_loss: 2.6154 - val_acc: 0.2333\n",
      "Epoch 2719/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2719 - acc: 0.5014 - val_loss: 2.5929 - val_acc: 0.2300\n",
      "Epoch 2720/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2720 - acc: 0.5029 - val_loss: 2.6036 - val_acc: 0.2367\n",
      "Epoch 2721/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2720 - acc: 0.4971 - val_loss: 2.6017 - val_acc: 0.2367\n",
      "Epoch 2722/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2712 - acc: 0.5014 - val_loss: 2.6046 - val_acc: 0.2333\n",
      "Epoch 2723/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2715 - acc: 0.5029 - val_loss: 2.6150 - val_acc: 0.2400\n",
      "Epoch 2724/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2719 - acc: 0.4971 - val_loss: 2.6119 - val_acc: 0.2400\n",
      "Epoch 2725/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2712 - acc: 0.5029 - val_loss: 2.6047 - val_acc: 0.2300\n",
      "Epoch 2726/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2709 - acc: 0.5043 - val_loss: 2.6244 - val_acc: 0.2400\n",
      "Epoch 2727/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2706 - acc: 0.5029 - val_loss: 2.6130 - val_acc: 0.2333\n",
      "Epoch 2728/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2702 - acc: 0.4986 - val_loss: 2.5806 - val_acc: 0.2367\n",
      "Epoch 2729/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2710 - acc: 0.4971 - val_loss: 2.6159 - val_acc: 0.2400\n",
      "Epoch 2730/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2708 - acc: 0.4971 - val_loss: 2.5853 - val_acc: 0.2267\n",
      "Epoch 2731/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2705 - acc: 0.4971 - val_loss: 2.6053 - val_acc: 0.2300\n",
      "Epoch 2732/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2719 - acc: 0.5014 - val_loss: 2.6205 - val_acc: 0.2400\n",
      "Epoch 2733/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2721 - acc: 0.5000 - val_loss: 2.5938 - val_acc: 0.2267\n",
      "Epoch 2734/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2702 - acc: 0.5114 - val_loss: 2.6160 - val_acc: 0.2333\n",
      "Epoch 2735/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2709 - acc: 0.4943 - val_loss: 2.6164 - val_acc: 0.2367\n",
      "Epoch 2736/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2705 - acc: 0.5029 - val_loss: 2.6161 - val_acc: 0.2367\n",
      "Epoch 2737/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2696 - acc: 0.5014 - val_loss: 2.6073 - val_acc: 0.2367\n",
      "Epoch 2738/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2710 - acc: 0.5057 - val_loss: 2.6009 - val_acc: 0.2333\n",
      "Epoch 2739/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2701 - acc: 0.5043 - val_loss: 2.6352 - val_acc: 0.2333\n",
      "Epoch 2740/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2703 - acc: 0.5000 - val_loss: 2.5897 - val_acc: 0.2333\n",
      "Epoch 2741/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2712 - acc: 0.4943 - val_loss: 2.6271 - val_acc: 0.2433\n",
      "Epoch 2742/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2705 - acc: 0.5000 - val_loss: 2.6220 - val_acc: 0.2433\n",
      "Epoch 2743/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2709 - acc: 0.4986 - val_loss: 2.6016 - val_acc: 0.2433\n",
      "Epoch 2744/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2717 - acc: 0.4986 - val_loss: 2.6030 - val_acc: 0.2333\n",
      "Epoch 2745/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2705 - acc: 0.5000 - val_loss: 2.6206 - val_acc: 0.2400\n",
      "Epoch 2746/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2710 - acc: 0.5029 - val_loss: 2.5990 - val_acc: 0.2367\n",
      "Epoch 2747/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2706 - acc: 0.4943 - val_loss: 2.6002 - val_acc: 0.2367\n",
      "Epoch 2748/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2701 - acc: 0.5029 - val_loss: 2.6341 - val_acc: 0.2400\n",
      "Epoch 2749/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2708 - acc: 0.4886 - val_loss: 2.6272 - val_acc: 0.2433\n",
      "Epoch 2750/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2696 - acc: 0.5014 - val_loss: 2.6271 - val_acc: 0.2367\n",
      "Epoch 2751/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2704 - acc: 0.4971 - val_loss: 2.6161 - val_acc: 0.2433\n",
      "Epoch 2752/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2697 - acc: 0.5029 - val_loss: 2.6150 - val_acc: 0.2400\n",
      "Epoch 2753/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2696 - acc: 0.4943 - val_loss: 2.6209 - val_acc: 0.2367\n",
      "Epoch 2754/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2702 - acc: 0.5014 - val_loss: 2.6185 - val_acc: 0.2400\n",
      "Epoch 2755/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2692 - acc: 0.5000 - val_loss: 2.5838 - val_acc: 0.2333\n",
      "Epoch 2756/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2696 - acc: 0.4986 - val_loss: 2.6616 - val_acc: 0.2400\n",
      "Epoch 2757/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2707 - acc: 0.5029 - val_loss: 2.6071 - val_acc: 0.2400\n",
      "Epoch 2758/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2709 - acc: 0.5043 - val_loss: 2.5869 - val_acc: 0.2233\n",
      "Epoch 2759/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2698 - acc: 0.5014 - val_loss: 2.6261 - val_acc: 0.2333\n",
      "Epoch 2760/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2695 - acc: 0.5000 - val_loss: 2.6235 - val_acc: 0.2433\n",
      "Epoch 2761/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2695 - acc: 0.5014 - val_loss: 2.6039 - val_acc: 0.2367\n",
      "Epoch 2762/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.5057 - val_loss: 2.6207 - val_acc: 0.2267\n",
      "Epoch 2763/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2702 - acc: 0.4986 - val_loss: 2.6268 - val_acc: 0.2433\n",
      "Epoch 2764/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2684 - acc: 0.5014 - val_loss: 2.6346 - val_acc: 0.2367\n",
      "Epoch 2765/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2694 - acc: 0.5000 - val_loss: 2.6121 - val_acc: 0.2367\n",
      "Epoch 2766/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2690 - acc: 0.5014 - val_loss: 2.6279 - val_acc: 0.2367\n",
      "Epoch 2767/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2697 - acc: 0.5029 - val_loss: 2.6283 - val_acc: 0.2367\n",
      "Epoch 2768/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2691 - acc: 0.5029 - val_loss: 2.6363 - val_acc: 0.2367\n",
      "Epoch 2769/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2684 - acc: 0.5000 - val_loss: 2.6290 - val_acc: 0.2400\n",
      "Epoch 2770/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2688 - acc: 0.4971 - val_loss: 2.6114 - val_acc: 0.2267\n",
      "Epoch 2771/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2683 - acc: 0.5057 - val_loss: 2.6032 - val_acc: 0.2233\n",
      "Epoch 2772/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2715 - acc: 0.4957 - val_loss: 2.6058 - val_acc: 0.2300\n",
      "Epoch 2773/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2691 - acc: 0.5029 - val_loss: 2.6323 - val_acc: 0.2400\n",
      "Epoch 2774/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.4986 - val_loss: 2.5978 - val_acc: 0.2333\n",
      "Epoch 2775/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.5057 - val_loss: 2.6183 - val_acc: 0.2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2776/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2692 - acc: 0.4986 - val_loss: 2.6194 - val_acc: 0.2433\n",
      "Epoch 2777/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2697 - acc: 0.5029 - val_loss: 2.6436 - val_acc: 0.2400\n",
      "Epoch 2778/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2694 - acc: 0.4986 - val_loss: 2.6305 - val_acc: 0.2367\n",
      "Epoch 2779/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2676 - acc: 0.4971 - val_loss: 2.6150 - val_acc: 0.2367\n",
      "Epoch 2780/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2690 - acc: 0.5014 - val_loss: 2.6420 - val_acc: 0.2400\n",
      "Epoch 2781/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2693 - acc: 0.5029 - val_loss: 2.6240 - val_acc: 0.2400\n",
      "Epoch 2782/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2686 - acc: 0.5000 - val_loss: 2.5881 - val_acc: 0.2233\n",
      "Epoch 2783/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2686 - acc: 0.4986 - val_loss: 2.6375 - val_acc: 0.2400\n",
      "Epoch 2784/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2678 - acc: 0.5014 - val_loss: 2.6339 - val_acc: 0.2367\n",
      "Epoch 2785/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2684 - acc: 0.5086 - val_loss: 2.6262 - val_acc: 0.2367\n",
      "Epoch 2786/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.4986 - val_loss: 2.5485 - val_acc: 0.2333\n",
      "Epoch 2787/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.5000 - val_loss: 2.6192 - val_acc: 0.2400\n",
      "Epoch 2788/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2686 - acc: 0.4971 - val_loss: 2.6288 - val_acc: 0.2433\n",
      "Epoch 2789/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2683 - acc: 0.5014 - val_loss: 2.6421 - val_acc: 0.2433\n",
      "Epoch 2790/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2685 - acc: 0.5014 - val_loss: 2.6356 - val_acc: 0.2333\n",
      "Epoch 2791/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2688 - acc: 0.4971 - val_loss: 2.6131 - val_acc: 0.2400\n",
      "Epoch 2792/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2685 - acc: 0.4971 - val_loss: 2.6272 - val_acc: 0.2367\n",
      "Epoch 2793/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2674 - acc: 0.5057 - val_loss: 2.6011 - val_acc: 0.2233\n",
      "Epoch 2794/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2685 - acc: 0.4986 - val_loss: 2.6269 - val_acc: 0.2367\n",
      "Epoch 2795/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2678 - acc: 0.5000 - val_loss: 2.6047 - val_acc: 0.2233\n",
      "Epoch 2796/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2683 - acc: 0.4986 - val_loss: 2.6275 - val_acc: 0.2333\n",
      "Epoch 2797/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2683 - acc: 0.5029 - val_loss: 2.6291 - val_acc: 0.2400\n",
      "Epoch 2798/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2673 - acc: 0.5057 - val_loss: 2.6442 - val_acc: 0.2400\n",
      "Epoch 2799/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2677 - acc: 0.5014 - val_loss: 2.6113 - val_acc: 0.2367\n",
      "Epoch 2800/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2689 - acc: 0.5000 - val_loss: 2.6200 - val_acc: 0.2400\n",
      "Epoch 2801/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2679 - acc: 0.5071 - val_loss: 2.6521 - val_acc: 0.2433\n",
      "Epoch 2802/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2686 - acc: 0.5029 - val_loss: 2.6322 - val_acc: 0.2400\n",
      "Epoch 2803/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2673 - acc: 0.4986 - val_loss: 2.6269 - val_acc: 0.2367\n",
      "Epoch 2804/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2678 - acc: 0.5043 - val_loss: 2.6342 - val_acc: 0.2367\n",
      "Epoch 2805/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2674 - acc: 0.4986 - val_loss: 2.6293 - val_acc: 0.2400\n",
      "Epoch 2806/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2676 - acc: 0.5000 - val_loss: 2.6431 - val_acc: 0.2367\n",
      "Epoch 2807/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2676 - acc: 0.5043 - val_loss: 2.6327 - val_acc: 0.2333\n",
      "Epoch 2808/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2672 - acc: 0.5029 - val_loss: 2.6158 - val_acc: 0.2367\n",
      "Epoch 2809/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2675 - acc: 0.5000 - val_loss: 2.6329 - val_acc: 0.2367\n",
      "Epoch 2810/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2673 - acc: 0.5000 - val_loss: 2.6260 - val_acc: 0.2367\n",
      "Epoch 2811/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2669 - acc: 0.4971 - val_loss: 2.6358 - val_acc: 0.2367\n",
      "Epoch 2812/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2677 - acc: 0.5014 - val_loss: 2.6228 - val_acc: 0.2333\n",
      "Epoch 2813/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2677 - acc: 0.5071 - val_loss: 2.6478 - val_acc: 0.2400\n",
      "Epoch 2814/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2672 - acc: 0.5014 - val_loss: 2.6212 - val_acc: 0.2267\n",
      "Epoch 2815/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2672 - acc: 0.5029 - val_loss: 2.6464 - val_acc: 0.2333\n",
      "Epoch 2816/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2675 - acc: 0.5014 - val_loss: 2.6240 - val_acc: 0.2400\n",
      "Epoch 2817/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2666 - acc: 0.4986 - val_loss: 2.6319 - val_acc: 0.2433\n",
      "Epoch 2818/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2670 - acc: 0.5000 - val_loss: 2.6298 - val_acc: 0.2367\n",
      "Epoch 2819/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2673 - acc: 0.5000 - val_loss: 2.6237 - val_acc: 0.2400\n",
      "Epoch 2820/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2676 - acc: 0.5029 - val_loss: 2.6055 - val_acc: 0.2367\n",
      "Epoch 2821/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2679 - acc: 0.5014 - val_loss: 2.6452 - val_acc: 0.2433\n",
      "Epoch 2822/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2668 - acc: 0.5014 - val_loss: 2.6391 - val_acc: 0.2433\n",
      "Epoch 2823/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2664 - acc: 0.5029 - val_loss: 2.6382 - val_acc: 0.2400\n",
      "Epoch 2824/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2664 - acc: 0.5029 - val_loss: 2.6398 - val_acc: 0.2433\n",
      "Epoch 2825/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2672 - acc: 0.5029 - val_loss: 2.6379 - val_acc: 0.2433\n",
      "Epoch 2826/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2659 - acc: 0.5014 - val_loss: 2.6533 - val_acc: 0.2433\n",
      "Epoch 2827/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2670 - acc: 0.5000 - val_loss: 2.6165 - val_acc: 0.2367\n",
      "Epoch 2828/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2674 - acc: 0.4986 - val_loss: 2.6483 - val_acc: 0.2433\n",
      "Epoch 2829/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2670 - acc: 0.4986 - val_loss: 2.6193 - val_acc: 0.2267\n",
      "Epoch 2830/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2657 - acc: 0.5057 - val_loss: 2.6143 - val_acc: 0.2333\n",
      "Epoch 2831/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2673 - acc: 0.5071 - val_loss: 2.6125 - val_acc: 0.2333\n",
      "Epoch 2832/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2662 - acc: 0.5043 - val_loss: 2.6622 - val_acc: 0.2367\n",
      "Epoch 2833/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2663 - acc: 0.5014 - val_loss: 2.6336 - val_acc: 0.2367\n",
      "Epoch 2834/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2667 - acc: 0.4986 - val_loss: 2.6342 - val_acc: 0.2433\n",
      "Epoch 2835/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2662 - acc: 0.4986 - val_loss: 2.6582 - val_acc: 0.2333\n",
      "Epoch 2836/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2664 - acc: 0.4986 - val_loss: 2.6222 - val_acc: 0.2367\n",
      "Epoch 2837/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2659 - acc: 0.5014 - val_loss: 2.6427 - val_acc: 0.2433\n",
      "Epoch 2838/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2662 - acc: 0.5071 - val_loss: 2.5930 - val_acc: 0.2400\n",
      "Epoch 2839/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2660 - acc: 0.5029 - val_loss: 2.6334 - val_acc: 0.2433\n",
      "Epoch 2840/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2660 - acc: 0.4971 - val_loss: 2.6370 - val_acc: 0.2400\n",
      "Epoch 2841/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2659 - acc: 0.4971 - val_loss: 2.6427 - val_acc: 0.2333\n",
      "Epoch 2842/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2660 - acc: 0.5043 - val_loss: 2.6284 - val_acc: 0.2367\n",
      "Epoch 2843/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2662 - acc: 0.5000 - val_loss: 2.6289 - val_acc: 0.2233\n",
      "Epoch 2844/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2654 - acc: 0.5029 - val_loss: 2.6356 - val_acc: 0.2433\n",
      "Epoch 2845/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2661 - acc: 0.5029 - val_loss: 2.6531 - val_acc: 0.2433\n",
      "Epoch 2846/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2648 - acc: 0.5043 - val_loss: 2.6806 - val_acc: 0.2400\n",
      "Epoch 2847/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2656 - acc: 0.5014 - val_loss: 2.6472 - val_acc: 0.2400\n",
      "Epoch 2848/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2655 - acc: 0.4986 - val_loss: 2.6315 - val_acc: 0.2300\n",
      "Epoch 2849/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2656 - acc: 0.5029 - val_loss: 2.6429 - val_acc: 0.2433\n",
      "Epoch 2850/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2655 - acc: 0.5014 - val_loss: 2.6381 - val_acc: 0.2433\n",
      "Epoch 2851/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2659 - acc: 0.4986 - val_loss: 2.6329 - val_acc: 0.2433\n",
      "Epoch 2852/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2652 - acc: 0.5029 - val_loss: 2.6683 - val_acc: 0.2400\n",
      "Epoch 2853/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2655 - acc: 0.4986 - val_loss: 2.6565 - val_acc: 0.2367\n",
      "Epoch 2854/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2658 - acc: 0.5000 - val_loss: 2.6214 - val_acc: 0.2367\n",
      "Epoch 2855/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2650 - acc: 0.5043 - val_loss: 2.6475 - val_acc: 0.2433\n",
      "Epoch 2856/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2653 - acc: 0.5000 - val_loss: 2.6525 - val_acc: 0.2367\n",
      "Epoch 2857/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2650 - acc: 0.5000 - val_loss: 2.6606 - val_acc: 0.2367\n",
      "Epoch 2858/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2650 - acc: 0.5014 - val_loss: 2.6547 - val_acc: 0.2433\n",
      "Epoch 2859/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2648 - acc: 0.4986 - val_loss: 2.6259 - val_acc: 0.2267\n",
      "Epoch 2860/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2650 - acc: 0.5071 - val_loss: 2.6093 - val_acc: 0.2400\n",
      "Epoch 2861/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2650 - acc: 0.5057 - val_loss: 2.6517 - val_acc: 0.2333\n",
      "Epoch 2862/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2648 - acc: 0.5014 - val_loss: 2.6424 - val_acc: 0.2433\n",
      "Epoch 2863/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2644 - acc: 0.5000 - val_loss: 2.6390 - val_acc: 0.2333\n",
      "Epoch 2864/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2652 - acc: 0.4971 - val_loss: 2.6248 - val_acc: 0.2300\n",
      "Epoch 2865/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2651 - acc: 0.5014 - val_loss: 2.6624 - val_acc: 0.2433\n",
      "Epoch 2866/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2652 - acc: 0.5057 - val_loss: 2.6514 - val_acc: 0.2367\n",
      "Epoch 2867/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2644 - acc: 0.5057 - val_loss: 2.6542 - val_acc: 0.2367\n",
      "Epoch 2868/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2653 - acc: 0.5057 - val_loss: 2.5953 - val_acc: 0.2367\n",
      "Epoch 2869/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2645 - acc: 0.5000 - val_loss: 2.6369 - val_acc: 0.2367\n",
      "Epoch 2870/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2643 - acc: 0.4971 - val_loss: 2.6871 - val_acc: 0.2400\n",
      "Epoch 2871/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2649 - acc: 0.5071 - val_loss: 2.6651 - val_acc: 0.2400\n",
      "Epoch 2872/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2646 - acc: 0.5014 - val_loss: 2.6450 - val_acc: 0.2333\n",
      "Epoch 2873/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2643 - acc: 0.5029 - val_loss: 2.6233 - val_acc: 0.2267\n",
      "Epoch 2874/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2648 - acc: 0.5086 - val_loss: 2.6105 - val_acc: 0.2367\n",
      "Epoch 2875/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2644 - acc: 0.5014 - val_loss: 2.6417 - val_acc: 0.2433\n",
      "Epoch 2876/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2656 - acc: 0.4986 - val_loss: 2.6753 - val_acc: 0.2400\n",
      "Epoch 2877/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2646 - acc: 0.4986 - val_loss: 2.6633 - val_acc: 0.2433\n",
      "Epoch 2878/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2643 - acc: 0.5014 - val_loss: 2.6313 - val_acc: 0.2400\n",
      "Epoch 2879/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2633 - acc: 0.5029 - val_loss: 2.6536 - val_acc: 0.2400\n",
      "Epoch 2880/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2639 - acc: 0.5029 - val_loss: 2.6594 - val_acc: 0.2433\n",
      "Epoch 2881/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2651 - acc: 0.5000 - val_loss: 2.6428 - val_acc: 0.2400\n",
      "Epoch 2882/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2645 - acc: 0.5014 - val_loss: 2.6134 - val_acc: 0.2367\n",
      "Epoch 2883/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2636 - acc: 0.5014 - val_loss: 2.6169 - val_acc: 0.2267\n",
      "Epoch 2884/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2635 - acc: 0.5029 - val_loss: 2.6928 - val_acc: 0.2433\n",
      "Epoch 2885/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2641 - acc: 0.5029 - val_loss: 2.6207 - val_acc: 0.2267\n",
      "Epoch 2886/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2642 - acc: 0.5000 - val_loss: 2.6400 - val_acc: 0.2400\n",
      "Epoch 2887/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2642 - acc: 0.5057 - val_loss: 2.6498 - val_acc: 0.2400\n",
      "Epoch 2888/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2638 - acc: 0.4986 - val_loss: 2.6682 - val_acc: 0.2367\n",
      "Epoch 2889/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2643 - acc: 0.5071 - val_loss: 2.6222 - val_acc: 0.2400\n",
      "Epoch 2890/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5086 - val_loss: 2.6298 - val_acc: 0.2367\n",
      "Epoch 2891/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2642 - acc: 0.4986 - val_loss: 2.6491 - val_acc: 0.2433\n",
      "Epoch 2892/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5014 - val_loss: 2.6549 - val_acc: 0.2400\n",
      "Epoch 2893/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2647 - acc: 0.4986 - val_loss: 2.6574 - val_acc: 0.2433\n",
      "Epoch 2894/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2638 - acc: 0.5000 - val_loss: 2.6698 - val_acc: 0.2433\n",
      "Epoch 2895/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5043 - val_loss: 2.6618 - val_acc: 0.2433\n",
      "Epoch 2896/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2636 - acc: 0.5043 - val_loss: 2.6573 - val_acc: 0.2433\n",
      "Epoch 2897/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2636 - acc: 0.5071 - val_loss: 2.6854 - val_acc: 0.2433\n",
      "Epoch 2898/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2632 - acc: 0.5014 - val_loss: 2.6683 - val_acc: 0.2433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2899/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5014 - val_loss: 2.6453 - val_acc: 0.2400\n",
      "Epoch 2900/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2629 - acc: 0.5043 - val_loss: 2.6464 - val_acc: 0.2333\n",
      "Epoch 2901/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2630 - acc: 0.5057 - val_loss: 2.6514 - val_acc: 0.2433\n",
      "Epoch 2902/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5057 - val_loss: 2.6810 - val_acc: 0.2433\n",
      "Epoch 2903/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2638 - acc: 0.5014 - val_loss: 2.6442 - val_acc: 0.2400\n",
      "Epoch 2904/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2633 - acc: 0.5071 - val_loss: 2.6391 - val_acc: 0.2333\n",
      "Epoch 2905/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2629 - acc: 0.5071 - val_loss: 2.6562 - val_acc: 0.2400\n",
      "Epoch 2906/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2633 - acc: 0.5043 - val_loss: 2.6522 - val_acc: 0.2400\n",
      "Epoch 2907/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2624 - acc: 0.5014 - val_loss: 2.6636 - val_acc: 0.2333\n",
      "Epoch 2908/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2627 - acc: 0.5029 - val_loss: 2.6457 - val_acc: 0.2367\n",
      "Epoch 2909/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2628 - acc: 0.5043 - val_loss: 2.6299 - val_acc: 0.2333\n",
      "Epoch 2910/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2630 - acc: 0.5086 - val_loss: 2.6397 - val_acc: 0.2400\n",
      "Epoch 2911/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2633 - acc: 0.5029 - val_loss: 2.6133 - val_acc: 0.2367\n",
      "Epoch 2912/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2636 - acc: 0.5043 - val_loss: 2.6215 - val_acc: 0.2267\n",
      "Epoch 2913/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2633 - acc: 0.5014 - val_loss: 2.6405 - val_acc: 0.2433\n",
      "Epoch 2914/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2634 - acc: 0.4986 - val_loss: 2.6461 - val_acc: 0.2433\n",
      "Epoch 2915/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2637 - acc: 0.5014 - val_loss: 2.6167 - val_acc: 0.2367\n",
      "Epoch 2916/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2630 - acc: 0.5000 - val_loss: 2.6793 - val_acc: 0.2433\n",
      "Epoch 2917/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2625 - acc: 0.5000 - val_loss: 2.6587 - val_acc: 0.2367\n",
      "Epoch 2918/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2635 - acc: 0.5057 - val_loss: 2.6680 - val_acc: 0.2367\n",
      "Epoch 2919/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2618 - acc: 0.5000 - val_loss: 2.6790 - val_acc: 0.2433\n",
      "Epoch 2920/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2625 - acc: 0.5000 - val_loss: 2.6452 - val_acc: 0.2367\n",
      "Epoch 2921/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2615 - acc: 0.5071 - val_loss: 2.6237 - val_acc: 0.2233\n",
      "Epoch 2922/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2615 - acc: 0.5014 - val_loss: 2.6614 - val_acc: 0.2400\n",
      "Epoch 2923/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2612 - acc: 0.5057 - val_loss: 2.6501 - val_acc: 0.2400\n",
      "Epoch 2924/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2613 - acc: 0.5057 - val_loss: 2.6608 - val_acc: 0.2400\n",
      "Epoch 2925/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2609 - acc: 0.5014 - val_loss: 2.6877 - val_acc: 0.2433\n",
      "Epoch 2926/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2613 - acc: 0.5086 - val_loss: 2.6654 - val_acc: 0.2333\n",
      "Epoch 2927/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2616 - acc: 0.5057 - val_loss: 2.6975 - val_acc: 0.2400\n",
      "Epoch 2928/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2618 - acc: 0.5086 - val_loss: 2.6492 - val_acc: 0.2333\n",
      "Epoch 2929/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2609 - acc: 0.5071 - val_loss: 2.6719 - val_acc: 0.2433\n",
      "Epoch 2930/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2605 - acc: 0.5086 - val_loss: 2.6949 - val_acc: 0.2400\n",
      "Epoch 2931/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2617 - acc: 0.5043 - val_loss: 2.6799 - val_acc: 0.2433\n",
      "Epoch 2932/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2611 - acc: 0.5071 - val_loss: 2.6759 - val_acc: 0.2433\n",
      "Epoch 2933/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2610 - acc: 0.5043 - val_loss: 2.6493 - val_acc: 0.2433\n",
      "Epoch 2934/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2612 - acc: 0.5057 - val_loss: 2.6706 - val_acc: 0.2433\n",
      "Epoch 2935/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2607 - acc: 0.5043 - val_loss: 2.6643 - val_acc: 0.2433\n",
      "Epoch 2936/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2605 - acc: 0.5086 - val_loss: 2.6420 - val_acc: 0.2367\n",
      "Epoch 2937/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2871 - acc: 0.475 - ETA: 0s - loss: 1.2617 - acc: 0.5029 - val_loss: 2.6750 - val_acc: 0.2433\n",
      "Epoch 2938/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2607 - acc: 0.5000 - val_loss: 2.6670 - val_acc: 0.2433\n",
      "Epoch 2939/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2608 - acc: 0.5029 - val_loss: 2.6371 - val_acc: 0.2367\n",
      "Epoch 2940/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2603 - acc: 0.5057 - val_loss: 2.6765 - val_acc: 0.2433\n",
      "Epoch 2941/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2599 - acc: 0.5043 - val_loss: 2.6540 - val_acc: 0.2300\n",
      "Epoch 2942/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2628 - acc: 0.5057 - val_loss: 2.6727 - val_acc: 0.2433\n",
      "Epoch 2943/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2599 - acc: 0.5057 - val_loss: 2.6924 - val_acc: 0.2433\n",
      "Epoch 2944/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2602 - acc: 0.5029 - val_loss: 2.6709 - val_acc: 0.2433\n",
      "Epoch 2945/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2605 - acc: 0.5071 - val_loss: 2.6832 - val_acc: 0.2367\n",
      "Epoch 2946/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2602 - acc: 0.5043 - val_loss: 2.6954 - val_acc: 0.2433\n",
      "Epoch 2947/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2608 - acc: 0.5057 - val_loss: 2.6639 - val_acc: 0.2433\n",
      "Epoch 2948/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2601 - acc: 0.5071 - val_loss: 2.6806 - val_acc: 0.2433\n",
      "Epoch 2949/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2607 - acc: 0.5071 - val_loss: 2.6459 - val_acc: 0.2367\n",
      "Epoch 2950/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2598 - acc: 0.5100 - val_loss: 2.6698 - val_acc: 0.2433\n",
      "Epoch 2951/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2604 - acc: 0.5043 - val_loss: 2.7002 - val_acc: 0.2433\n",
      "Epoch 2952/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2605 - acc: 0.5000 - val_loss: 2.6436 - val_acc: 0.2400\n",
      "Epoch 2953/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2596 - acc: 0.5043 - val_loss: 2.6823 - val_acc: 0.2333\n",
      "Epoch 2954/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2617 - acc: 0.5029 - val_loss: 2.6744 - val_acc: 0.2400\n",
      "Epoch 2955/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2595 - acc: 0.5057 - val_loss: 2.6376 - val_acc: 0.2233\n",
      "Epoch 2956/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2604 - acc: 0.5071 - val_loss: 2.6839 - val_acc: 0.2433\n",
      "Epoch 2957/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2597 - acc: 0.5071 - val_loss: 2.6774 - val_acc: 0.2433\n",
      "Epoch 2958/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2606 - acc: 0.5057 - val_loss: 2.6654 - val_acc: 0.2433\n",
      "Epoch 2959/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2601 - acc: 0.5014 - val_loss: 2.6624 - val_acc: 0.2400\n",
      "Epoch 2960/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.2601 - acc: 0.5086 - val_loss: 2.6644 - val_acc: 0.2433\n",
      "Epoch 2961/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2593 - acc: 0.5071 - val_loss: 2.6603 - val_acc: 0.2433\n",
      "Epoch 2962/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2594 - acc: 0.5086 - val_loss: 2.6716 - val_acc: 0.2400\n",
      "Epoch 2963/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2591 - acc: 0.5071 - val_loss: 2.6576 - val_acc: 0.2400\n",
      "Epoch 2964/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2595 - acc: 0.5043 - val_loss: 2.6568 - val_acc: 0.2333\n",
      "Epoch 2965/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2599 - acc: 0.5043 - val_loss: 2.6553 - val_acc: 0.2433\n",
      "Epoch 2966/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2593 - acc: 0.5043 - val_loss: 2.6530 - val_acc: 0.2367\n",
      "Epoch 2967/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2592 - acc: 0.5057 - val_loss: 2.6554 - val_acc: 0.2433\n",
      "Epoch 2968/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2592 - acc: 0.5071 - val_loss: 2.6863 - val_acc: 0.2433\n",
      "Epoch 2969/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2596 - acc: 0.5057 - val_loss: 2.6801 - val_acc: 0.2433\n",
      "Epoch 2970/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2589 - acc: 0.5071 - val_loss: 2.7007 - val_acc: 0.2433\n",
      "Epoch 2971/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2595 - acc: 0.5086 - val_loss: 2.6471 - val_acc: 0.2433\n",
      "Epoch 2972/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2595 - acc: 0.5043 - val_loss: 2.6638 - val_acc: 0.2400\n",
      "Epoch 2973/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2588 - acc: 0.5071 - val_loss: 2.6606 - val_acc: 0.2433\n",
      "Epoch 2974/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2585 - acc: 0.5129 - val_loss: 2.6739 - val_acc: 0.2433\n",
      "Epoch 2975/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2590 - acc: 0.5071 - val_loss: 2.6759 - val_acc: 0.2433\n",
      "Epoch 2976/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2590 - acc: 0.5057 - val_loss: 2.6741 - val_acc: 0.2400\n",
      "Epoch 2977/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5071 - val_loss: 2.6912 - val_acc: 0.2433\n",
      "Epoch 2978/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5086 - val_loss: 2.6811 - val_acc: 0.2333\n",
      "Epoch 2979/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5100 - val_loss: 2.6439 - val_acc: 0.2400\n",
      "Epoch 2980/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2588 - acc: 0.5043 - val_loss: 2.6888 - val_acc: 0.2367\n",
      "Epoch 2981/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2587 - acc: 0.5057 - val_loss: 2.6803 - val_acc: 0.2433\n",
      "Epoch 2982/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2596 - acc: 0.5014 - val_loss: 2.6508 - val_acc: 0.2400\n",
      "Epoch 2983/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2588 - acc: 0.5071 - val_loss: 2.6833 - val_acc: 0.2433\n",
      "Epoch 2984/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2586 - acc: 0.5114 - val_loss: 2.6807 - val_acc: 0.2367\n",
      "Epoch 2985/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2587 - acc: 0.5086 - val_loss: 2.6639 - val_acc: 0.2367\n",
      "Epoch 2986/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2585 - acc: 0.5086 - val_loss: 2.6763 - val_acc: 0.2433\n",
      "Epoch 2987/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2585 - acc: 0.5071 - val_loss: 2.6872 - val_acc: 0.2433\n",
      "Epoch 2988/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2587 - acc: 0.5143 - val_loss: 2.6868 - val_acc: 0.2433\n",
      "Epoch 2989/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2582 - acc: 0.5043 - val_loss: 2.6454 - val_acc: 0.2300\n",
      "Epoch 2990/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5100 - val_loss: 2.6730 - val_acc: 0.2300\n",
      "Epoch 2991/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2580 - acc: 0.5086 - val_loss: 2.6821 - val_acc: 0.2300\n",
      "Epoch 2992/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2575 - acc: 0.5143 - val_loss: 2.6686 - val_acc: 0.2433\n",
      "Epoch 2993/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5071 - val_loss: 2.6772 - val_acc: 0.2433\n",
      "Epoch 2994/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2583 - acc: 0.5100 - val_loss: 2.6746 - val_acc: 0.2333\n",
      "Epoch 2995/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2578 - acc: 0.5086 - val_loss: 2.6743 - val_acc: 0.2367\n",
      "Epoch 2996/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2579 - acc: 0.5043 - val_loss: 2.6639 - val_acc: 0.2400\n",
      "Epoch 2997/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2575 - acc: 0.5057 - val_loss: 2.6603 - val_acc: 0.2433\n",
      "Epoch 2998/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2581 - acc: 0.5029 - val_loss: 2.6591 - val_acc: 0.2367\n",
      "Epoch 2999/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2579 - acc: 0.5086 - val_loss: 2.6899 - val_acc: 0.2433\n",
      "Epoch 3000/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.2577 - acc: 0.5071 - val_loss: 2.6584 - val_acc: 0.2333\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUTIGNVqKoihKxvD/AeQznR8+VB2WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272d6d8dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s\n",
      "\n",
      "loss : 2.94597704792\n",
      "accuray : 0.2594\n"
     ]
    }
   ],
   "source": [
    "# 6. 모델 사용하기\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuray : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2072 - acc: 0.1657 - val_loss: 2.1908 - val_acc: 0.1800\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1730 - acc: 0.1729 - val_loss: 2.1631 - val_acc: 0.1867\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1441 - acc: 0.1786 - val_loss: 2.1372 - val_acc: 0.1867\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1177 - acc: 0.1900 - val_loss: 2.1141 - val_acc: 0.1867\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0940 - acc: 0.2029 - val_loss: 2.0931 - val_acc: 0.2033\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.2086 - val_loss: 2.0727 - val_acc: 0.2067\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0521 - acc: 0.2129 - val_loss: 2.0563 - val_acc: 0.2067\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0342 - acc: 0.2157 - val_loss: 2.0409 - val_acc: 0.2033\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0188 - acc: 0.2129 - val_loss: 2.0271 - val_acc: 0.2067\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0042 - acc: 0.2200 - val_loss: 2.0125 - val_acc: 0.2100\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9912 - acc: 0.2186 - val_loss: 2.0037 - val_acc: 0.2100\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9788 - acc: 0.2271 - val_loss: 1.9953 - val_acc: 0.2100\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9684 - acc: 0.2314 - val_loss: 1.9833 - val_acc: 0.2033\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9582 - acc: 0.2214 - val_loss: 1.9749 - val_acc: 0.2033\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9484 - acc: 0.2357 - val_loss: 1.9684 - val_acc: 0.2000\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9393 - acc: 0.2343 - val_loss: 1.9611 - val_acc: 0.2033\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9309 - acc: 0.2300 - val_loss: 1.9536 - val_acc: 0.2100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.2286 - val_loss: 1.9451 - val_acc: 0.2100\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9156 - acc: 0.2386 - val_loss: 1.9391 - val_acc: 0.2100\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9085 - acc: 0.2329 - val_loss: 1.9362 - val_acc: 0.2100\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9011 - acc: 0.2386 - val_loss: 1.9289 - val_acc: 0.2033\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8954 - acc: 0.2357 - val_loss: 1.9234 - val_acc: 0.2100\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8894 - acc: 0.2314 - val_loss: 1.9208 - val_acc: 0.2100\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8830 - acc: 0.2343 - val_loss: 1.9181 - val_acc: 0.2167\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8769 - acc: 0.2300 - val_loss: 1.9107 - val_acc: 0.2167\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8714 - acc: 0.2357 - val_loss: 1.9099 - val_acc: 0.2167\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8662 - acc: 0.2400 - val_loss: 1.9096 - val_acc: 0.2000\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2400 - val_loss: 1.9057 - val_acc: 0.1867\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8566 - acc: 0.2243 - val_loss: 1.8980 - val_acc: 0.2167\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8514 - acc: 0.2471 - val_loss: 1.8973 - val_acc: 0.1933\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8463 - acc: 0.2386 - val_loss: 1.8929 - val_acc: 0.1900\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8424 - acc: 0.2229 - val_loss: 1.8874 - val_acc: 0.2100\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8383 - acc: 0.2314 - val_loss: 1.8812 - val_acc: 0.1967\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8336 - acc: 0.2471 - val_loss: 1.8837 - val_acc: 0.1933\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping() # 조기종료 콜백함수 정의\n",
    "hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272d6c3e470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s\n",
      "\n",
      "loss : 1.91097395439\n",
      "accuray : 0.1994\n"
     ]
    }
   ],
   "source": [
    "# 6. 모델 사용하기\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuray : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2576 - acc: 0.1643 - val_loss: 2.2272 - val_acc: 0.1633\n",
      "Epoch 2/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.2072 - acc: 0.1657 - val_loss: 2.1908 - val_acc: 0.1800\n",
      "Epoch 3/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1730 - acc: 0.1729 - val_loss: 2.1631 - val_acc: 0.1867\n",
      "Epoch 4/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1441 - acc: 0.1786 - val_loss: 2.1372 - val_acc: 0.1867\n",
      "Epoch 5/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.1177 - acc: 0.1900 - val_loss: 2.1141 - val_acc: 0.1867\n",
      "Epoch 6/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0940 - acc: 0.2029 - val_loss: 2.0931 - val_acc: 0.2033\n",
      "Epoch 7/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0719 - acc: 0.2086 - val_loss: 2.0727 - val_acc: 0.2067\n",
      "Epoch 8/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0521 - acc: 0.2129 - val_loss: 2.0564 - val_acc: 0.2067\n",
      "Epoch 9/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0342 - acc: 0.2157 - val_loss: 2.0409 - val_acc: 0.2033\n",
      "Epoch 10/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0188 - acc: 0.2129 - val_loss: 2.0271 - val_acc: 0.2067\n",
      "Epoch 11/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 2.0042 - acc: 0.2200 - val_loss: 2.0125 - val_acc: 0.2100\n",
      "Epoch 12/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9912 - acc: 0.2186 - val_loss: 2.0036 - val_acc: 0.2100\n",
      "Epoch 13/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9788 - acc: 0.2271 - val_loss: 1.9953 - val_acc: 0.2100\n",
      "Epoch 14/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9684 - acc: 0.2329 - val_loss: 1.9833 - val_acc: 0.2033\n",
      "Epoch 15/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9582 - acc: 0.2214 - val_loss: 1.9753 - val_acc: 0.2067\n",
      "Epoch 16/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9484 - acc: 0.2357 - val_loss: 1.9686 - val_acc: 0.2000\n",
      "Epoch 17/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9394 - acc: 0.2343 - val_loss: 1.9612 - val_acc: 0.2033\n",
      "Epoch 18/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9309 - acc: 0.2314 - val_loss: 1.9537 - val_acc: 0.2100\n",
      "Epoch 19/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9232 - acc: 0.2271 - val_loss: 1.9452 - val_acc: 0.2100\n",
      "Epoch 20/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9156 - acc: 0.2386 - val_loss: 1.9392 - val_acc: 0.2100\n",
      "Epoch 21/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9085 - acc: 0.2329 - val_loss: 1.9359 - val_acc: 0.2067\n",
      "Epoch 22/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.9011 - acc: 0.2386 - val_loss: 1.9288 - val_acc: 0.2033\n",
      "Epoch 23/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8954 - acc: 0.2357 - val_loss: 1.9233 - val_acc: 0.2100\n",
      "Epoch 24/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8895 - acc: 0.2314 - val_loss: 1.9200 - val_acc: 0.2067\n",
      "Epoch 25/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8831 - acc: 0.2371 - val_loss: 1.9166 - val_acc: 0.2167\n",
      "Epoch 26/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8771 - acc: 0.2329 - val_loss: 1.9112 - val_acc: 0.2167\n",
      "Epoch 27/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8715 - acc: 0.2400 - val_loss: 1.9101 - val_acc: 0.2167\n",
      "Epoch 28/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8663 - acc: 0.2386 - val_loss: 1.9095 - val_acc: 0.2000\n",
      "Epoch 29/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8615 - acc: 0.2414 - val_loss: 1.9055 - val_acc: 0.1900\n",
      "Epoch 30/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8566 - acc: 0.2257 - val_loss: 1.8973 - val_acc: 0.2167\n",
      "Epoch 31/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8514 - acc: 0.2471 - val_loss: 1.8970 - val_acc: 0.1900\n",
      "Epoch 32/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8464 - acc: 0.2371 - val_loss: 1.8924 - val_acc: 0.1900\n",
      "Epoch 33/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8424 - acc: 0.2229 - val_loss: 1.8873 - val_acc: 0.2100\n",
      "Epoch 34/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8383 - acc: 0.2371 - val_loss: 1.8810 - val_acc: 0.1933\n",
      "Epoch 35/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8336 - acc: 0.2471 - val_loss: 1.8836 - val_acc: 0.1933\n",
      "Epoch 36/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8300 - acc: 0.2357 - val_loss: 1.8755 - val_acc: 0.1967\n",
      "Epoch 37/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8258 - acc: 0.2486 - val_loss: 1.8744 - val_acc: 0.1800\n",
      "Epoch 38/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8221 - acc: 0.2371 - val_loss: 1.8700 - val_acc: 0.1933\n",
      "Epoch 39/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8185 - acc: 0.2457 - val_loss: 1.8705 - val_acc: 0.1767\n",
      "Epoch 40/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8144 - acc: 0.2329 - val_loss: 1.8678 - val_acc: 0.2000\n",
      "Epoch 41/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8105 - acc: 0.2400 - val_loss: 1.8668 - val_acc: 0.1833\n",
      "Epoch 42/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8076 - acc: 0.2471 - val_loss: 1.8635 - val_acc: 0.1800\n",
      "Epoch 43/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8044 - acc: 0.2443 - val_loss: 1.8607 - val_acc: 0.1700\n",
      "Epoch 44/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.8002 - acc: 0.2386 - val_loss: 1.8571 - val_acc: 0.2000\n",
      "Epoch 45/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7970 - acc: 0.2457 - val_loss: 1.8560 - val_acc: 0.1700\n",
      "Epoch 46/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7938 - acc: 0.2229 - val_loss: 1.8530 - val_acc: 0.1900\n",
      "Epoch 47/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7915 - acc: 0.2614 - val_loss: 1.8546 - val_acc: 0.1800\n",
      "Epoch 48/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7886 - acc: 0.2457 - val_loss: 1.8541 - val_acc: 0.1867\n",
      "Epoch 49/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7858 - acc: 0.2486 - val_loss: 1.8509 - val_acc: 0.1833\n",
      "Epoch 50/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7819 - acc: 0.2471 - val_loss: 1.8442 - val_acc: 0.2267\n",
      "Epoch 51/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7794 - acc: 0.2643 - val_loss: 1.8467 - val_acc: 0.1900\n",
      "Epoch 52/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7762 - acc: 0.2500 - val_loss: 1.8410 - val_acc: 0.1933\n",
      "Epoch 53/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7744 - acc: 0.2514 - val_loss: 1.8486 - val_acc: 0.2000\n",
      "Epoch 54/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7716 - acc: 0.2700 - val_loss: 1.8475 - val_acc: 0.1800\n",
      "Epoch 55/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7687 - acc: 0.2500 - val_loss: 1.8364 - val_acc: 0.2067\n",
      "Epoch 56/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7673 - acc: 0.2543 - val_loss: 1.8431 - val_acc: 0.2200\n",
      "Epoch 57/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7641 - acc: 0.2729 - val_loss: 1.8390 - val_acc: 0.2167\n",
      "Epoch 58/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7616 - acc: 0.2571 - val_loss: 1.8347 - val_acc: 0.2267\n",
      "Epoch 59/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7590 - acc: 0.2671 - val_loss: 1.8330 - val_acc: 0.2233\n",
      "Epoch 60/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7552 - acc: 0.2629 - val_loss: 1.8256 - val_acc: 0.2500\n",
      "Epoch 61/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7567 - acc: 0.2814 - val_loss: 1.8335 - val_acc: 0.2400\n",
      "Epoch 62/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7532 - acc: 0.2729 - val_loss: 1.8311 - val_acc: 0.2233\n",
      "Epoch 63/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.7505 - acc: 0.2857 - val_loss: 1.8299 - val_acc: 0.2000\n",
      "Epoch 64/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7484 - acc: 0.2814 - val_loss: 1.8270 - val_acc: 0.2200\n",
      "Epoch 65/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7457 - acc: 0.2814 - val_loss: 1.8299 - val_acc: 0.2033\n",
      "Epoch 66/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7439 - acc: 0.2800 - val_loss: 1.8297 - val_acc: 0.2067\n",
      "Epoch 67/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7419 - acc: 0.2700 - val_loss: 1.8299 - val_acc: 0.2067\n",
      "Epoch 68/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7405 - acc: 0.2729 - val_loss: 1.8238 - val_acc: 0.2000\n",
      "Epoch 69/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7376 - acc: 0.2814 - val_loss: 1.8298 - val_acc: 0.2167\n",
      "Epoch 70/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7356 - acc: 0.2857 - val_loss: 1.8269 - val_acc: 0.2433\n",
      "Epoch 71/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7345 - acc: 0.2814 - val_loss: 1.8214 - val_acc: 0.2167\n",
      "Epoch 72/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7328 - acc: 0.2871 - val_loss: 1.8226 - val_acc: 0.2133\n",
      "Epoch 73/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7298 - acc: 0.2829 - val_loss: 1.8252 - val_acc: 0.2467\n",
      "Epoch 74/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7283 - acc: 0.2843 - val_loss: 1.8257 - val_acc: 0.1967\n",
      "Epoch 75/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7268 - acc: 0.2786 - val_loss: 1.8190 - val_acc: 0.2267\n",
      "Epoch 76/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7255 - acc: 0.2857 - val_loss: 1.8197 - val_acc: 0.2233\n",
      "Epoch 77/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7228 - acc: 0.3043 - val_loss: 1.8232 - val_acc: 0.2033\n",
      "Epoch 78/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7212 - acc: 0.2857 - val_loss: 1.8187 - val_acc: 0.1933\n",
      "Epoch 79/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7199 - acc: 0.2829 - val_loss: 1.8204 - val_acc: 0.2433\n",
      "Epoch 80/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7188 - acc: 0.2929 - val_loss: 1.8197 - val_acc: 0.2067\n",
      "Epoch 81/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7165 - acc: 0.2843 - val_loss: 1.8256 - val_acc: 0.2067\n",
      "Epoch 82/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7142 - acc: 0.2843 - val_loss: 1.8145 - val_acc: 0.2667\n",
      "Epoch 83/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7133 - acc: 0.2857 - val_loss: 1.8190 - val_acc: 0.2400\n",
      "Epoch 84/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7127 - acc: 0.3000 - val_loss: 1.8221 - val_acc: 0.2133\n",
      "Epoch 85/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7097 - acc: 0.2986 - val_loss: 1.8160 - val_acc: 0.2267\n",
      "Epoch 86/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7081 - acc: 0.2800 - val_loss: 1.8146 - val_acc: 0.2633\n",
      "Epoch 87/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7052 - acc: 0.3086 - val_loss: 1.8198 - val_acc: 0.2733\n",
      "Epoch 88/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7062 - acc: 0.3029 - val_loss: 1.8114 - val_acc: 0.2433\n",
      "Epoch 89/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7044 - acc: 0.3029 - val_loss: 1.8172 - val_acc: 0.2167\n",
      "Epoch 90/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7027 - acc: 0.2843 - val_loss: 1.8151 - val_acc: 0.2233\n",
      "Epoch 91/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.7017 - acc: 0.3086 - val_loss: 1.8185 - val_acc: 0.2167\n",
      "Epoch 92/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6987 - acc: 0.3129 - val_loss: 1.8215 - val_acc: 0.2067\n",
      "Epoch 93/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6980 - acc: 0.3071 - val_loss: 1.8173 - val_acc: 0.2767\n",
      "Epoch 94/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6969 - acc: 0.3157 - val_loss: 1.8176 - val_acc: 0.2100\n",
      "Epoch 95/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6943 - acc: 0.2986 - val_loss: 1.8194 - val_acc: 0.2833\n",
      "Epoch 96/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6948 - acc: 0.3014 - val_loss: 1.8096 - val_acc: 0.2233\n",
      "Epoch 97/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6930 - acc: 0.3057 - val_loss: 1.8228 - val_acc: 0.2300\n",
      "Epoch 98/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6921 - acc: 0.3057 - val_loss: 1.8117 - val_acc: 0.2200\n",
      "Epoch 99/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6901 - acc: 0.3129 - val_loss: 1.8252 - val_acc: 0.2267\n",
      "Epoch 100/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6890 - acc: 0.3114 - val_loss: 1.8210 - val_acc: 0.2267\n",
      "Epoch 101/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6878 - acc: 0.3143 - val_loss: 1.8191 - val_acc: 0.2200\n",
      "Epoch 102/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6877 - acc: 0.3014 - val_loss: 1.8220 - val_acc: 0.2300\n",
      "Epoch 103/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6842 - acc: 0.3000 - val_loss: 1.8101 - val_acc: 0.2500\n",
      "Epoch 104/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6842 - acc: 0.3214 - val_loss: 1.8127 - val_acc: 0.2200\n",
      "Epoch 105/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6821 - acc: 0.3071 - val_loss: 1.8058 - val_acc: 0.2033\n",
      "Epoch 106/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6815 - acc: 0.3129 - val_loss: 1.8174 - val_acc: 0.2200\n",
      "Epoch 107/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6805 - acc: 0.3071 - val_loss: 1.8231 - val_acc: 0.2367\n",
      "Epoch 108/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6784 - acc: 0.3071 - val_loss: 1.8168 - val_acc: 0.2767\n",
      "Epoch 109/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6782 - acc: 0.3143 - val_loss: 1.8179 - val_acc: 0.2300\n",
      "Epoch 110/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6775 - acc: 0.3171 - val_loss: 1.8148 - val_acc: 0.2133\n",
      "Epoch 111/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6775 - acc: 0.3043 - val_loss: 1.8174 - val_acc: 0.2233\n",
      "Epoch 112/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6745 - acc: 0.3129 - val_loss: 1.8194 - val_acc: 0.2267\n",
      "Epoch 113/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6737 - acc: 0.3100 - val_loss: 1.8197 - val_acc: 0.2300\n",
      "Epoch 114/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6724 - acc: 0.3014 - val_loss: 1.8222 - val_acc: 0.2300\n",
      "Epoch 115/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6711 - acc: 0.3086 - val_loss: 1.8129 - val_acc: 0.2333\n",
      "Epoch 116/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6704 - acc: 0.3186 - val_loss: 1.8233 - val_acc: 0.2267\n",
      "Epoch 117/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6692 - acc: 0.3100 - val_loss: 1.8226 - val_acc: 0.2333\n",
      "Epoch 118/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6682 - acc: 0.3129 - val_loss: 1.8272 - val_acc: 0.2333\n",
      "Epoch 119/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6670 - acc: 0.3243 - val_loss: 1.8224 - val_acc: 0.2200\n",
      "Epoch 120/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6660 - acc: 0.3143 - val_loss: 1.8227 - val_acc: 0.2200\n",
      "Epoch 121/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6646 - acc: 0.3071 - val_loss: 1.8111 - val_acc: 0.2233\n",
      "Epoch 122/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6637 - acc: 0.3200 - val_loss: 1.8189 - val_acc: 0.2167\n",
      "Epoch 123/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6629 - acc: 0.3100 - val_loss: 1.8137 - val_acc: 0.2200\n",
      "Epoch 124/3000\n",
      "700/700 [==============================] - ETA: 0s - loss: 1.6619 - acc: 0.3143 - val_loss: 1.8185 - val_acc: 0.2267\n",
      "Epoch 125/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - ETA: 0s - loss: 1.6608 - acc: 0.3257 - val_loss: 1.8218 - val_acc: 0.2333\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# 1. 데이터셋 준비하기\n",
    "\n",
    "# 훈련셋과 시험셋 로딩\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "X_val = X_train[50000:]\n",
    "Y_val = Y_train[50000:]\n",
    "X_train = X_train[:50000]\n",
    "Y_train = Y_train[:50000]\n",
    "\n",
    "X_train = X_train.reshape(50000, 784).astype('float32') / 255.0\n",
    "X_val = X_val.reshape(10000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋, 검증셋 고르기\n",
    "train_rand_idxs = np.random.choice(50000, 700)\n",
    "val_rand_idxs = np.random.choice(10000, 300)\n",
    "\n",
    "X_train = X_train[train_rand_idxs]\n",
    "Y_train = Y_train[train_rand_idxs]\n",
    "X_val = X_val[val_rand_idxs]\n",
    "Y_val = Y_val[val_rand_idxs]\n",
    "\n",
    "# 라벨링 전환\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(units=2, input_dim=28*28, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 엮기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(patience = 20) # 조기종료 콜백함수 정의\n",
    "hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEKCAYAAAChTwphAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncNEyM7NiuGiZmVkxXLTMzKwY/wByTyMborlihgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x272d6c5a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 0s\n",
      "\n",
      "loss : 1.8419726757\n",
      "accuray : 0.2717\n"
     ]
    }
   ],
   "source": [
    "# 6. 모델 사용하기\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "\n",
    "print('')\n",
    "print('loss : ' + str(loss_and_metrics[0]))\n",
    "print('accuray : ' + str(loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
